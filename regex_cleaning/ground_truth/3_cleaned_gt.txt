Non-detects—measurements reported as “below the detection limit”—are ubiquitous in
environmental science and engineering. They are frequently replaced with a constant,
but this biases estimates of means, regression slopes, and correlation coefficients.
Omitting non-detects is worse, and has led to serious errors. Simple alternatives are
available: rank-based statistics, maximum likelihood estimation, and re-purposed
survival analysis routines. But many environmental datasets do not align well with the
assumptions these methods make—it is often necessary to account for hierarchy (e.g.,
measurements nested within lakes), sampling strategy (e.g., measurements collected
as time series), heterogeneity (e.g., site-dependent variance), and measurement error.
Bayesian methods offer the flexibility to do this; incorporating non-detects is also easy
and does not bias model parameter estimates as substitution does. Here we discuss
Bayesian implementations of common bivariate and multivariate statistical methods
relevant to environmental science. We use a dataset comprising time series of Ag, As,
Cd, Ce, Co, Sb, Ti, U, and V concentrations in municipal biosolids that includes many
non-detects. The models can be reproduced and extended to new problems using the
data and code accompanying this paper.

Non-detects—measurements recorded as less than a detection or reporting limit—are
ubiquitous in environmental science and engineering. In the statistical literature, they
are known as left-censored observations. A popular method of representing them in
statistical routines is to replace them with one-half, or some other fraction, of the
detection limit. But while common, this strategy can severely bias estimates based on
the data when the censoring rate is high. Worse still is omission—leaving out non-
detects has led to serious and well-documented errors.
For basic tasks like comparing two groups or estimating a mean, linear regression
slope, or correlation coefficient, there are simple alternatives to substitution and
omission. These include rank-based methods, maximum likelihood estimation, and re-
purposed survival analysis routines.1 But many environmental datasets require more
complex models that account for hierarchy, sampling strategy, heterogeneity, and
measurement error. For instance, measurements may be collected across multiple
lakes with different characteristics or they may be recorded as time series (i.e., serially
dependent data). The standard toolbox for censored data analysis does not always
accommodate these features.

Bayesian methods excel in this context—since the sampling techniques they rely on
provide a near-universal approach to parameter estimation, they can be very flexible. In
particular, it is straightforward to account for non-detects in almost any model. Here, we

provide examples of common statistical models in environmental science and
engineering whose Bayesian versions can easily accommodate non-detects. They are
reproducible via the code and data that accompany this paper.

Materials and methods

Data collection

We fit models to a dataset comprising concentrations of Ag, As, Cd, Ce, Co, Sb, Ti, U,
and V in municipal biosolids. Biosolids samples were collected from the clarifiers of
three wastewater treatment facilities in 125 mL polypropylene bottles. Samples were
autoclaved, desiccated by baking at 105°C for approximately 60 hours, digested
according to EPA Method 3050B,2 diluted serially, and quantified by inductively coupled
plasma mass spectrometry according to Standard Method 3125.3 A summary of the
data is available in Table 1.

The data and code necessary to reproduce the main results from this paper are
available at https://github.com/bentrueman/censored-env-data-analysis; several
functions used to fit the models in Stan are available in a separate R package. We
used R version 4.3.3 throughout,5 along with a collection of contributed packages.

Results and discussion

Bayesian modeling


Bayesian inference entails fitting a probability model to data, then summarizing it as the
joint distribution of the model parameters, 𝜃. The model starts as a prior, 𝑃(𝜃),
quantifying the plausibility of all possible parameter values. The prior reflects
background knowledge and practical considerations.
The data, 𝑥, are used to update the prior via Bayes’ theorem. It relates the posterior or
updated joint parameter distribution, 𝑃(𝜃|𝑥), with the prior and the likelihood. The
likelihood, 𝑃(𝑥|𝜃), quantifies the compatibility of the data with the proposed model.
In practice, model fitting follows these basic steps:



1. Assign a probability distribution to the data.
2. Choose a model for each of the distributional parameters.
3. Choose a distribution of prior probability for each parameter.
4. Iterate the following steps to obtain a sample from the posterior:

(a) Propose values for all parameters.
(b) Quantify their plausibility without reference to the data (via the prior distributions).
(c) Quantify the plausibility of each data point given the assumed data distribution and the
proposed parameter values (i.e., the likelihood).
(d) Obtain the relative posterior probability as the product of the likelihood and the prior (i.e.,
Bayes’ theorem).

Iterating over steps 4 (a–d) may require searching a high-dimensional parameter space,
which is often accomplished via the Hamiltonian Monte Carlo algorithm. Fortunately,
software packages make this straightforward: the R package brms, for instance, fits a
huge variety of Bayesian regression models—including censored data models—with a
standard and familiar syntax. Further customization is possible using Stan, a
programming platform for Bayesian statistics written in C++.

Substitution biases parameter estimates
Replacing non-detects with a constant can bias parameter estimates substantially,
especially when the censoring rate is high. We show this using a small simulation study
that compares substitution at one-half the detection limit with a parameter estimation
strategy that relies on the cumulative distribution function.

When the dependent variable in a linear regression model includes left-censored
observations, one method of accounting for them is to construct the likelihood for every
censored observation using the appropriate cumulative distribution function in place of
the probability density function. Here, the cumulative distribution function quantifies the
probability that a data point is less than the detection limit—that is, the compatibility of a
non-detect with the proposed model. The likelihood, then, becomes 𝑃(𝑥|𝜃) =
𝐹(𝑥!"#$%&$' |𝜃)𝐺(𝑥($)#!%$' |𝜃), where 𝐹 and 𝐺 are the probability density and cumulative
distribution functions, respectively.

We simulated from a simple linear regression model, 𝑦* ∼ 𝑁(𝜇* = 3 + 0.15𝑥* , 𝜎 = 0.5),
where the dependent variable was partially censored—here, 𝑁 represents the normal
distribution with mean 𝜇* and standard deviation 𝜎. We fit a censored regression using
brms where the simulated non-detects were modeled using the normal cumulative
distribution function. We compared it to a naive model where the non-detects were
replaced with one-half the detection limit.
Specifically, the 𝑖 simulated observations, 𝑦* , were modeled as follows. Except for the
special handling of left-censored observations (𝑦* |𝑐𝑒𝑛𝑠𝑜𝑟𝑒𝑑* = 1), the naive model was
identical.

likelihood:
𝑦* |𝑐𝑒𝑛𝑠𝑜𝑟𝑒𝑑* = 0 ∼ 𝑁(𝜇* , 𝜎) [𝑜𝑏𝑠𝑒𝑟𝑣𝑒𝑑 ]
𝑦* |𝑐𝑒𝑛𝑠𝑜𝑟𝑒𝑑* = 1 ∼ 𝑁-𝐶𝐷𝐹(𝜇* , 𝜎) [𝑙𝑒𝑓𝑡-𝑐𝑒𝑛𝑠𝑜𝑟𝑒𝑑 ]

(1)

model for 𝜇:
𝜇* = 𝛽+ + 𝛽, 𝑥*
priors:
𝛽- ∼ 𝑇J𝜇. = 0, 𝜎. = 2.5, 𝜈. = 3M, for j = 0,1
𝜎 ∼ 𝑇(𝜇/ = 0, 𝜎/ = 2.5, 𝜈/ = 3)

In equation (1), 𝑐𝑒𝑛𝑠𝑜𝑟𝑒𝑑* is a binary variable (0 = 𝑜𝑏𝑠𝑒𝑟𝑣𝑒𝑑, 1 = 𝑙𝑒𝑓𝑡-𝑐𝑒𝑛𝑠𝑜𝑟𝑒𝑑), and
𝑁-𝐶𝐷𝐹 is the normal cumulative distribution function (i.e., 𝑃(𝑋 ≤ 𝑥), the probability that
a random variable 𝑋 is less than or equal to some value 𝑥).14,15 The parameters 𝛽+ and

𝛽, define the linear model for 𝜇* , and 𝑇 denotes the t distribution with degrees of
freedom—which controls probability density in the tails—parameterized by 𝜈.

The censored regression model recovered the true parameter values much more
accurately than the naive model (Figure 1). That is, the censored model yielded 95%
credible intervals on the intercept, slope, and residual standard deviation that included
the true parameter values 96, 98, and 97% of the time, respectively. The naive model
yielded intervals that included the true values just 2, 14, and 1% of the time.

Accounting for non-detects in a more complex model
The same strategy can be incorporated into more complex models: here, we use a
dataset of metals concentrations in municipal biosolids to demonstrate fitting a
smoothing spline, a popular method for characterizing environmental time series and
other problems. It was fitted using bgamcar1, an extension of brms that
accommodates continuous-time autoregression—accounting for the dependence of one
observation on the previous one in unequally spaced time series. Titanium
concentrations at three wastewater treatment facilities (Sites 1–3) were modeled asfollows:
likelihood:
[
]
[
)|𝑐𝑒𝑛𝑠𝑜𝑟𝑒𝑑
𝑙𝑜𝑔( 𝑇𝑖 0
0 = 0 ∼ 𝑇(𝜇0 , 𝜎, 𝜈) 𝑜𝑏𝑠𝑒𝑟𝑣𝑒𝑑 ]
𝑙𝑜𝑔([𝑇𝑖]0 )|𝑐𝑒𝑛𝑠𝑜𝑟𝑒𝑑0 = 1 ∼ 𝑇-𝐶𝐷𝐹(𝜇0 , 𝜎, 𝜈) [𝑙𝑒𝑓𝑡-𝑐𝑒𝑛𝑠𝑜𝑟𝑒𝑑]


(2)

model for 𝜇0 :
𝜇0 = 𝛼 + 𝛽#*0$ 𝑋#*0$ + 𝑓 (𝑡) + 𝜙 # 𝑟01#
𝑟01# = 𝑙𝑜𝑔([𝑇𝑖]01# ) − 𝛼 − 𝑓(𝑡 − 𝑠)
priors:
𝜎 ∼ 𝐻𝑎𝑙𝑓-𝑇(𝜇/ = 0, 𝜎/ = 2.5, 𝜈/ = 3)
𝛽#*0$ ∼ 𝑇J𝜇. = 0, 𝜎. = 2.5, 𝜈. = 3M
𝜈 ∼ 𝐺𝑎𝑚𝑚𝑎 (𝜇2 = 2, 𝛼2 = 0.1)
𝜙 ∼ 𝑁J𝜇3 = 0.5, 𝜎3 = 0.5M
𝛼 ∼ 𝑇(𝜇4 = 0, 𝜎4 = 2.5, 𝜈4 = 3)

where, in addition to the symbols defined above, 𝐻𝑎𝑙𝑓-𝑇 represents the positive-valued t
distribution and 𝐺𝑎𝑚𝑚𝑎 the gamma distribution, parameterized by mean 𝜇 and shape
parameter 𝛼. The linear model 𝛽#*0$ 𝑋#*0$ estimates a separate intercept for each site,
where 𝑋#*0$ is the design matrix and 𝛽#*0$ the coefficients. The autocorrelation
coefficient, 𝜙 # , and the residual at time 𝑡 − 𝑠, 𝑟01# , define the dependence of each
observation on the previous one, where 𝑠 is the spacing between adjacent
observations.

1The term 𝑓(𝑡) is a smooth spline function that captures nonlinear variation in the mean
with time. It takes the following form:

𝑓(𝑡) = 𝑋#56*)$ 𝛽#56*)$ + 𝑍𝑏

priors on spline parameters:
(3) 𝛽#56*)$ ∼ 𝑇J𝜇. = 0, 𝜎. = 2.5, 𝜈. = 3M [𝑢𝑛𝑝𝑒𝑛𝑎𝑙𝑖𝑧𝑒𝑑 ]
𝑏 ∼ 𝑁(0, 𝜎" ) [𝑝𝑒𝑛𝑎𝑙𝑖𝑧𝑒𝑑 ]
𝜎" ∼ 𝐻𝑎𝑙𝑓-𝑇J𝜇/! = 0, 𝜎/! = 2.5, 𝜈/! = 3M

where 𝑍 and 𝑋#56*)$ are matrices representing the penalized and unpenalized basis

functions, while 𝛽#56*)$ and 𝑏 represent the corresponding spline coefficient vectors.

Geometric mean titanium concentrations varied in a quasi-seasonal pattern (Figure 2),
and samples collected at one facility, Site 3, had mean concentrations approximately
20–30 µg g-1 higher than those representing the other two facilities. Observations
exhibited mild serial correlation, which quantifies the dependence of each observation

on the previous one, after accounting for trends and site-specific variation. The serial
correlation parameter in the model, 𝜙, had a posterior median of 0.11 with a 95%
credible interval spanning 0.03–0.26. In general, accounting for serial correlation
improves the accuracy of predictions and helps avoid overfitting.

A censored predictor
Left-censoring may also occur in the predictor variable. One potential application is
building a linear regression model to predict missing values in one variable using
another, partially censored, variable. Left-censored predictors, though, are not
amenable to substituting a cumulative distribution function in the likelihood—another
strategy is required.



Fortunately, there is a straightforward alternative: all the non-detects can be treated as
missing values with an upper bound and represented by parameters in the model. Since
Bayesian modeling results in a set of posterior draws—vectors of plausible parameter

values—this is similar to multiple imputation of missing values with an upper—and
optionally a lower—bound. But since it is done in one step, we get a joint distribution
that quantifies the uncertainty and interrelationships among all the parameters, including
the censored values. This might be important, for instance, when there is serial
dependence in the data.

Another advantage is the size of the imputed dataset: since the model is fitted to the
data just once, it is straightforward to generate several thousand imputed values, even
for complex models. This may not be practical if values are imputed before model fitting:
the conventional imputation strategy entails fitting one model for each set of imputed
values. Furthermore, it may be difficult to find a multiple imputation routine that meets all
of the needs of a particular data analysis.

Using the Bayesian approach, we can specify any predictive model we would like to
impute the censored values. Since there were only a few censored observations in this
pair of variables (Figure 3), we chose a simple intercept-only imputation model, fitted
using bgamcar14. It was defined as follows:
likelihood:
[𝐶𝑜]* ∼ 𝑇J𝜇[8!]" , 𝜎[8!] , 𝜈[8!] M
[𝐶𝑑]* ∼ 𝑇J𝜇[8'] , 𝜎[8'] , 𝜈[8'] M

(4)

model for 𝜇:
𝜇[8!]" = 𝛽+[$%] + 𝛽, [𝐶𝑑]*
𝜇[8']" = 𝛽+[$']
priors:
𝛽- ∼ 𝑇J𝜇. = 0, 𝜎. = 2.5, 𝜈. = 3M, for j = 0,1
𝜎 ∼ 𝑇(𝜇/ = 0, 𝜎/ = 2.5, 𝜈/ = 3)
𝜈 ∼ 𝐺𝑎𝑚𝑚𝑎(𝜇2 = 2, 𝛼2 = 0.1)

When the model was fitted with a Gaussian likelihood, the extreme cobalt concentration
of 309 µg g-1 yielded a posterior mean with an extremely wide credible interval (Figure
3). The robust model, fitted with a Student t likelihood, yielded a much narrower credible


interval and a posterior mean that was much less heavily influenced by the extreme
value. A disadvantage of both models is that simulating from them may generate
negative concentrations, even though the posterior mean remains positive over its
range. This could be solved by modeling log-transformed Co concentrations instead,
resulting in a slightly different interpretation: the model would then predict geometric
mean concentrations on the scale of measurement.

214

Multivariate models
In a multivariate context, the one-step multiple imputation strategy is often simpler to
apply, since multivariate cumulative distribution functions can be difficult to implement.
Two multivariate models with applications in environmental science are the intercept-
only model, used to estimate a correlation matrix, and principal component analysis.

219

A Bayesian correlation matrix
In addition to handling non-detects, Bayesian correlation has the advantage that it can
be readily applied to variables that are best described using distributions other than the
Gaussian. A relevant example for environmental sciences is robust correlation, where a
Student t distribution is assigned to each variable and its parameters estimated. This
tends to limit the influence of extreme values, which might otherwise exert undue
influence on the estimated correlation coefficients.
Given 𝑦, an 𝑁 × 𝐷 matrix containing 𝑁 concentrations of 𝐷 elements, we can estimate
the pairwise correlations as follows:

(5)

likelihood:
𝜇,
𝑦 ∼ 𝑀𝑢𝑙𝑡𝑖𝑣𝑎𝑟𝑖𝑎𝑡𝑒𝑁𝑜𝑟𝑚𝑎𝑙 _` … b , 𝛴d
𝜇:
𝜎, … 0
𝜎, … 0
𝛴 = ` ⋮ ⋱ ⋮ b𝑅` ⋮ ⋱ ⋮ b
0 … 𝜎:
0 … 𝜎:
priors:
𝜇- ∼ 𝑇J𝜇; = 0, 𝜎; = 10, 𝜈; = 3M, for j = 1, . . , 𝐷
𝜎- ∼ 𝑇(𝜇/ = 0, 𝜎/ = 10, 𝜈/ = 3), for j = 1, . . , 𝐷
𝑅 ∼ 𝐿𝐾𝐽𝑐𝑜𝑟𝑟(2)

229

where the 𝜇,...: are the column means of 𝑦, 𝛴 is the covariance matrix, the 𝜎,...: are the
column standard deviations of 𝑦, and 𝑅 is the correlation matrix. 𝐿𝐾𝐽𝑐𝑜𝑟𝑟(2) is a
regularizing prior that encodes mild skepticism of extreme correlation coefficients near -
1 or 1.

Arsenic, vanadium, and cadmium concentrations were most strongly correlated in
biosolids (Figure 4). And overall, the robust model—incorporating Student t
likelihoods—identified more correlation among the variables than the conventional, non-
robust model fitted with Gaussian likelihoods. This is due to the much smaller influence
that extreme values have on the robust model.


Probabilistic principal components analysis
Principal component analysis is a method for summarizing a multivariate dataset using
a subset of derived variables that capture the majority of the data’s variance. Here we
implement probabilistic principal component analysis, a Bayesian generalization of the
classical approach. Our model is modified from the approach described in a recent
paper to accommodate left-censoring of the data and is written in Stan. Given 𝑦, a
𝐷 × 𝑁 matrix containing 𝑁 concentrations of 𝐷 elements,

𝑙𝑖𝑘𝑒𝑙𝑖ℎ𝑜𝑜𝑑:
𝑌 ∼ 𝑀𝑢𝑙𝑡𝑖𝑣𝑎𝑟𝑖𝑎𝑡𝑒𝑁𝑜𝑟𝑚𝑎𝑙(𝑊𝑧 + 𝜇, 𝜎𝐼)


(6)

𝑝𝑟𝑖𝑜𝑟𝑠:
𝑧 ∼ 𝑁(0, 𝐼)
𝑊 ∼ 𝑁(0, 𝜎𝛼)
𝜇 ∼ 𝐿𝑜𝑔𝑛𝑜𝑟𝑚𝑎𝑙J𝜇; = 2.5, 𝜎; = 1M
𝜎 ∼ 𝐿𝑜𝑔𝑛𝑜𝑟𝑚𝑎𝑙(𝜇/ = 0, 𝜎/ = 1)
𝛼 ∼ 𝐼𝑛𝑣𝑔𝑎𝑚𝑚𝑎(𝛼4 = 1, 𝛽4 = 1)


where 𝑧 is a 𝑘 × 𝑁 matrix of latent (i.e., unobserved) variables with 𝑘 ≤ 𝐷, 𝑊 is a 𝐷 × 𝑘
transformation matrix mapping from the latent space to the data space, 𝜎 is the
standard deviation of the error (also a latent parameter), 𝐼 is the identity matrix, and
𝐼𝑛𝑣𝐺𝑎𝑚𝑚𝑎 is the inverse gamma distribution.

Differences in metals concentrations among the three sites are apparent in Figure 5a. In
particular, Site 3 scored differently on the two principal components, resulting in
substantial separation in the two-component space from the data representing the other
two sites. Differences in titanium concentrations among treatment facilities play a strong
role here: the principal axes—that is, the orthonormalized columns of the transformation
matrix 𝑊 (equation 6)—load titanium concentrations heavily. And titanium
concentrations were high in biosolids from Site 3 relative to Sites 1 and 2 (Figure 2).

Conclusion
Replacing non-detects with a constant—often one-half the detection limit—biases
estimates of means, regression slopes, and correlation coefficients. Simple alternatives
exist, but they are limited and not always applicable to complex environmental datasets
that exhibit hierarchy, complex dependence structures, and heterogeneity. Bayesian
methods have the flexibility to model all of these features, and they can easily
accommodate left-censoring by either modifying the likelihood or one-step multiple
imputation as a part of model fitting.
