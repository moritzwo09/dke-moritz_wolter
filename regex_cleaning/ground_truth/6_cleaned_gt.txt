Continuous monitoring systems (CMS) that utilize fixed-point sensors provide high temporal
resolution point-in-space measurements of ambient methane concentration. This study introduces
a modular framework for optimizing CMS configurations, encompassing sensor density (number
of sensors) and near-optimal placement. By introducing a metric called ‘blind time’, this study
attempts to capture periods where the network fails to make detections that could satisfy the
regulatory requirement of quantifying emissions every 12 hours. This framework is then applied
to 124 operational oil and gas production facilities with a wide variety of site characteristics and
meteorological conditions. This study determines a representative blind time for near-optimum
CMS configurations for operational facilities and then investigates the impact of different sensor
network densities on the performance of the CMS. The results demonstrate that 3-sensor networks,
when placed in near-optimum arrangements, can achieve blind time of less than 10% and a mean
time to detection of approximately 82 minutes.

The role of methane, as a short-lived greenhouse gas with high heat-trapping potential, in global
warming is undeniable. Given these characteristics, mitigating methane emissions is one of the most
effective and efficient strategies for curbing short-term global warming. The energy sector, particularly
the oil and gas industry, is a significant contributor to human-caused methane emissions. Many oil
and gas methane reduction strategies are cost-effective or even profitable, as captured gas can often be
directed to pipelines and sold for additional profit, making oil and gas methane mitigation a financially
viable option.
Gas releases from the oil and gas industry can be categorized as venting (releases of gas for known
and planned reasons), fugitive (unintended releases or leaks), and incomplete combustion emissions
(combustion slip). Accurate interpretation of oil and gas emissions profile requires considering source
types, event timing and intermittency (duration and frequency), release rate, and spatial emission
source distribution. This underscores the critical need for timely and accurate detection and localization of emission events, reliable quantification of gas release rate, and swift mitigation actions.
A combination of measurement technologies is often recommended for effective detection and quantification of methane emissions events [1–3]. Satellite measurements can remotely detect large-scale
methane emissions occurring at the time of observation, enabling independent oversight and enhancing transparency, particularly in understudied regions with limited site access. However, satellite
surveys have high detection thresholds and limited overflight frequency. Aerial platforms, including mass-balance piloted aircraft and uncrewed aerial vehicle (UAV) surveys, offer lower detection
thresholds compared to satellites, but are susceptible to spatial (below the lowest safe altitude for

flying for aircraft, and above maximum flying height for UAVs) and temporal extrapolation errors.
Other aerial platforms, including downward-looking laser methane column measurements, offer more
accurate plume detection and source localization. However, uncertainties in detection and flux rate
quantification are significantly influenced by factors such as flight altitude and wind speed.
In general, snapshot emission measurement methods, including vehicle-based, aerial, and satellite
measurements, often encounter limitations due to the lack of site-specific meteorological data and
information on emission events timing (e.g., duration and frequency of intermittent events). This
shortcoming is particularly problematic for emission estimations at complex and congested sites, such
as midstream compressor stations.
Continuous monitoring systems using fixed-point sensors (hereafter referred to as CMS) are a cornerstone of multiscale methane emissions measurement approaches, providing site-specific emissions
knowledge. A CMS consists of one or more sensing units strategically placed at a safe distance from
potential emission sources to continuously measure ambient pollution concentration levels. Combined
with on-site meteorological measurements, this data can enable the detection, timing, localization, and
quantification of emission events. Key benefits of the CMS include (i) rapid detection of emissions ranging from relatively low rates to super emitting events, (ii) capture of both short-duration/intermittent
and continuous events (iii) accurate time-bounding of intermittent emission events, and (iv) complimenting other measurement methods using a continuous stream of site-specific data on emissions and
meteorology.
To realize the full potential of CMS, reliable ambient concentration and meteorological measurements (hardware capabilities) should couple with algorithms capable of transforming raw data into
accurate insights related to emission event timing, location, and magnitude (data analytics). The
optimal CMS configuration, including the number of sensing units (sensor network density) and placement (finding optimum locations for fixed point-sensor installations, from a large number of candidate
locations to satisfy a given objective function), is another key factor influencing the performance of
CMS systems. Operational records and knowledge, such as parametric information and inspection
records, can complement CMS measurements and enhance the differentiation between intended and
unintended emission events, providing actionable insights.
Performance evaluation of continuous methane monitoring systems has been the subject of several
studies in recent years. Single-blind testing at controlled release facilities, such as Colorado State
University’s Methane Emissions Technology Evaluation Center (METEC) [4] and Stanford University’s testing facility in Casa Grande, Arizona [5], simulates challenging yet simplified field conditions
to assess the leak detection and quantification capabilities of various CMS solutions. These studies demonstrate the rapid evolution of CMS. Recent METEC single-blind testing, conducted using
the Advancing Development of Emissions Detection (ADED) protocol, revealed that one of the topperforming solutions achieved a 90% probability of detecting 0.5 kg/hr emissions while accurately
localizing releases in 98.7% of cases, with a mean quantification error of -0.064 kg/hr [6]. This results
in a cumulative emissions quantification error of 2.8% when comparing the overall quantified methane
release amount to the actual methane release over the course of the ADED study. These studies investigate the performance of CMS assuming that the solution provider has employed an optimum network
configuration in its deployment. It is essential to emphasize that effective monitoring relies not only on
measurement hardware and emissions analytics capabilities but also on optimum CMS configuration.
In other words, the 2024 ADED results indicate that CMS systems are capable of achieving these
performance levels, with optimal sensor network configurations, including appropriate sensor density
and proper placement configuration, considering facility-specific variables.
A well-configured CMS is expected to frequently reflect variations in ambient methane concentration
levels caused by site-level emissions. Two primary characteristics of a well-configured CMS are proper
sensor network density (considering the limited sensor budget) and optimum placement of sensors in
the oil and gas facility. This multi-objective optimization network design problem aims to maximize
signal coverage (in this context, ‘signal’ refers to ambient methane concentration levels reflecting the
presence or absence of an on-site gas release event), ensuring that for a maximum amount of time,
fluctuations in ambient methane concentration levels due to on-site gas releases are captured by the
CMS. These characteristics enhance rapid detection, time-bounding, localization, and quantification
of emission events using subsequent analyses.


Many factors can influence near-source pollutant transport and consequently, signal delivery from
the release point(s) to sensors. The primary factors include wind direction, wind speed, atmospheric
stability class, facility setup, surface elevation profile, on-site airflow obstructions, source-receptor
distance, plume release height, receptor height (specifically the intake air height for each sensor), nearby
offsite emission sources, and for specific sources (e.g., combustors) gas velocity and temperature.
Ideally, a CMS sensor network configuration algorithm should consider all relevant factors. However, such an optimization algorithm can be computationally intensive and impractical with limited
data access and operational constraints. A more practical approach involves reducing the complexity of
this problem by focusing on the most significant factors. This study aims to present a framework that
incorporates some of the key factors into the CMS sensor network configuration process. Although all
of the factors are not considered in this framework, this offers an initial and foundational step toward a
more comprehensive, data-driven approach to CMS configuration, considering a wider range of factors
in decision-making.
Reliable emissions event detection requires a CMS configuration (sensor network density and placement) that maximizes the frequency of direct pollution transport from source to sensor. In this context,
a new metric, “blind time” is introduced in this work, representing the fraction of 12-hour periods with
insufficient data to enable accurate emissions quantification due to limited direct pollution transport
from sources to the sensor network. The definition of blind time is aligned with the EPA’s criteria for
CMS to be considered as an alternative test method under the recently established OOOOb regulation
to reduce methane emissions from the oil and natural gas industry. This regulation allows for the
use of alternative test methods, including CMS, to assess compliance with emission standards, if the
following requirements, among others, are satisfied the specific measurement solution is designated as
an approved alternative test method by the EPA: (i) “... continuous monitoring means the ability of a
methane monitoring system to determine and record a valid methane mass emissions rate or equivalent
of affected facilities at least once for every 12-hour block”, and (ii) the 90-day rolling average actionlevel is 1.6 kg/hr (3.6 lb/hr) of methane over the site-specific baseline emissions” and “The 7-day
rolling average action level is 21 kg/hr (46 lb/hr) of methane over the site-specific baseline emissions.”
Building upon these regulatory developments, optimizing sensor placement becomes even more critical
to meet the requirements outlined by the EPA.
Previous studies have explored various aspects of methane emissions from oil and gas operations,
including sensor placement optimization, and performance evaluation of CMS. The following literature
review synthesizes key findings and identifies areas for further research. Chen, Kimura, and Allen
[7] employed dispersion modeling to develop robust detection limit definitions, for evaluating CMS
performance characteristics based on a short timeframe of historical data (2 weeks) and a single
emission source. This study offered a comparison of simulations to controlled release testing results for
CMS. This study showed that the performance of CMS and their minimum detection limits expressed
as an emission rate, depend on meteorological conditions, facility-specific emissions characteristics,
sensor placement, and the amount of time allowed for the CMS to detect an emission source (time
to detection). This study demonstrated that a single sensor with a detection limit below 2 ppm,
positioned 10 meters from a 0.4 kg/h source, could detect the emission within 12 hours. Deploying
three sensors resulted in a lower detection time of 6 hours. While increasing the number of sensors
from 4 to 8 slightly improved detection time, the most significant reduction occurs between 1 and 4
sensors. Enhancing sensor sensitivity or adding more sensors generally decreases the average time to
detection within the sensor network.
An earlier study by Chen et al. [8] evaluated the performance of CMS on a regional spatio-temporal
simulation domain that incorporates multiple facilities, indicating that based on atmospheric dispersion
simulations of 26 oil and gas production sites in the Permian Basin in Texas, fifteen sensors, each
capable of detecting concentration increases of 1 ppm, successfully identified emissions at all 26 sites
during four weeks of meteorological episodes, assuming continuous emissions of 10 kg/h.
Sensor placement optimization falls under the broader category of combinatorial optimization,
aiming to find the best solution from a finite set of possibilities. These inherently complex problems
arise in a variety of real-world scenarios, including sensor placement for environmental monitoring
in ocean observation [9, 10], water contamination detection [11], hazardous gas leakage monitoring
in chemical plants [12], pipeline leaks [13–15] and air quality monitoring [16]. Depending on the


application and objectives, a wide range of methods have been employed to achieve optimum sensor
placement, including heuristic methods like greedy algorithms [13, 14, 17], genetic algorithms [18],
deterministic approaches like mixed integer programming [11, 19], and decomposition techniques [10,
16].
In the context of CMS for methane emission monitoring, several optimization methods have been
employed to determine the best sensor placement and evaluate their performance in a simulated environment. Klise et al. [19] developed an open-source Python package, called Chama, employing
atmospheric dispersion modeling and mixed integer linear programming (MILP) to determine optimum sensor locations and detection thresholds to maximize emission event detection. The algorithm
offered in this work formulates and solved maximum-coverage and minimum-impact problems in two
steps, where first methane emission dispersion was modeled using given leak rates, source locations,
and wind variables. Then, sensor placement problem for a given sensor network density was solved
using MILP. The application of Chama was showcased in the form of a simulated case study in their
publication. In our study, we use Chama as an open-source benchmarking tool, offering a comparative
analysis of Chama and our proposed framework (Section 5).
Zi et al. [20] attempted to address challenges in sensor placement optimization using stochastic
programming by introducing a distributionally robust optimization formulation of sensor placement
under the uncertainty of wind conditions. Leveraging MILP for optimization, this study demonstrated
significant improvements to Chama, minimizing the detection time expectation, with a particular focus
on worst-case scenarios.
Liu et al. [21] investigated the optimal sensor placement problem in a 2-dimensional domain using
bi-level optimization. This algorithm used inverse modeling to estimate emission rates and determine
the optimal sensor placement by minimizing the overall mean squared error of the estimated emission
rates over various wind conditions. To solve the sensor placement problem, this study investigated the
repeated sample average approximation and the stochastic gradient descent-based bi-level approximation methods. Numerical examples were used to illustrate the application of the proposed method.
The authors noted that the sensor allocation problems in the 3-dimensional space (representing emission source and sensor height) will be more challenging, as it requires incorporating surface terrain
modeling and computationally efficient algorithms.
Jia, Sorensen, and Hammerling [18] presented an open-source modular framework using the Gaussian puff dispersion model and genetic algorithm to solve the CMS sensor placement optimization
problem in a 3-dimensional domain, considering sensor budget limitation and detection efficiency.
This publication offered case studies implementing the proposed framework for determining optimum
sensor placements for CMS including 4 and 8 sensors. These case studies were implemented in both
within-the-property-boundary and fenceline placement fashions. Results of this study indicated significant improvement in detection coverage, compared to a greedy search method. While a 3-dimensional
domain offers theoretical advantages, practical considerations, such as the implementation challenges,
occupational safety considerations, and costs of installing sensors at various heights, often limit its applicability in real-world scenarios. Additionally, the resource-intensive nature of employing Gaussian
puff models for simulation, genetic algorithms for optimization, and a fine grid size as candidate locations for sensor placement results in high computational demands, which impose practical constraints
and hinder the at-scale implementation of this framework in real-world scenarios.
Typically, sensor optimization frameworks and case studies follow a structured approach, incorporating the following steps with minor variations: (a) define emission source locations and potential
sensor locations, (b) use historical wind data in tandem with a forward dispersion model to simulate
methane concentrations, (c) identify optimal sensor locations through various approaches, and (d)
evaluate the detection capabilities of CMS.
Building upon the the existing knowledge from previous studies on the optimization of the CMS
configuration, the present study aims to take a more practical approach to this problem, by focusing
on the performance capabilities of CMS installed in 124 real-world operational oil and gas facilities,
to draw more general conclusions about the detection capabilities of CMS across a wide variety of
geographic regions or facility designs. In the current work, we explore a greedy sensor placement
framework for CMS at oil and gas facilities specifically designed to meet the criteria outlined in the

recently finalized EPA OOOOb regulation, to study CMS capabilities to meet the criterion of 12-hour
facility-level quantification.
This study proposes a modular framework employing forward dispersion modeling and a greedy
optimization algorithm to determine the best CMS configuration (network sensor density and nearoptimum placement) and consequently determine several CMS performance metrics, including information gain, network blind time, and time to detection. An extensive real-world evaluation effort is
offered to investigate the performance of the proposed framework across 124 oil and gas operational
facilities. The overarching objective of this study is to investigate whether, based on the data from operational facilities, the proposed framework and CMS are capable of consistently detecting emissions of
a given level. To the best of our knowledge, this is the first study to rigorously test a sensor placement
framework across a wide variety of real-world operational facilities and emission sources. Another novelty of this work lies in its purposeful design of the modular framework and metrics around regulatory
requirements. Using a wide variety of real-world upstream oil and gas facilities for model evaluation,
this study determines sensor network blind time based on the site-specific meteorological data. As part
of a benchmarking study, a smaller operational dataset is processed by both the open-source Chama
model and the proposed framework. The findings of this study will inform future efforts related to
the understanding of the capabilities and limitations of CMS related to continuous emissions measurement and quantification. It should be noted that performance evaluation of emissions quantification
algorithms is beyond the scope of this study and will be discussed in a future publication.

This modular framework optimizes the CMS configuration and consequently determines sensor network blind time. Three primary steps are included in this framework: (i) input data processing and
receptor grid determination, (ii) simulation of emissions using a forward dispersion model of choice,
and (iii) optimization of CMS configuration using a greedy algorithm to determine the best sensor
placement configuration for various sensor network densities. Figure 1 provides a schematic view of
this framework. The algorithm is designed to optimize the performance of CMS in light of 40 CFR
§60.5398b(c), meaning that CMS performance is characterized by the network’s ability to consistently
detect new information from emission sources over 12-hour periods.
Three metrics are considered for the performance evaluation of CMS, including information gain,
blind time, and time to detection. By introducing a metric called blind time, this study attempts to
capture periods where the network fails to make detections that could satisfy the regulatory requirement of generating valid emissions quantification every 12 hours. By applying this framework to 124
operational oil and gas production facilities with a wide variety of site characteristics (e.g., layout and
complexity) and meteorological conditions, this study attempts to determine a representative blind
time for near-optimum CMS configurations for operational facilities and then investigate the impact
of different sensor network densities on the performance of the CMS. The following subsections outline
the key components of the proposed framework, including data (input data processing and receptor
grid determination), simulation (forward dispersion modeling of emissions), and optimization (determining the best sensor placement for CMS configuration using a greedy algorithm for various sensor
network densities).

This framework leverages site-specific data to accurately simulate real-world conditions from operational facilities. Three sets of input data are imported into the framework, including facility layout,
facility-specific atmospheric data, and financial constraints that may impose limitations on the sensor
network budget. This section outlines data sources for facilities with existing site-specific data and
provides alternative resources for gathering data in the absence of such programs.
Facility layout data includes site boundaries, on-site equipment inventory, and high-precision spa5

(HRRR) model can be used. To balance computational efficiency and data representation, the application of the CMS configuration framework optimization involving operational oil and gas facilities
uses four weeks from each season instead of a full year of meteorological data. This approach captures seasonal variability while reducing computational burden, enabling minute-based concentration
simulations and ensuring that the results are not biased toward the wind characteristics of a specific
season. A brief examination of how the wind speed and the atmospheric stability classes (ASC) vary
across the different basins is presented in section A.2. Further determination of the optimal amount
of meteorological data is case-dependent and beyond the scope of this paper.
Plume dispersion patterns are known to be more complex in the presence of low wind speed.
This condition negatively impacts the capability of pollution transport models to accurately estimate
concentration levels at given locations. As a result, this study adopts a common practice of excluding
time windows with wind speeds U lower than 0.5 m/s. An optimization/test approach is employed
to assess model performance. This study uses 75% of the meteorological data (12 weeks) for sensor
placement optimization, while the remaining 25% of the data (4 weeks) serves as the test set, allowing
a blind evaluation of out-of-sample data. The 75/25 split aligns with common practices and ensures
proper training while the model’s generalizability is assessed using unseen data.
Facility-specific meteorological one-minute intervals of average wind speed and direction data were
collected using Gill 2D ultrasonic anemometers or RM Young Ultrasonic Anemometers. In addition to
facility layout and site-specific atmospheric data, operator-provided information on equipment inventory and monitoring budget constraints is crucial. Insights such as operator input regarding equipment
inventory (for the labeling of potential emission sources) and additional sensor placement constraints
(for the designation of infeasible sensor installation area) are used to further enhance this framework.


Input data processing involves source clustering and defining feasible sensor installation regions. Facility layout and operator-provided information are used to label potential emission sources associated
with on-site equipment. To reduce the dimensionality of the optimization, agglomerative clustering
with a 15-meter spatial threshold is employed to define spatially distinct equipment groups, with each
equipment group representing all the potential emission sources located within its 15-meter distance.
This approach avoids bias towards equipment groups with dense potential emission points.
Following source group definition and characterization, a search grid is established for potential
sensor locations. The grid boundaries are constructed by extending the minimum and maximum UTM
X,Y coordinates of the set of emission sources by 30.8 meters. This results in 4 vertices that define the
search grid area and will be referred to as the feasible region. Grid size impacts the resolution of the
sensor placement optimization problem. A 10x10 meter mesh grid is overlaid on the feasible region to
define search grid nodes (potential sensor locations). This resolution is chosen to adequately capture
spatial variations in emission concentration while having reasonable processing times.
A 15.4-meter safety buffer is established around the equipment groups, defining a buffered hull
that represents infeasible regions (areas where sensor installation is prohibited). Additional operatorprovided information is incorporated to account for other operational restrictions when defining infeasible regions. Examples of such operational restrictions could be inside-facility driving paths and areas
preserved for future equipment installation or other purposes. The feasible region is further refined by
removing search grid nodes that fall within the infeasible region.
Figure 2 illustrates this process with an example. This modular framework allows for flexibility in
modifying selected values of a range of variables, including grid size and the distance used to define
the outer limit of the feasible sensor installation region.


A forward dispersion modeling tool is employed, utilizing historical atmospheric measurements to
simulate pollution transport for plumes originating at the clustered source location. A simulated
plume will inform the expected atmospheric concentration levels originating from a specific source at
each search grid node at any given time. Current work employs a Gaussian plume model to simulate
pollution dispersion for the 124 operational oil and gas production facilities. However, other pollution
transport models, such as Gaussian puff can be used for this simulation. Our previous studies have not
indicated significant improvement in the simulation accuracy as a result of employing more complex
models like Gaussian puff for relatively simple site setups such as simple upstream oil and gas facilities.
The OOOOb rule mandates a response when the 90-day rolling average exceeds the action threshold
of 1.6 kg/h above facility baseline. This algorithm is designed to optimize the performance of CMS
in light of OOOOb. As a result, continuous emissions are simulated with a release rate of 1.6 kg/hr
for each emission group. This approach reflects operational conditions where most of the leaks in the
real world often persist until repairs are implemented. An in-depth sensitivity analysis of the model
performance across various release rates is offered in the supporting document (subsection A.3).

A source-sensitivity matrix S of size (n,m) is generated for every search grid node. Each row (indexed
by i) represents a minute of atmospheric methane concentration (totaling n minutes of simulation)
and each column (indexed by j) represents a given source group (totaling m source groups). The i, jth
element of the source-sensitivity matrix represents the simulated atmospheric concentration value observed at the location of the search grid node, resulting from a release rate of 1.6 kg/hr from source j
during minute i. Two sensitivity matrices are constructed for each search grid node, representing training and testing simulation data, hereafter referred to as “in-sample” and “out-of-sample”. In-sample
and out-of-sample sensitivity matrices include 12 and 4 weeks of data, corresponding to 120,960 and
40,320 minutes of simulated atmospheric concentrations per sensor per source, respectively. It corresponds to a total of more than 305 million minutes of simulation across all 124 oil and gas operational
facilities. This calculation is applied to every potential sensor, generating two unique sensitivity matrices, in-sample and out-of-sample, for each search grid node located in the feasible region. This results
in approximately 47 billion simulated concentration values captured in the sensitivity matrices.

In this study, detection is defined as an increase in atmospheric concentrations from a given source
exceeding 1 ppm. This binary approach avoids optimization biases towards signal strength. This study
assumes the use of tunable diode laser absorption spectroscopy (TDLAS) sensors, with a sensitivity
of 0.4 ppm [22]. Therefore, the 1 ppm threshold provides a conservative detection level, 2.5 times the
hardware sensitivity. By equally weighting concentrations greater than 1 ppm, we transfer the problem
into a binary dimension (detection vs. non-detection), simplifying it to the presence or absence of a
detectable signal.
For each search grid node, each element in the sensitivity matrix with a value greater than 1
ppm indicates an elevated atmospheric concentration detected at the specific search grid node, where
the increase in atmospheric concentration results from a 1.6 kg/h release originating from source j
and is detectable by a sensor during minute i. A threshold is applied to convert continuous concentration values (sensitivity matrix) into binary detection values in a node-specific detection matrix,
Sd . Each element i,j in the detection matrix is binary, with 1 indicating detection and 0 indicating
non-detection. This binary element indicates detection status at the specific search grid node, during
minute i, resulting from source group j.

This framework employs a multi-objective optimization approach to solve CMS sensor network configuration problem. This approach optimizes the network configuration based on two key metrics: information density and blind time. By maximizing information density, the framework ensures that the
exposure of the CMS to the elevated concentrations originating from onsite release events is maximized.
Simultaneously, minimizing blind time helps prevent long periods where sources remain undetected by
the CMS.
The information density for every single search grid node is calculated as the fraction of a given
detection matrix that is nonzero. It represents the fraction of the time a sensor located at a given
search grid node detects emissions from any onsite release event. Information density (ID) is calculated
as:
Pn
ID =

i=0

Pm

j=0 Si,j

n×m

.

(1)

For a CMS with p sensors, the overall information density is calculated based on a ‘network detection
matrix’, a binary matrix formed by applying the boolean logical or operator across the node-specific
detection matrices associated with the p search grid nodes. The binary nature of the CMS detection
matrix prevents double-counting of events detected using multiple sensors simultaneously. This metric
is similar to those used in other studies on sensor location optimization [18, 19].
Since information density does not consider temporal variability in detections, relying solely on
this metric for CMS configuration can lead to extended periods where no sources are observed by the
network. To fully leverage the continuous nature of the CMS measurements, incorporating a metric
that minimizes the characteristic timescale of “gaps” between detections is essential. This paper
introduces a new metric, blind time, to minimize such gaps between detections.
The term blind time notionally refers to the time period over which the CMS, on average, does not
register enough ‘information’ to enable a sufficiently accurate estimate of the site-level emissions. It
is a function of several factors including sensor count, placement, and the modeling approach among
others. An ensemble average of the blind time is obtained by sampling over a set of similar operational
oil and gas facilities. Within the present context, a 12-hour rolling window is used to segment the
simulated time horizon following the OOOOb rule requirements. On a per-source-group basis (i.e., for
each column of S), the CMS network is assumed to be operating in the blind for the 12-hour duration


highlighted by the rolling window if there are fewer than 10 non-zero entries in the relevant section
of the sensitivity matrix S corresponding. This count of the 12-hour-long time periods identified as
blind intervals is aggregated for each potential source group to estimate a cumulative count of blind
intervals (BT) for each site. Mathematically, this can be expressed as:
P
m X
w 
X
1, if P Qk ≤ 10
Blind Time (BT) =
.
0, if
Qk > 10

(2)

j=1 k=0

Here, the number of rolling 12-hour interval covering a simulation period n minutes long is determined via w = n − (12 × 60) + 1. Qk represents a subset of N over the window k for column j,
where N is defined below. The CMS detection matrix, N , is simply the boolean logical or operator
(∨) applied element-wise across the p search grid nodes that are selected for the placement of the
individual sensors. This is mathematically expressed as:
N = Sd,1 ∨ Sd,2 ∨ ... ∨ Sd,p

(3)

The CMS detection matrix is initialized as a matrix of size (n, m), where n is the number of minutes
in the sample data and m is the number of source groups at the given site. This matrix indicates
the detection status achieved using a given CMS of p sensors. A sensitivity analysis, detailed in A.1,
informed the decision to use at least 10 nonzero entries in N to classify a 12-hour interval for each
source group as either blind or detected.

As noted, the objective of this framework is to find a near-optimum CMS configuration that minimizes
blind time while maximizing information density. Blind time is a non-linear metric based on a threshold
within a rolling window. This framework implements greedy algorithm for ease of implementation and
efficiency. Greedy is a simple optimization algorithm that makes locally optimal choices at each step,
hoping to find a near-globally optimal solution. While greedy algorithms are efficient and easy to
implement, they don’t guarantee the global optimal solution [17]. However, in this case, finding a
near-optimal solution will be sufficient.
The greedy algorithm evaluates all local options of the search grid nodes in the feasible region
at each step. By subtracting blind time from information density for all possible search grid node
combinations for a CMS with sensor density of p, the algorithm computes ‘marginal gain’, ϵ, the
difference between the information density and blind time as the optimization parameter. For every
iteration (e.g., every possible combination of p search grid nodes) the search grid node that maximizes
the marginal gain relative to the existing CMS detection matrix is selected. The total number of
iterations is equal to the number of sensors to be deployed at the site. In each iteration, the CMS
detection matrix is updated according to Equation 3 using all of the currently selected nodes. The
algorithm is summarized in Algorithm 1.

124 operational oil and gas facilities were selected across a wide range of emission source geometries,
atmospheric conditions, and associated geographical locations. Facility layout and site-specific historical atmospheric data were collected for each facility. Then the proposed framework was implemented
to determine near-optimal CMS configuration with varying network densities. On average, 15.3 source
groups exist in each facility, with the minimum and maximum numbers of 4 and 39 source groups
per facility, respectively. Several key metrics were calculated for the sensor network at each site, including information density, percentage blind time, and average time-to-detection. This section offers
the aggregated results of implementing the proposed framework using a CMS with 3 sensors in 124
operational oil and gas production facilities.


Algorithm 1 Sensor Placement Optimization Algorithm
1: Input: Detection matrices Sd,1 , Sd,2 , . . . , Sd,n
2: Input: Number of sensors p
3: Output: Optimized sensor network with minimized Blind Time and maximized Information Den-

sity
4: Initialize network sensitivity matrix N of size n × m to zeros
5: Initialize list of selected sensors SelectedSensors = []
6: for each iteration from 1 to p do
7:
Initialize BestSensor = None
8:
Initialize BestEpsilon = −∞
9:
for each sensor i in feasible region do
10:
Compute NewNetwork = N ∨ Sd,i

Compute ID using Equation 1 using NewNetwork
Compute BT using Equation 2 using NewNetwork
Compute ϵi = ID − BT
if ϵi > BestEpsilon then
BestEpsilon = ϵi
BestSensor = i
17:
end if
18:
end for
19:
Add BestSensor to SelectedSensors
20:
Update network sensitivity N = N ∨ Sd,BestSensor
21: end for
22: Return SelectedSensors
11:
12:
13:
14:
15:
16:

This section investigates the impact of sensor network density on information density and compares
primary findings to other studies. Most of this section focuses on a CMS with a set density of three
sensors, as our studies of relatively simple oil and gas facilities suggest that this configuration is
sufficient for achieving reasonable information density with minimal blind time. A sensitivity analysis
is conducted to assess the impact of varying sensor network density on blind time and information
gain.
While CMS sensing networks are designed for continuous measurement of the ambient concentration
of methane, they can experience “blind periods” when there is a lack of direct pollution transport from
source to sensor network due to unfavorable wind direction for a sustained period of time. Previous
studies [7, 19] have employed metrics similar to information density for the performance evaluation
of CMS sensor networks. These metrics often estimate the fraction of time a given CMS can detect
ongoing emissions.
As depicted in Figure 3, information density (in percentage form) increases as more sensors are
added to the network. As an expected trend, increasing sensor density in the network enhances spatiotemporal coverage, thereby increasing the likelihood of detecting ongoing emissions by covering a wider
range of wind directions. A near-linear relationship exists between sensor network density and percent
information, with marginal returns slightly diminishing for additional sensors beyond three. Globally,
the median percent information across sites for 1, 2, and 3 sensor networks is 8.2% 14.2% and 18.8%
respectively.
A median information density of 18.8% for a 3-sensor network indicates that, due to the relatively
sparse coverage of a 3-sensor network, it is unsurprising that the information density is correspondingly
low (as evidenced by the fact that roughly 4/5 of atmospheric measurements correspond to conditions
when the sensor network will not receive information about sources on a given facility). These results align with previous findings reported in [7], in terms of the achieved information density for a
recommended CMS density.
Note that the efficacy of CMS depends on a multitude of factors and cannot be solely assessed

by detection fraction or other metrics similar to information density. To evaluate the effectiveness of
CMS, it’s essential to consider the specific objectives, such as providing at least one valid quantification
estimate every 12 hours (as indicated in the OOOOb regulation), detecting all short emission events,
or simply having the capability to detect large persistent leaks sooner than a quarterly OGI inspection
would reveal them. Under different objectives, the placement and density of sensors should be adjusted
accordingly.
Chen, Kimura, and Allen [7] (hereafter referred to as the UT Austin study) defined temporal
coverage as the percent of time the network would detect a single emission source. This metric is
comparable to the information density used throughout our study. However, in the present work,
information density is calculated based on the temporal coverage across all emission sources. The UT
Austin study concluded that an eight-sensor network, with detection capabilities of 1 ppm at 20 meters
from the source, achieved 20% temporal coverage. These conclusions are consistent with the results of
the present work.
Comparing the results of the present work and the UT Austin study, five key differences exist in the
research objectives and methodologies: (i) The UT Austin study presents a theoretical implementation
of CMS in a simulation setup, evaluating the impact of source-sensor distance, number of sensors, and
detection thresholds on detection efficiency. In contrast, the present work focuses on the optimization
of CMS configuration across a wide range of conditions (including facility complexity, atmospheric
conditions, site geometry, and regional variations), to satisfy the relevant requirements defined in the
OOOOb regulation. (ii) The UT Austin study utilized CALPUFF, an open-source Gaussian Puff
dispersion model, to simulate concentrations. The present work employs the less computationally
intensive Gaussian Plume model to accommodate longer simulation periods for a larger number of
facilities. As stated before, the modularity of this framework offers flexibility in selecting concentration
simulation methods, among other aspects. (iii) The UT Austin study simulated a single emission
source with a release rate of 0.4 kg/hr, compared to the present study with multiple emission sources
located according to operational site layouts, each emitting 1.6 kg/hr. (iv) The UT Austin study
simulated a CMS grid with five concentric rings of eight sensors (intervals of 45◦ ), each at a distinct
distance from the source, ranging from 10 to 50 meters. The present work, in contrast, focuses
on optimizing CMS density and placement with varying source-sensor distances and realistic facility
geometries and associated constraints. While sensor sensitivity was a variable in their study, we
specifically compared our results to their analysis of the 1 ppm sensitivity threshold, which aligns
with our study’s detection threshold. Lastly, (v) Unlike the UT Austin study, which used 2-week

simulations, this study incorporates 16 weeks of simulations per year to comprehensively account for
seasonal variations in atmospheric conditions.
Comparable temporal coverage, despite varying sensor numbers, can be partially attributed to the
higher release rate used in this study (1.6 kg/h), which is four times that of the UT Austin study. This
increased release rate significantly enhances detection probability. A detailed analysis of the impact
of release rate on information density indicates that with a 0.4 kg/hr release rate, a 3-sensor network
achieves 8% information density. This falls between the UT Austin study results for sensor networks
at 30 and 40 meters, which have temporal coverage of 10.2% and 6.2% respectively. Further details
related to this subject are presented in the Supporting Information (Figure A.3).

This section introduces a new metric to evaluate CMS effectiveness, aligned with the OOOOb requirements for continuous monitoring technologies to ensure that facility-level emissions are quantified at
least once every 12 hours. We define percent blind time as:

Percent Blind Time =

Blind Time
w×m

× 100

where percent blind time is calculated as the fraction of 12-hour periods (in %) where a network
fails to detect at least 10 measurements from each source group, summed across all sources. Here,
w represents the total number of 12-hour rolling windows within the sampled data and m is the
number of source groups. Percent blind time effectively represents the probability that a CMS system
fails to provide a reliable quantification estimate within 12 hours, mainly due to insufficient direct
source-to-sensor atmospheric transport.
Note that wind direction is the primary factor influencing the detectability of elevated concentrations resulting from onsite release events. This highlights the role of site-specific wind data in
optimizing sensor placement to ensure that the elements of CMS collectively maximize overall detections and minimize long gaps in direct source-to-sensor signals.
Figure 4 illustrates variations in percent blind time as a function of the CMS density, based on all
124 example sites in the dataset. A non-linear relationship exists between sensor density and percent
blind time, with significant reductions in percent blind time achieved by adding the first few sensors.
For example, the mean percent blind time decreases from 35.5% for a 1-sensor network to 15.0% for
a 2-sensor network and 7.1% for a 3-sensor network, demonstrating a more than 2x reduction with
each additional sensor. This contrasts with the near-linear trend observed in percent information. It
indicates that for a CMS with 3 sensors, 7.1% of rolling windows in the detection matrices across all
sites have fewer than 10 positive detections per source group. This translates to a 92.9% probability
of a 3-sensor network providing a reliable quantification estimate within a 12-hour period.
These results may be somewhat surprising given the relatively low information density demonstrated in Section 3.1. This apparent discrepancy highlights an important distinction: even though
the probability of detecting emissions on a site with a 3-sensor network is relatively low, on average,
there is sufficient wind variability over 12-hour windows such that, with proper sensor placement, each
source on a site is observed 93% of the time.

Time-to-detection (TTD) represents the delay between the start of an emission event and the detection
of an elevated concentration associated with the event by the CMS. TTD is often used in studies as one
of the metrics to evaluate the performance of CMS. This metric quantifies the rapidity of direct sourceto-receptor pollution transport, which is influenced by favorable wind direction and the configuration
of the CMS.
This study employs a Monte Carlo approach to generate 10,000 random event start times for every


source group in each network detection matrix. The time elapsed between the selected event start
time and the next detection associated with that specific source group in the time series is recorded for
each Monte Carlo realization. Finally, the average time to detection is determined across all sources
and Monte Carlo iterations at each of the 124 operational facilities.
Figure 5 illustrates the relationship between mean TTD and CMS density. Similar to the blind time
analysis, the first few additional sensors significantly reduce mean TTD. For example, increasing from
1 to 2 sensors decreases mean TTD from 442.4 minutes to 164.0 minutes, a 64% reduction. Adding a
third sensor further reduces TTD to 82.3 minutes, with 99.1% sites in the study having a mean TTD
below 250 minutes.
The UT Austin study conducted a similar TTD analysis using a concentric ring of eight sensors
at a 10-meter distance from the source. They optimized sensor placement iteratively, selecting sensors
with the highest temporal coverage. The results of that study indicated mean TTDs of 567.1, 191.7,
and 107.6 minutes for 1, 2, and 3 sensors, respectively. The findings of this present work align with
their results, with the small differences in mean TTD which could be attributed to variations in sourcesensor positioning and release rate. Our optimized sensor placements and higher release rate likely
contribute to the slightly lower TTDs observed in this study. Despite these differences, both studies
consistently demonstrate that near-optimally configured 3-sensor CMS could achieve mean TTDs on
the order of 80 to 100 minutes.
To analyze the probability of detecting a continuous release of a given duration, the Monte Carlo
iterations were sampled to construct empirical cumulative distribution functions (CDF) for CMS with
varying densities (Figure 6). These CDFs illustrate the theoretical probabilities of TTD at various
CMS sensor network densities based on a large number of randomly selected release event start times.
The results indicate that on average, a 3-sensor network achieves a 90% POD within 3.7 hours, demonstrating that optimal CMS sensor placement can consistently achieve timely detections concerning the
OOOOb expectations.

As an illustrative example, this section presents a case study involving operational oil and gas facilities,
applying the proposed framework to determine near-optimal CMS configuration. An in-depth discussion of the near-optimal sensor placement algorithm and framework outputs is included. This section
also compares simulation framework results with direct on-site methane concentration measurement
data to assess agreement between simulated and measured blind time statistics.

Figure 7 illustrates a representative example of a facility layout, windrose plot, and optimized sensor
locations for a 3-sensor CMS. The numbered green dots indicate the order in which the optimization
procedure selected sensor locations. The windrose plot reveals that the first recommended sensor
location is downwind of a large group of emission sources. The second sensor is positioned on the
opposite side the sources, relative to the first sensor, capturing emissions from the second most common
wind direction. The algorithm positions the third sensor near an isolated source, demonstrating its
ability to prioritize new information over redundant detections.
Figure 8 illustrates network detection matrices for this site, showcasing the impact of varying sensor
network density. To enhance visual clarity, we focus on a single week of data instead of the full dataset
used throughout this study, transposed minute-level network detection matrices. In other words, each
row represents a source group’s timeseries of detections/nondetections, with black corresponding to a
detection of the source at a given time, and white corresponding to a non-detection. Time periods
that would contribute to blind time are highlighted with red rectangles.
Significant blind time periods are observed across all sources for a single-sensor network (red rectangles in each row). However, adding more sensors to the network substantially reduces blind time.
For the illustrated time period, the network’s percent blind time decreases from 52.8% for a 1-sensor
network to 5.8% and 2.6% for 2 and 3-sensor networks, respectively.
For the single-sensor network, the sparse detections (5% information density) indicate that the
dominant wind direction during this week was unfavorable. Adding a second sensor doubled information density and significantly reduced blind time from 52.8% to 5.8%. A third sensor further enhanced


A comparison of simulation results with actual on-site concentration measurement data was conducted to assess agreement between key simulated statistics (blind time, information density, timeto-detection) and statistics derived from the concentration timeseries directly measured by CMS at
operational facilities. Five operational oil and gas facilities with 3-sensor CMS were selected and six
months of ambient concentration data was collected from all sensors at each facility. These facilities
were chosen for their likelihood of continuous operational emissions from equipment like compressors,
minimizing temporal gaps in continuous release events.
The data across the sensors on each facility was aggregated by computing the maximum concentration from the sensor network at each minute. After subtracting facility-specific background methane
concentration levels, the same thresholding process used in the simulations was applied to the actual

measurements to define detections. The gap between detections (duration of continuous zeros) in the
network detection matrices was then calculated, representing the “operational blind times” during the
6-month measurement period.
It’s important to note that this analysis assumes continuous emissions from sources. If emissions are
intermittent, additional “blind periods” may occur due to operational breaks rather than the impact of
wind direction. This results in more conservative (an upper limit) estimates of operational blind time,
compared to the simulated results. The distribution of operational blind periods for the 5 operational
facilities is illustrated in Figure 9. These results demonstrate good agreement with the simulated cases.
The mean operational blind time was 1.7 hours (102 minutes), aligning closely with the mean
simulated TTD of 82 minutes across all sites. As stated before, intermittent operational emissions may
contribute to the higher blind time based on direct measurements. Although calculated differently, both
metrics effectively characterize the duration of gaps where the sensor network fails to detect sources
due to unfavorable wind direction. The strong agreement between simulated and measured blind times
for 3-sensor networks suggests that: (i) the proposed framework provides acceptable outputs related
to the determination of blind time for near-optimal CMS, and (ii) both simulated and operational
data indicate that a CMS with 3 sensors will detect changing emissions within, on average, 80 to 102
minutes. This result is in good agreement with the theoretical results from [7]. While site complexity
and other factors can influence these results, we expect these findings to be generally applicable to
facilities with comparable sizes and number of source groups.

To evaluate the performance of the proposed framework compared to the publicly available sources, a
comparison between Chama [19] (a sensor placement optimization tool based on mixed-integer linear
programming formulations) and the proposed framework (employing the greedy algorithm) was conducted across 15 randomly selected operational oil and gas production facilities. Using identical input
parameters, optimal 3-sensor CMS configurations were identified for the second calendar quarter, and
their performance was evaluated during the subsequent quarter. The atmospheric data used in this
comparison is consistent with that described in Section 2.1.1. As a modular framework, Chama allows
for external simulation data. For consistency, both Chama and the proposed framework use the same
Gaussian Plume model as the simulation method. By using separate quarters for optimization and
testing, we assess network performance on unobserved data. Note that the use of minute-level wind
data for one calendar quarter differs from the original Chama algorithm’s implementation, which could
potentially introduce inconsistencies in its results.
First, a set of metrics, including percent information, percent blind time, and mean time to detection, is used to compare the performance of the models for different CMS network densities (Figure 10).
Chama demonstrated relatively better performance in achieving higher percent information compared
to the proposed framework (Figure 10 (a)). The multi-objective optimization approach used in this
framework, which prioritizes both percent information increase and blind time reduction, contributes
to the observed performance difference. Figure 10 (b) and (c) illustrate key performance differences
between Chama and the proposed framework in terms of blind time and TTD. The proposed framework effectively reduces both blind time and TTD compared to Chama, albeit with a slight trade-off
in percent information. For 3-sensor networks, the proposed framework achieves a mean percent blind
time of 7.6% and a mean TTD of 81 minutes, outperforming Chama’s 16.7% blind time and 180 minutes TTD. This comparison demonstrates the effectiveness of the proposed framework in achieving
significantly lower blind time and TTD with slightly lower percent information.
To demonstrate why the proposed framework achieves lower blind time and TTD, we compare


the sensor locations resulting from Chama (red) and the proposed framework (green) for an example
facility (Figure 11). While Chama effectively maximizes information percent, it exhibits a bias towards
predominant wind directions. This bias can lead to extended blind periods when the wind deviates
from the prevailing direction. The proposed framework addresses this shortcoming by strategically
placing two sensors downwind of emission sources and a third sensor on the north-western side of the
facility. This configuration ensures that the CMS detects elevated concentrations under a wider range
of wind directions. This example highlights the impact of different objectives on CMS configuration
and resulting performance.

This study introduces a modular framework, employing forward dispersion modeling and a greedy
algorithm, to optimize the configuration of the continuous monitoring systems (CMS) using fixed-point
sensors. This framework aims to select near-optimum sensor placement for different CMS network
densities, specifically to meet the criteria outlined in the recently finalized EPA OOOOb rule, to
ensure that CMS are able to determine a valid methane mass emissions rate at least once for every
12-hour block. A multi-objective optimization approach is used to maximize information density and
minimize CMS blind time simultaneously. The framework was applied to 124 operational oil and gas
production facilities with a wide variety of site characteristics and meteorological conditions to evaluate
the effectiveness of CMS in detecting and quantifying emissions. Three metrics are considered for the
performance evaluation of CMS, including information gain, blind time, and time to detection.
Application of the framework to operational oil and gas production facilities indicates that on
average, 3-sensor CMS have an information density of approximately 20% meaning that one out of five
elements in the CMS detection matrix corresponds to a detection. However, systems rarely experience
prolonged periods of blind time, with an average blind time of 7.1% for a 3-sensor CMS. This indicates
that the probability of observing a 12-hour block without obtaining enough information to make a
reasonable rate estimate on a given source is 7.1%. The average time to detection for a 3-sensor CMS
is estimated to be 82.3 minutes, with all sites in the study having a mean time to detection below 250
minutes. These results demonstrate that, with proper CMS configuration and sufficient sensitivity, on
average, CMS can reliably detect emissions on 12-hour time blocks in 92.9% of cases, and also provide
alerts of anomalous emissions within less than 1.5 hours of the event starting time.
These results demonstrate the critical nature of optimizing sensor placement for the specific objective of the system. If, for instance, the system is required to provide a quantified rate every 12


hours, then as demonstrated, a metric such as blind time must be considered in the sensor placement
optimization process. This study demonstrates that the proposed framework using multi-objective
optimization processes can result in improved CMS performance, which is essential for meeting the
criteria outlined in the EPA OOOOb rule.
