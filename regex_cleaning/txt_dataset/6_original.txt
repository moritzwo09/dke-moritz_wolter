A Framework for Optimizing Continuous Methane Monitoring
System Configuration for Minimal Blind Time: Application and
Insights from over 100 Operational Oil & Gas Facilities
Noah Metzger1 , Ali Lashgari1* , Umair Ismail1 , David Ball1 , and Nathan Eichenlaub1
1

*

Project Canary, Denver CO
Corresponding author: ali.lashgari@projectcanary.com
October 2024
Abstract

Continuous monitoring systems (CMS) that utilize fixed-point sensors provide high temporal
resolution point-in-space measurements of ambient methane concentration. This study introduces
a modular framework for optimizing CMS configurations, encompassing sensor density (number
of sensors) and near-optimal placement. By introducing a metric called ‘blind time’, this study
attempts to capture periods where the network fails to make detections that could satisfy the
regulatory requirement of quantifying emissions every 12 hours. This framework is then applied
to 124 operational oil and gas production facilities with a wide variety of site characteristics and
meteorological conditions. This study determines a representative blind time for near-optimum
CMS configurations for operational facilities and then investigates the impact of different sensor
network densities on the performance of the CMS. The results demonstrate that 3-sensor networks,
when placed in near-optimum arrangements, can achieve blind time of less than 10% and a mean
time to detection of approximately 82 minutes.

1

Introduction

The role of methane, as a short-lived greenhouse gas with high heat-trapping potential, in global
warming is undeniable. Given these characteristics, mitigating methane emissions is one of the most
effective and efficient strategies for curbing short-term global warming. The energy sector, particularly
the oil and gas industry, is a significant contributor to human-caused methane emissions. Many oil
and gas methane reduction strategies are cost-effective or even profitable, as captured gas can often be
directed to pipelines and sold for additional profit, making oil and gas methane mitigation a financially
viable option.
Gas releases from the oil and gas industry can be categorized as venting (releases of gas for known
and planned reasons), fugitive (unintended releases or leaks), and incomplete combustion emissions
(combustion slip). Accurate interpretation of oil and gas emissions profile requires considering source
types, event timing and intermittency (duration and frequency), release rate, and spatial emission
source distribution. This underscores the critical need for timely and accurate detection and localization of emission events, reliable quantification of gas release rate, and swift mitigation actions.
A combination of measurement technologies is often recommended for effective detection and quantification of methane emissions events [1–3]. Satellite measurements can remotely detect large-scale
methane emissions occurring at the time of observation, enabling independent oversight and enhancing transparency, particularly in understudied regions with limited site access. However, satellite
surveys have high detection thresholds and limited overflight frequency. Aerial platforms, including mass-balance piloted aircraft and uncrewed aerial vehicle (UAV) surveys, offer lower detection
thresholds compared to satellites, but are susceptible to spatial (below the lowest safe altitude for
1
https://doi.org/10.26434/chemrxiv-2024-q844g ORCID: https://orcid.org/0000-0002-5174-8882 Content not peer-reviewed by ChemRxiv. License: CC BY-NC 4.0

flying for aircraft, and above maximum flying height for UAVs) and temporal extrapolation errors.
Other aerial platforms, including downward-looking laser methane column measurements, offer more
accurate plume detection and source localization. However, uncertainties in detection and flux rate
quantification are significantly influenced by factors such as flight altitude and wind speed.
In general, snapshot emission measurement methods, including vehicle-based, aerial, and satellite
measurements, often encounter limitations due to the lack of site-specific meteorological data and
information on emission events timing (e.g., duration and frequency of intermittent events). This
shortcoming is particularly problematic for emission estimations at complex and congested sites, such
as midstream compressor stations.
Continuous monitoring systems using fixed-point sensors (hereafter referred to as CMS) are a cornerstone of multiscale methane emissions measurement approaches, providing site-specific emissions
knowledge. A CMS consists of one or more sensing units strategically placed at a safe distance from
potential emission sources to continuously measure ambient pollution concentration levels. Combined
with on-site meteorological measurements, this data can enable the detection, timing, localization, and
quantification of emission events. Key benefits of the CMS include (i) rapid detection of emissions ranging from relatively low rates to super emitting events, (ii) capture of both short-duration/intermittent
and continuous events (iii) accurate time-bounding of intermittent emission events, and (iv) complimenting other measurement methods using a continuous stream of site-specific data on emissions and
meteorology.
To realize the full potential of CMS, reliable ambient concentration and meteorological measurements (hardware capabilities) should couple with algorithms capable of transforming raw data into
accurate insights related to emission event timing, location, and magnitude (data analytics). The
optimal CMS configuration, including the number of sensing units (sensor network density) and placement (finding optimum locations for fixed point-sensor installations, from a large number of candidate
locations to satisfy a given objective function), is another key factor influencing the performance of
CMS systems. Operational records and knowledge, such as parametric information and inspection
records, can complement CMS measurements and enhance the differentiation between intended and
unintended emission events, providing actionable insights.
Performance evaluation of continuous methane monitoring systems has been the subject of several
studies in recent years. Single-blind testing at controlled release facilities, such as Colorado State
University’s Methane Emissions Technology Evaluation Center (METEC) [4] and Stanford University’s testing facility in Casa Grande, Arizona [5], simulates challenging yet simplified field conditions
to assess the leak detection and quantification capabilities of various CMS solutions. These studies demonstrate the rapid evolution of CMS. Recent METEC single-blind testing, conducted using
the Advancing Development of Emissions Detection (ADED) protocol, revealed that one of the topperforming solutions achieved a 90% probability of detecting 0.5 kg/hr emissions while accurately
localizing releases in 98.7% of cases, with a mean quantification error of -0.064 kg/hr [6]. This results
in a cumulative emissions quantification error of 2.8% when comparing the overall quantified methane
release amount to the actual methane release over the course of the ADED study. These studies investigate the performance of CMS assuming that the solution provider has employed an optimum network
configuration in its deployment. It is essential to emphasize that effective monitoring relies not only on
measurement hardware and emissions analytics capabilities but also on optimum CMS configuration.
In other words, the 2024 ADED results indicate that CMS systems are capable of achieving these
performance levels, with optimal sensor network configurations, including appropriate sensor density
and proper placement configuration, considering facility-specific variables.
A well-configured CMS is expected to frequently reflect variations in ambient methane concentration
levels caused by site-level emissions. Two primary characteristics of a well-configured CMS are proper
sensor network density (considering the limited sensor budget) and optimum placement of sensors in
the oil and gas facility. This multi-objective optimization network design problem aims to maximize
signal coverage (in this context, ‘signal’ refers to ambient methane concentration levels reflecting the
presence or absence of an on-site gas release event), ensuring that for a maximum amount of time,
fluctuations in ambient methane concentration levels due to on-site gas releases are captured by the
CMS. These characteristics enhance rapid detection, time-bounding, localization, and quantification
of emission events using subsequent analyses.

2
https://doi.org/10.26434/chemrxiv-2024-q844g ORCID: https://orcid.org/0000-0002-5174-8882 Content not peer-reviewed by ChemRxiv. License: CC BY-NC 4.0

Many factors can influence near-source pollutant transport and consequently, signal delivery from
the release point(s) to sensors. The primary factors include wind direction, wind speed, atmospheric
stability class, facility setup, surface elevation profile, on-site airflow obstructions, source-receptor
distance, plume release height, receptor height (specifically the intake air height for each sensor), nearby
offsite emission sources, and for specific sources (e.g., combustors) gas velocity and temperature.
Ideally, a CMS sensor network configuration algorithm should consider all relevant factors. However, such an optimization algorithm can be computationally intensive and impractical with limited
data access and operational constraints. A more practical approach involves reducing the complexity of
this problem by focusing on the most significant factors. This study aims to present a framework that
incorporates some of the key factors into the CMS sensor network configuration process. Although all
of the factors are not considered in this framework, this offers an initial and foundational step toward a
more comprehensive, data-driven approach to CMS configuration, considering a wider range of factors
in decision-making.
Reliable emissions event detection requires a CMS configuration (sensor network density and placement) that maximizes the frequency of direct pollution transport from source to sensor. In this context,
a new metric, “blind time” is introduced in this work, representing the fraction of 12-hour periods with
insufficient data to enable accurate emissions quantification due to limited direct pollution transport
from sources to the sensor network. The definition of blind time is aligned with the EPA’s criteria for
CMS to be considered as an alternative test method under the recently established OOOOb regulation
to reduce methane emissions from the oil and natural gas industry. This regulation allows for the
use of alternative test methods, including CMS, to assess compliance with emission standards, if the
following requirements, among others, are satisfied the specific measurement solution is designated as
an approved alternative test method by the EPA: (i) “... continuous monitoring means the ability of a
methane monitoring system to determine and record a valid methane mass emissions rate or equivalent
of affected facilities at least once for every 12-hour block”, and (ii) the 90-day rolling average actionlevel is 1.6 kg/hr (3.6 lb/hr) of methane over the site-specific baseline emissions” and “The 7-day
rolling average action level is 21 kg/hr (46 lb/hr) of methane over the site-specific baseline emissions.”
Building upon these regulatory developments, optimizing sensor placement becomes even more critical
to meet the requirements outlined by the EPA.
Previous studies have explored various aspects of methane emissions from oil and gas operations,
including sensor placement optimization, and performance evaluation of CMS. The following literature
review synthesizes key findings and identifies areas for further research. Chen, Kimura, and Allen
[7] employed dispersion modeling to develop robust detection limit definitions, for evaluating CMS
performance characteristics based on a short timeframe of historical data (2 weeks) and a single
emission source. This study offered a comparison of simulations to controlled release testing results for
CMS. This study showed that the performance of CMS and their minimum detection limits expressed
as an emission rate, depend on meteorological conditions, facility-specific emissions characteristics,
sensor placement, and the amount of time allowed for the CMS to detect an emission source (time
to detection). This study demonstrated that a single sensor with a detection limit below 2 ppm,
positioned 10 meters from a 0.4 kg/h source, could detect the emission within 12 hours. Deploying
three sensors resulted in a lower detection time of 6 hours. While increasing the number of sensors
from 4 to 8 slightly improved detection time, the most significant reduction occurs between 1 and 4
sensors. Enhancing sensor sensitivity or adding more sensors generally decreases the average time to
detection within the sensor network.
An earlier study by Chen et al. [8] evaluated the performance of CMS on a regional spatio-temporal
simulation domain that incorporates multiple facilities, indicating that based on atmospheric dispersion
simulations of 26 oil and gas production sites in the Permian Basin in Texas, fifteen sensors, each
capable of detecting concentration increases of 1 ppm, successfully identified emissions at all 26 sites
during four weeks of meteorological episodes, assuming continuous emissions of 10 kg/h.
Sensor placement optimization falls under the broader category of combinatorial optimization,
aiming to find the best solution from a finite set of possibilities. These inherently complex problems
arise in a variety of real-world scenarios, including sensor placement for environmental monitoring
in ocean observation [9, 10], water contamination detection [11], hazardous gas leakage monitoring
in chemical plants [12], pipeline leaks [13–15] and air quality monitoring [16]. Depending on the

3
https://doi.org/10.26434/chemrxiv-2024-q844g ORCID: https://orcid.org/0000-0002-5174-8882 Content not peer-reviewed by ChemRxiv. License: CC BY-NC 4.0

application and objectives, a wide range of methods have been employed to achieve optimum sensor
placement, including heuristic methods like greedy algorithms [13, 14, 17], genetic algorithms [18],
deterministic approaches like mixed integer programming [11, 19], and decomposition techniques [10,
16].
In the context of CMS for methane emission monitoring, several optimization methods have been
employed to determine the best sensor placement and evaluate their performance in a simulated environment. Klise et al. [19] developed an open-source Python package, called Chama, employing
atmospheric dispersion modeling and mixed integer linear programming (MILP) to determine optimum sensor locations and detection thresholds to maximize emission event detection. The algorithm
offered in this work formulates and solved maximum-coverage and minimum-impact problems in two
steps, where first methane emission dispersion was modeled using given leak rates, source locations,
and wind variables. Then, sensor placement problem for a given sensor network density was solved
using MILP. The application of Chama was showcased in the form of a simulated case study in their
publication. In our study, we use Chama as an open-source benchmarking tool, offering a comparative
analysis of Chama and our proposed framework (Section 5).
Zi et al. [20] attempted to address challenges in sensor placement optimization using stochastic
programming by introducing a distributionally robust optimization formulation of sensor placement
under the uncertainty of wind conditions. Leveraging MILP for optimization, this study demonstrated
significant improvements to Chama, minimizing the detection time expectation, with a particular focus
on worst-case scenarios.
Liu et al. [21] investigated the optimal sensor placement problem in a 2-dimensional domain using
bi-level optimization. This algorithm used inverse modeling to estimate emission rates and determine
the optimal sensor placement by minimizing the overall mean squared error of the estimated emission
rates over various wind conditions. To solve the sensor placement problem, this study investigated the
repeated sample average approximation and the stochastic gradient descent-based bi-level approximation methods. Numerical examples were used to illustrate the application of the proposed method.
The authors noted that the sensor allocation problems in the 3-dimensional space (representing emission source and sensor height) will be more challenging, as it requires incorporating surface terrain
modeling and computationally efficient algorithms.
Jia, Sorensen, and Hammerling [18] presented an open-source modular framework using the Gaussian puff dispersion model and genetic algorithm to solve the CMS sensor placement optimization
problem in a 3-dimensional domain, considering sensor budget limitation and detection efficiency.
This publication offered case studies implementing the proposed framework for determining optimum
sensor placements for CMS including 4 and 8 sensors. These case studies were implemented in both
within-the-property-boundary and fenceline placement fashions. Results of this study indicated significant improvement in detection coverage, compared to a greedy search method. While a 3-dimensional
domain offers theoretical advantages, practical considerations, such as the implementation challenges,
occupational safety considerations, and costs of installing sensors at various heights, often limit its applicability in real-world scenarios. Additionally, the resource-intensive nature of employing Gaussian
puff models for simulation, genetic algorithms for optimization, and a fine grid size as candidate locations for sensor placement results in high computational demands, which impose practical constraints
and hinder the at-scale implementation of this framework in real-world scenarios.
Typically, sensor optimization frameworks and case studies follow a structured approach, incorporating the following steps with minor variations: (a) define emission source locations and potential
sensor locations, (b) use historical wind data in tandem with a forward dispersion model to simulate
methane concentrations, (c) identify optimal sensor locations through various approaches, and (d)
evaluate the detection capabilities of CMS.
Building upon the the existing knowledge from previous studies on the optimization of the CMS
configuration, the present study aims to take a more practical approach to this problem, by focusing
on the performance capabilities of CMS installed in 124 real-world operational oil and gas facilities,
to draw more general conclusions about the detection capabilities of CMS across a wide variety of
geographic regions or facility designs. In the current work, we explore a greedy sensor placement
framework for CMS at oil and gas facilities specifically designed to meet the criteria outlined in the

4
https://doi.org/10.26434/chemrxiv-2024-q844g ORCID: https://orcid.org/0000-0002-5174-8882 Content not peer-reviewed by ChemRxiv. License: CC BY-NC 4.0

recently finalized EPA OOOOb regulation, to study CMS capabilities to meet the criterion of 12-hour
facility-level quantification.
This study proposes a modular framework employing forward dispersion modeling and a greedy
optimization algorithm to determine the best CMS configuration (network sensor density and nearoptimum placement) and consequently determine several CMS performance metrics, including information gain, network blind time, and time to detection. An extensive real-world evaluation effort is
offered to investigate the performance of the proposed framework across 124 oil and gas operational
facilities. The overarching objective of this study is to investigate whether, based on the data from operational facilities, the proposed framework and CMS are capable of consistently detecting emissions of
a given level. To the best of our knowledge, this is the first study to rigorously test a sensor placement
framework across a wide variety of real-world operational facilities and emission sources. Another novelty of this work lies in its purposeful design of the modular framework and metrics around regulatory
requirements. Using a wide variety of real-world upstream oil and gas facilities for model evaluation,
this study determines sensor network blind time based on the site-specific meteorological data. As part
of a benchmarking study, a smaller operational dataset is processed by both the open-source Chama
model and the proposed framework. The findings of this study will inform future efforts related to
the understanding of the capabilities and limitations of CMS related to continuous emissions measurement and quantification. It should be noted that performance evaluation of emissions quantification
algorithms is beyond the scope of this study and will be discussed in a future publication.

2

Methodology

This modular framework optimizes the CMS configuration and consequently determines sensor network blind time. Three primary steps are included in this framework: (i) input data processing and
receptor grid determination, (ii) simulation of emissions using a forward dispersion model of choice,
and (iii) optimization of CMS configuration using a greedy algorithm to determine the best sensor
placement configuration for various sensor network densities. Figure 1 provides a schematic view of
this framework. The algorithm is designed to optimize the performance of CMS in light of 40 CFR
§60.5398b(c), meaning that CMS performance is characterized by the network’s ability to consistently
detect new information from emission sources over 12-hour periods.
Three metrics are considered for the performance evaluation of CMS, including information gain,
blind time, and time to detection. By introducing a metric called blind time, this study attempts to
capture periods where the network fails to make detections that could satisfy the regulatory requirement of generating valid emissions quantification every 12 hours. By applying this framework to 124
operational oil and gas production facilities with a wide variety of site characteristics (e.g., layout and
complexity) and meteorological conditions, this study attempts to determine a representative blind
time for near-optimum CMS configurations for operational facilities and then investigate the impact
of different sensor network densities on the performance of the CMS. The following subsections outline
the key components of the proposed framework, including data (input data processing and receptor
grid determination), simulation (forward dispersion modeling of emissions), and optimization (determining the best sensor placement for CMS configuration using a greedy algorithm for various sensor
network densities).

2.1

Data

2.1.1

Input Data

This framework leverages site-specific data to accurately simulate real-world conditions from operational facilities. Three sets of input data are imported into the framework, including facility layout,
facility-specific atmospheric data, and financial constraints that may impose limitations on the sensor
network budget. This section outlines data sources for facilities with existing site-specific data and
provides alternative resources for gathering data in the absence of such programs.
Facility layout data includes site boundaries, on-site equipment inventory, and high-precision spa5
https://doi.org/10.26434/chemrxiv-2024-q844g ORCID: https://orcid.org/0000-0002-5174-8882 Content not peer-reviewed by ChemRxiv. License: CC BY-NC 4.0

Figure 1: CMS Configuration Framework Flow Chart
tial information (e.g., equipment locations, obstacles, and dimensions). The facility boundary is used
to define the feasible region for CMS sensor placement. Equipment inventory aids in understanding
site complexity and ensuring all potential emission sources are accurately represented in the framework. High-precision spatial data, such as aerial site imagery is used to identify equipment locations,
obstacles, equipment dimensions, and the location of potential release points. It is crucial for defining
the search grid (for optimizing sensor placement), emissions simulation, and optimization processes.
In the application of the CMS configuration framework involving operational oil and gas facilities,
high-resolution aerial imagery of all 124 facilities is collected and used to define those features. However, obtaining precise spatial data without such imagery can be challenging, potentially impacting
emissions simulation and optimization accuracy.
Facility-specific atmospheric data includes minute-averaged samples of wind speed and wind direction. This framework uses site-specific meteorological data collected over the course of a year to
capture seasonal variability in the optimization of the CMS configuration. For greenfield projects
without site-specific meteorological data, wind characteristics from nearby facilities, or the NOAA’s
real-time 3-km resolution, hourly updated wind characteristics from the High-Resolution Rapid Refresh
6
https://doi.org/10.26434/chemrxiv-2024-q844g ORCID: https://orcid.org/0000-0002-5174-8882 Content not peer-reviewed by ChemRxiv. License: CC BY-NC 4.0

(HRRR) model can be used. To balance computational efficiency and data representation, the application of the CMS configuration framework optimization involving operational oil and gas facilities
uses four weeks from each season instead of a full year of meteorological data. This approach captures seasonal variability while reducing computational burden, enabling minute-based concentration
simulations and ensuring that the results are not biased toward the wind characteristics of a specific
season. A brief examination of how the wind speed and the atmospheric stability classes (ASC) vary
across the different basins is presented in section A.2. Further determination of the optimal amount
of meteorological data is case-dependent and beyond the scope of this paper.
Plume dispersion patterns are known to be more complex in the presence of low wind speed.
This condition negatively impacts the capability of pollution transport models to accurately estimate
concentration levels at given locations. As a result, this study adopts a common practice of excluding
time windows with wind speeds U lower than 0.5 m/s. An optimization/test approach is employed
to assess model performance. This study uses 75% of the meteorological data (12 weeks) for sensor
placement optimization, while the remaining 25% of the data (4 weeks) serves as the test set, allowing
a blind evaluation of out-of-sample data. The 75/25 split aligns with common practices and ensures
proper training while the model’s generalizability is assessed using unseen data.
Facility-specific meteorological one-minute intervals of average wind speed and direction data were
collected using Gill 2D ultrasonic anemometers or RM Young Ultrasonic Anemometers. In addition to
facility layout and site-specific atmospheric data, operator-provided information on equipment inventory and monitoring budget constraints is crucial. Insights such as operator input regarding equipment
inventory (for the labeling of potential emission sources) and additional sensor placement constraints
(for the designation of infeasible sensor installation area) are used to further enhance this framework.
2.1.2

Input Data Processing

Input data processing involves source clustering and defining feasible sensor installation regions. Facility layout and operator-provided information are used to label potential emission sources associated
with on-site equipment. To reduce the dimensionality of the optimization, agglomerative clustering
with a 15-meter spatial threshold is employed to define spatially distinct equipment groups, with each
equipment group representing all the potential emission sources located within its 15-meter distance.
This approach avoids bias towards equipment groups with dense potential emission points.
Following source group definition and characterization, a search grid is established for potential
sensor locations. The grid boundaries are constructed by extending the minimum and maximum UTM
X,Y coordinates of the set of emission sources by 30.8 meters. This results in 4 vertices that define the
search grid area and will be referred to as the feasible region. Grid size impacts the resolution of the
sensor placement optimization problem. A 10x10 meter mesh grid is overlaid on the feasible region to
define search grid nodes (potential sensor locations). This resolution is chosen to adequately capture
spatial variations in emission concentration while having reasonable processing times.
A 15.4-meter safety buffer is established around the equipment groups, defining a buffered hull
that represents infeasible regions (areas where sensor installation is prohibited). Additional operatorprovided information is incorporated to account for other operational restrictions when defining infeasible regions. Examples of such operational restrictions could be inside-facility driving paths and areas
preserved for future equipment installation or other purposes. The feasible region is further refined by
removing search grid nodes that fall within the infeasible region.
Figure 2 illustrates this process with an example. This modular framework allows for flexibility in
modifying selected values of a range of variables, including grid size and the distance used to define
the outer limit of the feasible sensor installation region.

7
https://doi.org/10.26434/chemrxiv-2024-q844g ORCID: https://orcid.org/0000-0002-5174-8882 Content not peer-reviewed by ChemRxiv. License: CC BY-NC 4.0

Figure 2: Example facility showing emission sources (blue), clustered sources (red), and the feasible
region for potential sensor locations (orange).

2.2

Simulation

2.2.1

Forward Dispersion Modeling

A forward dispersion modeling tool is employed, utilizing historical atmospheric measurements to
simulate pollution transport for plumes originating at the clustered source location. A simulated
plume will inform the expected atmospheric concentration levels originating from a specific source at
each search grid node at any given time. Current work employs a Gaussian plume model to simulate
pollution dispersion for the 124 operational oil and gas production facilities. However, other pollution
transport models, such as Gaussian puff can be used for this simulation. Our previous studies have not
indicated significant improvement in the simulation accuracy as a result of employing more complex
models like Gaussian puff for relatively simple site setups such as simple upstream oil and gas facilities.
The OOOOb rule mandates a response when the 90-day rolling average exceeds the action threshold
of 1.6 kg/h above facility baseline. This algorithm is designed to optimize the performance of CMS
in light of OOOOb. As a result, continuous emissions are simulated with a release rate of 1.6 kg/hr
for each emission group. This approach reflects operational conditions where most of the leaks in the
real world often persist until repairs are implemented. An in-depth sensitivity analysis of the model
performance across various release rates is offered in the supporting document (subsection A.3).
2.2.2

Sensitivity Matrix

A source-sensitivity matrix S of size (n,m) is generated for every search grid node. Each row (indexed
by i) represents a minute of atmospheric methane concentration (totaling n minutes of simulation)
and each column (indexed by j) represents a given source group (totaling m source groups). The i, jth
element of the source-sensitivity matrix represents the simulated atmospheric concentration value observed at the location of the search grid node, resulting from a release rate of 1.6 kg/hr from source j
during minute i. Two sensitivity matrices are constructed for each search grid node, representing training and testing simulation data, hereafter referred to as “in-sample” and “out-of-sample”. In-sample
and out-of-sample sensitivity matrices include 12 and 4 weeks of data, corresponding to 120,960 and
40,320 minutes of simulated atmospheric concentrations per sensor per source, respectively. It corresponds to a total of more than 305 million minutes of simulation across all 124 oil and gas operational
facilities. This calculation is applied to every potential sensor, generating two unique sensitivity matrices, in-sample and out-of-sample, for each search grid node located in the feasible region. This results
in approximately 47 billion simulated concentration values captured in the sensitivity matrices.

8
https://doi.org/10.26434/chemrxiv-2024-q844g ORCID: https://orcid.org/0000-0002-5174-8882 Content not peer-reviewed by ChemRxiv. License: CC BY-NC 4.0

2.2.3

Detection Matrix

In this study, detection is defined as an increase in atmospheric concentrations from a given source
exceeding 1 ppm. This binary approach avoids optimization biases towards signal strength. This study
assumes the use of tunable diode laser absorption spectroscopy (TDLAS) sensors, with a sensitivity
of 0.4 ppm [22]. Therefore, the 1 ppm threshold provides a conservative detection level, 2.5 times the
hardware sensitivity. By equally weighting concentrations greater than 1 ppm, we transfer the problem
into a binary dimension (detection vs. non-detection), simplifying it to the presence or absence of a
detectable signal.
For each search grid node, each element in the sensitivity matrix with a value greater than 1
ppm indicates an elevated atmospheric concentration detected at the specific search grid node, where
the increase in atmospheric concentration results from a 1.6 kg/h release originating from source j
and is detectable by a sensor during minute i. A threshold is applied to convert continuous concentration values (sensitivity matrix) into binary detection values in a node-specific detection matrix,
Sd . Each element i,j in the detection matrix is binary, with 1 indicating detection and 0 indicating
non-detection. This binary element indicates detection status at the specific search grid node, during
minute i, resulting from source group j.

2.3

Optimization

2.3.1

Objective Functions

This framework employs a multi-objective optimization approach to solve CMS sensor network configuration problem. This approach optimizes the network configuration based on two key metrics: information density and blind time. By maximizing information density, the framework ensures that the
exposure of the CMS to the elevated concentrations originating from onsite release events is maximized.
Simultaneously, minimizing blind time helps prevent long periods where sources remain undetected by
the CMS.
The information density for every single search grid node is calculated as the fraction of a given
detection matrix that is nonzero. It represents the fraction of the time a sensor located at a given
search grid node detects emissions from any onsite release event. Information density (ID) is calculated
as:
Pn
ID =

i=0

Pm

j=0 Si,j

n×m

.

(1)

For a CMS with p sensors, the overall information density is calculated based on a ‘network detection
matrix’, a binary matrix formed by applying the boolean logical or operator across the node-specific
detection matrices associated with the p search grid nodes. The binary nature of the CMS detection
matrix prevents double-counting of events detected using multiple sensors simultaneously. This metric
is similar to those used in other studies on sensor location optimization [18, 19].
Since information density does not consider temporal variability in detections, relying solely on
this metric for CMS configuration can lead to extended periods where no sources are observed by the
network. To fully leverage the continuous nature of the CMS measurements, incorporating a metric
that minimizes the characteristic timescale of “gaps” between detections is essential. This paper
introduces a new metric, blind time, to minimize such gaps between detections.
The term blind time notionally refers to the time period over which the CMS, on average, does not
register enough ‘information’ to enable a sufficiently accurate estimate of the site-level emissions. It
is a function of several factors including sensor count, placement, and the modeling approach among
others. An ensemble average of the blind time is obtained by sampling over a set of similar operational
oil and gas facilities. Within the present context, a 12-hour rolling window is used to segment the
simulated time horizon following the OOOOb rule requirements. On a per-source-group basis (i.e., for
each column of S), the CMS network is assumed to be operating in the blind for the 12-hour duration

9
https://doi.org/10.26434/chemrxiv-2024-q844g ORCID: https://orcid.org/0000-0002-5174-8882 Content not peer-reviewed by ChemRxiv. License: CC BY-NC 4.0

highlighted by the rolling window if there are fewer than 10 non-zero entries in the relevant section
of the sensitivity matrix S corresponding. This count of the 12-hour-long time periods identified as
blind intervals is aggregated for each potential source group to estimate a cumulative count of blind
intervals (BT) for each site. Mathematically, this can be expressed as:
P
m X
w 
X
1, if P Qk ≤ 10
Blind Time (BT) =
.
0, if
Qk > 10

(2)

j=1 k=0

Here, the number of rolling 12-hour interval covering a simulation period n minutes long is determined via w = n − (12 × 60) + 1. Qk represents a subset of N over the window k for column j,
where N is defined below. The CMS detection matrix, N , is simply the boolean logical or operator
(∨) applied element-wise across the p search grid nodes that are selected for the placement of the
individual sensors. This is mathematically expressed as:
N = Sd,1 ∨ Sd,2 ∨ ... ∨ Sd,p

(3)

The CMS detection matrix is initialized as a matrix of size (n, m), where n is the number of minutes
in the sample data and m is the number of source groups at the given site. This matrix indicates
the detection status achieved using a given CMS of p sensors. A sensitivity analysis, detailed in A.1,
informed the decision to use at least 10 nonzero entries in N to classify a 12-hour interval for each
source group as either blind or detected.
2.3.2

Optimization Method

As noted, the objective of this framework is to find a near-optimum CMS configuration that minimizes
blind time while maximizing information density. Blind time is a non-linear metric based on a threshold
within a rolling window. This framework implements greedy algorithm for ease of implementation and
efficiency. Greedy is a simple optimization algorithm that makes locally optimal choices at each step,
hoping to find a near-globally optimal solution. While greedy algorithms are efficient and easy to
implement, they don’t guarantee the global optimal solution [17]. However, in this case, finding a
near-optimal solution will be sufficient.
The greedy algorithm evaluates all local options of the search grid nodes in the feasible region
at each step. By subtracting blind time from information density for all possible search grid node
combinations for a CMS with sensor density of p, the algorithm computes ‘marginal gain’, ϵ, the
difference between the information density and blind time as the optimization parameter. For every
iteration (e.g., every possible combination of p search grid nodes) the search grid node that maximizes
the marginal gain relative to the existing CMS detection matrix is selected. The total number of
iterations is equal to the number of sensors to be deployed at the site. In each iteration, the CMS
detection matrix is updated according to Equation 3 using all of the currently selected nodes. The
algorithm is summarized in Algorithm 1.

3

Results and Discussion

124 operational oil and gas facilities were selected across a wide range of emission source geometries,
atmospheric conditions, and associated geographical locations. Facility layout and site-specific historical atmospheric data were collected for each facility. Then the proposed framework was implemented
to determine near-optimal CMS configuration with varying network densities. On average, 15.3 source
groups exist in each facility, with the minimum and maximum numbers of 4 and 39 source groups
per facility, respectively. Several key metrics were calculated for the sensor network at each site, including information density, percentage blind time, and average time-to-detection. This section offers
the aggregated results of implementing the proposed framework using a CMS with 3 sensors in 124
operational oil and gas production facilities.

10
https://doi.org/10.26434/chemrxiv-2024-q844g ORCID: https://orcid.org/0000-0002-5174-8882 Content not peer-reviewed by ChemRxiv. License: CC BY-NC 4.0

Algorithm 1 Sensor Placement Optimization Algorithm
1: Input: Detection matrices Sd,1 , Sd,2 , . . . , Sd,n
2: Input: Number of sensors p
3: Output: Optimized sensor network with minimized Blind Time and maximized Information Den-

sity
4: Initialize network sensitivity matrix N of size n × m to zeros
5: Initialize list of selected sensors SelectedSensors = []
6: for each iteration from 1 to p do
7:
Initialize BestSensor = None
8:
Initialize BestEpsilon = −∞
9:
for each sensor i in feasible region do
10:
Compute NewNetwork = N ∨ Sd,i

Compute ID using Equation 1 using NewNetwork
Compute BT using Equation 2 using NewNetwork
Compute ϵi = ID − BT
if ϵi > BestEpsilon then
BestEpsilon = ϵi
BestSensor = i
17:
end if
18:
end for
19:
Add BestSensor to SelectedSensors
20:
Update network sensitivity N = N ∨ Sd,BestSensor
21: end for
22: Return SelectedSensors
11:
12:
13:
14:
15:
16:

3.1

Sensor Network Density

This section investigates the impact of sensor network density on information density and compares
primary findings to other studies. Most of this section focuses on a CMS with a set density of three
sensors, as our studies of relatively simple oil and gas facilities suggest that this configuration is
sufficient for achieving reasonable information density with minimal blind time. A sensitivity analysis
is conducted to assess the impact of varying sensor network density on blind time and information
gain.
While CMS sensing networks are designed for continuous measurement of the ambient concentration
of methane, they can experience “blind periods” when there is a lack of direct pollution transport from
source to sensor network due to unfavorable wind direction for a sustained period of time. Previous
studies [7, 19] have employed metrics similar to information density for the performance evaluation
of CMS sensor networks. These metrics often estimate the fraction of time a given CMS can detect
ongoing emissions.
As depicted in Figure 3, information density (in percentage form) increases as more sensors are
added to the network. As an expected trend, increasing sensor density in the network enhances spatiotemporal coverage, thereby increasing the likelihood of detecting ongoing emissions by covering a wider
range of wind directions. A near-linear relationship exists between sensor network density and percent
information, with marginal returns slightly diminishing for additional sensors beyond three. Globally,
the median percent information across sites for 1, 2, and 3 sensor networks is 8.2% 14.2% and 18.8%
respectively.
A median information density of 18.8% for a 3-sensor network indicates that, due to the relatively
sparse coverage of a 3-sensor network, it is unsurprising that the information density is correspondingly
low (as evidenced by the fact that roughly 4/5 of atmospheric measurements correspond to conditions
when the sensor network will not receive information about sources on a given facility). These results align with previous findings reported in [7], in terms of the achieved information density for a
recommended CMS density.
Note that the efficacy of CMS depends on a multitude of factors and cannot be solely assessed

11
https://doi.org/10.26434/chemrxiv-2024-q844g ORCID: https://orcid.org/0000-0002-5174-8882 Content not peer-reviewed by ChemRxiv. License: CC BY-NC 4.0

Figure 3: Information Density as a function of sensor network density, for training and testing datasets
by detection fraction or other metrics similar to information density. To evaluate the effectiveness of
CMS, it’s essential to consider the specific objectives, such as providing at least one valid quantification
estimate every 12 hours (as indicated in the OOOOb regulation), detecting all short emission events,
or simply having the capability to detect large persistent leaks sooner than a quarterly OGI inspection
would reveal them. Under different objectives, the placement and density of sensors should be adjusted
accordingly.
Chen, Kimura, and Allen [7] (hereafter referred to as the UT Austin study) defined temporal
coverage as the percent of time the network would detect a single emission source. This metric is
comparable to the information density used throughout our study. However, in the present work,
information density is calculated based on the temporal coverage across all emission sources. The UT
Austin study concluded that an eight-sensor network, with detection capabilities of 1 ppm at 20 meters
from the source, achieved 20% temporal coverage. These conclusions are consistent with the results of
the present work.
Comparing the results of the present work and the UT Austin study, five key differences exist in the
research objectives and methodologies: (i) The UT Austin study presents a theoretical implementation
of CMS in a simulation setup, evaluating the impact of source-sensor distance, number of sensors, and
detection thresholds on detection efficiency. In contrast, the present work focuses on the optimization
of CMS configuration across a wide range of conditions (including facility complexity, atmospheric
conditions, site geometry, and regional variations), to satisfy the relevant requirements defined in the
OOOOb regulation. (ii) The UT Austin study utilized CALPUFF, an open-source Gaussian Puff
dispersion model, to simulate concentrations. The present work employs the less computationally
intensive Gaussian Plume model to accommodate longer simulation periods for a larger number of
facilities. As stated before, the modularity of this framework offers flexibility in selecting concentration
simulation methods, among other aspects. (iii) The UT Austin study simulated a single emission
source with a release rate of 0.4 kg/hr, compared to the present study with multiple emission sources
located according to operational site layouts, each emitting 1.6 kg/hr. (iv) The UT Austin study
simulated a CMS grid with five concentric rings of eight sensors (intervals of 45◦ ), each at a distinct
distance from the source, ranging from 10 to 50 meters. The present work, in contrast, focuses
on optimizing CMS density and placement with varying source-sensor distances and realistic facility
geometries and associated constraints. While sensor sensitivity was a variable in their study, we
specifically compared our results to their analysis of the 1 ppm sensitivity threshold, which aligns
with our study’s detection threshold. Lastly, (v) Unlike the UT Austin study, which used 2-week
12
https://doi.org/10.26434/chemrxiv-2024-q844g ORCID: https://orcid.org/0000-0002-5174-8882 Content not peer-reviewed by ChemRxiv. License: CC BY-NC 4.0

simulations, this study incorporates 16 weeks of simulations per year to comprehensively account for
seasonal variations in atmospheric conditions.
Comparable temporal coverage, despite varying sensor numbers, can be partially attributed to the
higher release rate used in this study (1.6 kg/h), which is four times that of the UT Austin study. This
increased release rate significantly enhances detection probability. A detailed analysis of the impact
of release rate on information density indicates that with a 0.4 kg/hr release rate, a 3-sensor network
achieves 8% information density. This falls between the UT Austin study results for sensor networks
at 30 and 40 meters, which have temporal coverage of 10.2% and 6.2% respectively. Further details
related to this subject are presented in the Supporting Information (Figure A.3).

3.2

Percent Blind Time

This section introduces a new metric to evaluate CMS effectiveness, aligned with the OOOOb requirements for continuous monitoring technologies to ensure that facility-level emissions are quantified at
least once every 12 hours. We define percent blind time as:

Percent Blind Time =

Blind Time
w×m


× 100

where percent blind time is calculated as the fraction of 12-hour periods (in %) where a network
fails to detect at least 10 measurements from each source group, summed across all sources. Here,
w represents the total number of 12-hour rolling windows within the sampled data and m is the
number of source groups. Percent blind time effectively represents the probability that a CMS system
fails to provide a reliable quantification estimate within 12 hours, mainly due to insufficient direct
source-to-sensor atmospheric transport.
Note that wind direction is the primary factor influencing the detectability of elevated concentrations resulting from onsite release events. This highlights the role of site-specific wind data in
optimizing sensor placement to ensure that the elements of CMS collectively maximize overall detections and minimize long gaps in direct source-to-sensor signals.
Figure 4 illustrates variations in percent blind time as a function of the CMS density, based on all
124 example sites in the dataset. A non-linear relationship exists between sensor density and percent
blind time, with significant reductions in percent blind time achieved by adding the first few sensors.
For example, the mean percent blind time decreases from 35.5% for a 1-sensor network to 15.0% for
a 2-sensor network and 7.1% for a 3-sensor network, demonstrating a more than 2x reduction with
each additional sensor. This contrasts with the near-linear trend observed in percent information. It
indicates that for a CMS with 3 sensors, 7.1% of rolling windows in the detection matrices across all
sites have fewer than 10 positive detections per source group. This translates to a 92.9% probability
of a 3-sensor network providing a reliable quantification estimate within a 12-hour period.
These results may be somewhat surprising given the relatively low information density demonstrated in Section 3.1. This apparent discrepancy highlights an important distinction: even though
the probability of detecting emissions on a site with a 3-sensor network is relatively low, on average,
there is sufficient wind variability over 12-hour windows such that, with proper sensor placement, each
source on a site is observed 93% of the time.

3.3

Time to Detection

Time-to-detection (TTD) represents the delay between the start of an emission event and the detection
of an elevated concentration associated with the event by the CMS. TTD is often used in studies as one
of the metrics to evaluate the performance of CMS. This metric quantifies the rapidity of direct sourceto-receptor pollution transport, which is influenced by favorable wind direction and the configuration
of the CMS.
This study employs a Monte Carlo approach to generate 10,000 random event start times for every

13
https://doi.org/10.26434/chemrxiv-2024-q844g ORCID: https://orcid.org/0000-0002-5174-8882 Content not peer-reviewed by ChemRxiv. License: CC BY-NC 4.0

Figure 4: Percent blind time as a function of CMS density. The diamonds identify the outliers for the
box plot.
source group in each network detection matrix. The time elapsed between the selected event start
time and the next detection associated with that specific source group in the time series is recorded for
each Monte Carlo realization. Finally, the average time to detection is determined across all sources
and Monte Carlo iterations at each of the 124 operational facilities.
Figure 5 illustrates the relationship between mean TTD and CMS density. Similar to the blind time
analysis, the first few additional sensors significantly reduce mean TTD. For example, increasing from
1 to 2 sensors decreases mean TTD from 442.4 minutes to 164.0 minutes, a 64% reduction. Adding a
third sensor further reduces TTD to 82.3 minutes, with 99.1% sites in the study having a mean TTD
below 250 minutes.
The UT Austin study conducted a similar TTD analysis using a concentric ring of eight sensors
at a 10-meter distance from the source. They optimized sensor placement iteratively, selecting sensors
with the highest temporal coverage. The results of that study indicated mean TTDs of 567.1, 191.7,
and 107.6 minutes for 1, 2, and 3 sensors, respectively. The findings of this present work align with
their results, with the small differences in mean TTD which could be attributed to variations in sourcesensor positioning and release rate. Our optimized sensor placements and higher release rate likely
contribute to the slightly lower TTDs observed in this study. Despite these differences, both studies
consistently demonstrate that near-optimally configured 3-sensor CMS could achieve mean TTDs on
the order of 80 to 100 minutes.
To analyze the probability of detecting a continuous release of a given duration, the Monte Carlo
iterations were sampled to construct empirical cumulative distribution functions (CDF) for CMS with
varying densities (Figure 6). These CDFs illustrate the theoretical probabilities of TTD at various
CMS sensor network densities based on a large number of randomly selected release event start times.
The results indicate that on average, a 3-sensor network achieves a 90% POD within 3.7 hours, demonstrating that optimal CMS sensor placement can consistently achieve timely detections concerning the
OOOOb expectations.

14
https://doi.org/10.26434/chemrxiv-2024-q844g ORCID: https://orcid.org/0000-0002-5174-8882 Content not peer-reviewed by ChemRxiv. License: CC BY-NC 4.0

Figure 5: Relationship between mean time to detection and CMS density

4

Case Studies

As an illustrative example, this section presents a case study involving operational oil and gas facilities,
applying the proposed framework to determine near-optimal CMS configuration. An in-depth discussion of the near-optimal sensor placement algorithm and framework outputs is included. This section
also compares simulation framework results with direct on-site methane concentration measurement
data to assess agreement between simulated and measured blind time statistics.

4.1

CMS Configuration and Simulated Blind Time

Figure 7 illustrates a representative example of a facility layout, windrose plot, and optimized sensor
locations for a 3-sensor CMS. The numbered green dots indicate the order in which the optimization
procedure selected sensor locations. The windrose plot reveals that the first recommended sensor
location is downwind of a large group of emission sources. The second sensor is positioned on the
opposite side the sources, relative to the first sensor, capturing emissions from the second most common
wind direction. The algorithm positions the third sensor near an isolated source, demonstrating its
ability to prioritize new information over redundant detections.
Figure 8 illustrates network detection matrices for this site, showcasing the impact of varying sensor
network density. To enhance visual clarity, we focus on a single week of data instead of the full dataset
used throughout this study, transposed minute-level network detection matrices. In other words, each
row represents a source group’s timeseries of detections/nondetections, with black corresponding to a
detection of the source at a given time, and white corresponding to a non-detection. Time periods
that would contribute to blind time are highlighted with red rectangles.
Significant blind time periods are observed across all sources for a single-sensor network (red rectangles in each row). However, adding more sensors to the network substantially reduces blind time.
For the illustrated time period, the network’s percent blind time decreases from 52.8% for a 1-sensor
network to 5.8% and 2.6% for 2 and 3-sensor networks, respectively.
For the single-sensor network, the sparse detections (5% information density) indicate that the
dominant wind direction during this week was unfavorable. Adding a second sensor doubled information density and significantly reduced blind time from 52.8% to 5.8%. A third sensor further enhanced

15
https://doi.org/10.26434/chemrxiv-2024-q844g ORCID: https://orcid.org/0000-0002-5174-8882 Content not peer-reviewed by ChemRxiv. License: CC BY-NC 4.0

Figure 6: Empirical cumulative distribution function of time to detection for various network densities

Figure 7: Left: Output of the siting algorithm showing sources (blue), potential sensor locations
(orange), selected sensors (green), Percent Blind Time (PBT). Right: Windrose diagram showing
wind speed and direction distribution.
performance, increasing information density to 21% and halving blind time to 2.6%. This analysis demonstrates the algorithm’s ability to optimize sensor locations to maintain performance under
varying wind conditions.

4.2

Measured Blind Time

A comparison of simulation results with actual on-site concentration measurement data was conducted to assess agreement between key simulated statistics (blind time, information density, timeto-detection) and statistics derived from the concentration timeseries directly measured by CMS at
operational facilities. Five operational oil and gas facilities with 3-sensor CMS were selected and six
months of ambient concentration data was collected from all sensors at each facility. These facilities
were chosen for their likelihood of continuous operational emissions from equipment like compressors,
minimizing temporal gaps in continuous release events.
The data across the sensors on each facility was aggregated by computing the maximum concentration from the sensor network at each minute. After subtracting facility-specific background methane
concentration levels, the same thresholding process used in the simulations was applied to the actual

16
https://doi.org/10.26434/chemrxiv-2024-q844g ORCID: https://orcid.org/0000-0002-5174-8882 Content not peer-reviewed by ChemRxiv. License: CC BY-NC 4.0

Figure 8: Transposed network detection matrices with 1-3 sensors. The horizontal axis corresponds to
the time in the simulation (in minutes) and the vertical axis represents the sources at the site. White
corresponds to non-detections and black signifies detections. Periods that contribute to blind time (12
hour or greater time windows with less than 10 detections per source group) are depicted with red
rectangles
measurements to define detections. The gap between detections (duration of continuous zeros) in the
network detection matrices was then calculated, representing the “operational blind times” during the
6-month measurement period.
It’s important to note that this analysis assumes continuous emissions from sources. If emissions are
intermittent, additional “blind periods” may occur due to operational breaks rather than the impact of
wind direction. This results in more conservative (an upper limit) estimates of operational blind time,
compared to the simulated results. The distribution of operational blind periods for the 5 operational
facilities is illustrated in Figure 9. These results demonstrate good agreement with the simulated cases.
The mean operational blind time was 1.7 hours (102 minutes), aligning closely with the mean
simulated TTD of 82 minutes across all sites. As stated before, intermittent operational emissions may
contribute to the higher blind time based on direct measurements. Although calculated differently, both
metrics effectively characterize the duration of gaps where the sensor network fails to detect sources
due to unfavorable wind direction. The strong agreement between simulated and measured blind times
for 3-sensor networks suggests that: (i) the proposed framework provides acceptable outputs related
to the determination of blind time for near-optimal CMS, and (ii) both simulated and operational
data indicate that a CMS with 3 sensors will detect changing emissions within, on average, 80 to 102
minutes. This result is in good agreement with the theoretical results from [7]. While site complexity
and other factors can influence these results, we expect these findings to be generally applicable to
facilities with comparable sizes and number of source groups.

17
https://doi.org/10.26434/chemrxiv-2024-q844g ORCID: https://orcid.org/0000-0002-5174-8882 Content not peer-reviewed by ChemRxiv. License: CC BY-NC 4.0

Figure 9: Distribution of blind periods at operational facilities.

(a)

(b)

(c)

Figure 10: Comparison of Chama and the proposed framework (greedy): (a) Percent Information (b)
Percent Blind Time (c) Mean Time to Detection

5

Model Validation

To evaluate the performance of the proposed framework compared to the publicly available sources, a
comparison between Chama [19] (a sensor placement optimization tool based on mixed-integer linear
programming formulations) and the proposed framework (employing the greedy algorithm) was conducted across 15 randomly selected operational oil and gas production facilities. Using identical input
parameters, optimal 3-sensor CMS configurations were identified for the second calendar quarter, and
their performance was evaluated during the subsequent quarter. The atmospheric data used in this
comparison is consistent with that described in Section 2.1.1. As a modular framework, Chama allows
for external simulation data. For consistency, both Chama and the proposed framework use the same
Gaussian Plume model as the simulation method. By using separate quarters for optimization and
testing, we assess network performance on unobserved data. Note that the use of minute-level wind
data for one calendar quarter differs from the original Chama algorithm’s implementation, which could
potentially introduce inconsistencies in its results.
First, a set of metrics, including percent information, percent blind time, and mean time to detection, is used to compare the performance of the models for different CMS network densities (Figure 10).
Chama demonstrated relatively better performance in achieving higher percent information compared
to the proposed framework (Figure 10 (a)). The multi-objective optimization approach used in this
framework, which prioritizes both percent information increase and blind time reduction, contributes
to the observed performance difference. Figure 10 (b) and (c) illustrate key performance differences
between Chama and the proposed framework in terms of blind time and TTD. The proposed framework effectively reduces both blind time and TTD compared to Chama, albeit with a slight trade-off
in percent information. For 3-sensor networks, the proposed framework achieves a mean percent blind
time of 7.6% and a mean TTD of 81 minutes, outperforming Chama’s 16.7% blind time and 180 minutes TTD. This comparison demonstrates the effectiveness of the proposed framework in achieving
significantly lower blind time and TTD with slightly lower percent information.
To demonstrate why the proposed framework achieves lower blind time and TTD, we compare
18
https://doi.org/10.26434/chemrxiv-2024-q844g ORCID: https://orcid.org/0000-0002-5174-8882 Content not peer-reviewed by ChemRxiv. License: CC BY-NC 4.0

(a)

(b)

Figure 11: Comparison of Chama vs Greedy: (a) Scatter showing emission locations, selected optimal
locations from each sensor placement algorithm. Green (red) dots correspond to the output of the
Greedy (Chama) algorithm, (b) Q2 windrose used to optimize
the sensor locations resulting from Chama (red) and the proposed framework (green) for an example
facility (Figure 11). While Chama effectively maximizes information percent, it exhibits a bias towards
predominant wind directions. This bias can lead to extended blind periods when the wind deviates
from the prevailing direction. The proposed framework addresses this shortcoming by strategically
placing two sensors downwind of emission sources and a third sensor on the north-western side of the
facility. This configuration ensures that the CMS detects elevated concentrations under a wider range
of wind directions. This example highlights the impact of different objectives on CMS configuration
and resulting performance.

6

Conclusion

This study introduces a modular framework, employing forward dispersion modeling and a greedy
algorithm, to optimize the configuration of the continuous monitoring systems (CMS) using fixed-point
sensors. This framework aims to select near-optimum sensor placement for different CMS network
densities, specifically to meet the criteria outlined in the recently finalized EPA OOOOb rule, to
ensure that CMS are able to determine a valid methane mass emissions rate at least once for every
12-hour block. A multi-objective optimization approach is used to maximize information density and
minimize CMS blind time simultaneously. The framework was applied to 124 operational oil and gas
production facilities with a wide variety of site characteristics and meteorological conditions to evaluate
the effectiveness of CMS in detecting and quantifying emissions. Three metrics are considered for the
performance evaluation of CMS, including information gain, blind time, and time to detection.
Application of the framework to operational oil and gas production facilities indicates that on
average, 3-sensor CMS have an information density of approximately 20% meaning that one out of five
elements in the CMS detection matrix corresponds to a detection. However, systems rarely experience
prolonged periods of blind time, with an average blind time of 7.1% for a 3-sensor CMS. This indicates
that the probability of observing a 12-hour block without obtaining enough information to make a
reasonable rate estimate on a given source is 7.1%. The average time to detection for a 3-sensor CMS
is estimated to be 82.3 minutes, with all sites in the study having a mean time to detection below 250
minutes. These results demonstrate that, with proper CMS configuration and sufficient sensitivity, on
average, CMS can reliably detect emissions on 12-hour time blocks in 92.9% of cases, and also provide
alerts of anomalous emissions within less than 1.5 hours of the event starting time.
These results demonstrate the critical nature of optimizing sensor placement for the specific objective of the system. If, for instance, the system is required to provide a quantified rate every 12

19
https://doi.org/10.26434/chemrxiv-2024-q844g ORCID: https://orcid.org/0000-0002-5174-8882 Content not peer-reviewed by ChemRxiv. License: CC BY-NC 4.0

hours, then as demonstrated, a metric such as blind time must be considered in the sensor placement
optimization process. This study demonstrates that the proposed framework using multi-objective
optimization processes can result in improved CMS performance, which is essential for meeting the
criteria outlined in the EPA OOOOb rule.

References
[1]

Jiayang Lyra Wang et al. “Multiscale methane measurements at oil and gas facilities reveal necessary frameworks for improved emissions accounting”. In: Environmental science & technology
56.20 (2022), pp. 14743–14752.

[2]

William S Daniels et al. “Toward multiscale measurement-informed methane inventories: reconciling bottom-up site-level inventories with top-down measurements using continuous monitoring
systems”. In: Environmental Science & Technology 57.32 (2023), pp. 11823–11833.

[3]

Felipe J Cardoso-Saldaña. “Tiered leak detection and repair programs at simulated oil and
gas production facilities: Increasing emission reduction by targeting high-emitting sources”. In:
Environmental Science & Technology 57.19 (2023), pp. 7382–7390.

[4]

Chiemezie Ilonze et al. “Assessing the Progress of the Performance of Continuous Monitoring
Solutions under a Single-Blind Controlled Testing Protocol”. In: Environmental Science & Technology (2024).

[5]

Zhenlin Chen et al. “Comparing Continuous Methane Monitoring Technologies for High-Volume
Emissions: A Single-Blind Controlled Release Study”. In: ACS ES&T Air (2024).

[6]

CSU METEC. Continuous Monitoring Final Report. 2024. url: https://www.projectcanary.
com/wp-content/uploads/2024/07/Project-Canary-2024-ADED-Results.pdf.

[7]

Qining Chen, Yosuke Kimura, and David T Allen. “Defining Detection Limits for Continuous
Monitoring Systems for Methane Emissions at Oil and Gas Facilities”. In: Atmosphere 15.3
(2024), p. 383.

[8]

Qining Chen et al. “Simulated methane emission detection capabilities of continuous monitoring
networks in an oil and gas production region”. In: Atmosphere 13.4 (2022), p. 510.

[9]

Chen Wang et al. Learning to Optimise Climate Sensor Placement using a Transformer. 2024.
arXiv: 2310.12387. url: https://arxiv.org/abs/2310.12387.

[10]

B Yildirim, C Chryssostomidis, and GE Karniadakis. “Efficient sensor placement for ocean measurements using low-dimensional concepts”. In: Ocean Modelling 27.3-4 (2009), pp. 160–173.

[11]

Jonathan Berry et al. “Sensor placement in municipal water networks with temporal integer
programming models”. In: Journal of water resources planning and management 132.4 (2006),
pp. 218–224.

[12]

Jikai Dong et al. “Optimization of sensor deployment sequences for hazardous gas leakage monitoring and source term estimation”. In: Chinese Journal of Chemical Engineering 56 (2023),
pp. 169–179.

[13]

Andreas Krause, Ajit Singh, and Carlos Guestrin. “Near-optimal sensor placements in Gaussian
processes: Theory, efficient algorithms and empirical studies”. In: Journal of Machine Learning
Research 9.2 (2008).

[14]

Lina Sela and Saurabh Amin. “Robust sensor placement for pipeline monitoring: Mixed integer
and greedy optimization”. In: Advanced engineering informatics 36 (2018), pp. 55–63.

[15]

Safuriyawu Ahmed et al. “Development and Analysis of a Distributed Leak Detection and Localisation System for Crude Oil Pipelines”. In: Sensors 23.9 (2023), p. 4298.

[16]

Makoto M Kelp et al. “Data-Driven Placement of PM2.5 Air Quality Sensors in the United
States: An Approach to Target Urban Environmental Injustice”. In: Geohealth 7.9 (2023).

[17]

Dravyansh Sharma, Ashish Kapoor, and Amit Deshpande. “On greedy maximization of entropy”.
In: International Conference on Machine Learning. PMLR. 2015, pp. 1330–1338.

20
https://doi.org/10.26434/chemrxiv-2024-q844g ORCID: https://orcid.org/0000-0002-5174-8882 Content not peer-reviewed by ChemRxiv. License: CC BY-NC 4.0

[18]

Meng Jia, Troy Sorensen, and Dorit Hammerling. “Optimizing continuous monitoring sensor
placement on oil and gas sites”. In: (2024).

[19]

Katherine A Klise et al. “Sensor placement optimization software applied to site-scale methaneemissions monitoring”. In: Journal of Environmental Engineering 146.7 (2020), p. 04020054.

[20]

Yuan Zi et al. “Distributionally robust optimal sensor placement method for site-scale methaneemission monitoring”. In: IEEE Sensors Journal 22.23 (2022), pp. 23403–23412.

[21]

Xinchao Liu et al. “Optimal Sensor Allocation with Multiple Linear Dispersion Processes”. In:
arXiv preprint arXiv:2401.10437 (2024).

[22]

Project Canary. Project Canary Continuous Description of Technology. 2024. url: https://
methane.app.cloud.gov/review/46.

[23]

Clay Bell et al. “Performance of Continuous Emission Monitoring Solutions under a Single-Blind
Controlled Testing Protocol”. In: Environmental Science & Technology 57.14 (2023). PMID:
36977200, pp. 5794–5805. doi: 10.1021/acs.est.2c09235. eprint: https://doi.org/10.1021/
acs.est.2c09235. url: https://doi.org/10.1021/acs.est.2c09235.

[24]

Roland R Draxler. “Estimating vertical diffusion from routine meteorological tower measurements”. In: Atmospheric Environment (1967) 13.11 (1979), pp. 1559–1564.

21
https://doi.org/10.26434/chemrxiv-2024-q844g ORCID: https://orcid.org/0000-0002-5174-8882 Content not peer-reviewed by ChemRxiv. License: CC BY-NC 4.0

A

Supporting Information - A Framework for Optimizing Continuous Methane Monitoring System Configuration for Minimal Blind Time: Application and Insights from over 100
Operational Oil & Gas Facilities

This section provides statistics and supplemental analysis of the performance of CMS with respect to
site-specific characteristics. A rationale for our operational choice to categorize a 12-hour interval as
being ‘detected’ by the CMS when at least 10 entries per source group in Sd are non-zero is included
here as well.

A.1

A justification for the chosen blind time criteria

(a)

(b)

intensity
1ppm
Figure A.1: (a) Ratio= predicted
determined using randomly drawn
actual intensity of each experiment in D
samples, with sample sizes of t = 10, 20, 30 & 60, from the measurements for which the forwardmodel estimates exceed the 1ppm threshold. Each symbol corresponds to the average of 100 different
predictions via random samples of size t. The thick black line indicates the true ratio of 1.00 and
the thin blue line identifies the predicted ratio of 0.98. The
gray area corresponds to one
P highlighted
standard deviation. (b) Relative frequency histogram of
Qk .

A justification is offered here by revisiting the single-source controlled-release experiments from
Methane Emissions Technology Evaluation Center’s 2022 Advancing Development of Emissions Detection (ADED) campaign for which the in-house modeling approach correctly localizes the source
group and determines the source intensity within a factor of 0.2 of the actual rate. Only those minuteaveraged concentration measurements from each experiment in the experimental subset are selected
where the forward-model estimates of the ambient concentration at point-sensor positions exceed the
detection criteria of 1 ppm, hereafter referred to as dataset D1ppm . A random sample of t measurements is drawn from D1ppm for each experiment and a quantification estimate obtained by applying
the in-house modeling approach. This procedure is repeated a 100 times to reduce the uncertainty in
the individual quantification estimates. Note that the identity of the active source group is assumed
to be known during this analysis and the problem simplifies into one with single unknown constant
emission rate.
Figure A.1(a) presents the ratio of the predicted emission intensity to the actual value for the
experiments in D1ppm at t = 10, 20, 30 and 60. The mean and standard deviation values of the ratio for
t = 10 are at 0.98 and 0.29, respectively. It is instructive to compare these results from figure A.4 with
the overall quantification results for this ADED campaign. According to [23], more than 80% of the
results by only three CM solutions were within a factor of 3 of actual release rates. Careful examination
22
https://doi.org/10.26434/chemrxiv-2024-q844g ORCID: https://orcid.org/0000-0002-5174-8882 Content not peer-reviewed by ChemRxiv. License: CC BY-NC 4.0

reveals that symbols representing larger t values are missing for several experiments in figure A.1(a),
which is due to the absence of sufficient samples with estimates for concentration enhancement levels
by the forward model above the threshold. While it is clear that for most experiments in figure A.1(a)
the ratio moves towards unity as t increases, there are a few instances however, e.g. experiments
labeled 8 and 15, for which the ratio noticeably worsens as t grows. Inspection suggests that underprediction and its further exacerbation with increasing t maybe related to the well-known limitations of
the Gaussian plume model under very low wind speeds. For experiments 8 and 15, the mean horizontal
wind speed magnitudes at the anemometer height of about 2 meters over the duration of the release
are 0.64 and 0.98 m/s, respectively. It is further evident from figure A.1(a) that even when the ratio
improves on increasing the randomly drawn sample sizes, this refinement is generally marginal barring
a couple of outliers that show improvement of more than a factor of 0.2 as t changes from 10 to 20.
The two core arguments of the above analysis − i.e., a mean ratio around unity and observing limited
improvement in predicted intensity as t increases − extend some rationale to our choice of classifying
a 12-hour window as being detected by the CMS network when at least 10 estimates by the forward
model exceed the 1ppm enhancement criteria.
Next, we examine the integrated set of Qk (subset of N for each source group m and rolling window
w) constructed by amalgamating the complete forward simulation results from all the sites analyzed
in this study. The aim is to emphasize that the frequency of rolling windows with the number of
detections per source group hovering around the criteria for minimum detection count occur rather
seldomly. Figure A.1(b) plots
P the relative frequency histogram for the ‘sum of detections per source
group in a rolling window ( Qk )’ obtained after randomly sampling a million rows from the integrated
set of Qk mentioned earlier. It is evident that with a mean and median at 136Pand 116, respectively,
and the the relative frequency for the first two bins at about 0.12, generally
Qk well exceeds the
minimum threshold of 10 detections.

A.2

Wind Statistics

This section offers key statistics regarding wind parameters and investigates certain trends between
those parameters and various metrics. Figure A.2 shows wind speed distributions across major oil and
gas basins. The data has been collected using over 1,000 anemometers installed in operational oil and
gas production facilities in the US.
While there are differences in the mean characteristics across basins they all follow a log-normal
distribution with 81 percent of readings falling below 3 m/s. Next, we inspect atmospheric stability
class distributions across basins.
Table A.1 shows the percentage distribution of each stability class across the major basins. Stability classes are determined using the sigma theta method over a rolling time window. This method
computes the circular standard deviation of wind direction over this rolling window and maps it to
a Pasquill class via a table look-up [24]. The greater the sigma theta, the more unstable the conditions. At an individual stability class level, there are significant differences across basins. Specifically,
the Appalachian and Haynesville basins have larger percentages of very unstable regimes relative to
the other basins. When aggregating stability classes to unstable (A, B), neutral (C, D), and stable
(E, F), some similarities are observed. First, analyzing the unstable regime, an apparent difference
between Appalachian and Haynesville and other basins is observed. It indicates that these two basins
experience significantly more unstable atmospheric conditions. When analyzing the neutral stability
classes, all basins fall within -3.18 /+1.82 of the mean (60.68), indicating neutral stability conditions
are relatively consistent across all basins. The neutral stability classes are the predominant conditions,
with all basins having more than 50% stable conditions. Appalachian and Haynesville basins have
significantly lower stable atmospheric conditions relative to the other basins.

A.3

Key Metrics Stratified by Site Specific Variables

This subsection analyzes the effect of site-specific parameters on percent information, blind time, and
mean time to detection. We focus on the impact of the number of emission sources, site area, stability

23
https://doi.org/10.26434/chemrxiv-2024-q844g ORCID: https://orcid.org/0000-0002-5174-8882 Content not peer-reviewed by ChemRxiv. License: CC BY-NC 4.0

Figure A.2: Wind Speed (m/s) distributions across basins
Table A.1: Percentage Distribution of Stability Classes Across Different Regions

Stability Class
A
B
C
D
E
F

Appalachian
19.8
6.7
30.7
31.7
7.9
3.2

Barnett
9.0
3.8
17.2
44.2
10.9
14.8

Denver-Julesburg
10.2
4.0
18.5
44.0
10.6
12.6

Haynesville
30.7
7.3
37.7
19.8
3.8
0.7

Permian
11.0
4.4
17.9
41.7
11.9
13.1

class, and simulated release rate.
A.3.1

Percent Information Stratified

The number of emission sources varies on a case-by-case basis. The effect of the number of emissions
sources and site area on the percent information is investigated. Figure A.3 (a) shows the relationship
between the number of emission sources and percent information for a 3-sensor network. The relationship is inversely related such that by increasing the number of emission sources, percent information
decreases. This is evidenced visually by the downward trend and the best-fit line has a coefficient of
-0.35.
Only 18% of the variability in percent information is explained by the number of sources. It indicates
that other variables such as equipment orientation and site-specific wind characteristics impact the
ability of the CMS to detect emissions. The downward trend is in line with the underlying intuition
of spatial coverage. Increasing the number of sources results in the expansion of the coverage area (an
artifact of spatial clustering). While holding the number of devices constant, a decrease in the percent
information of the CMS is expected.
Figure A.3 (b) shows the relationship between site area and percent information. As mentioned
24
https://doi.org/10.26434/chemrxiv-2024-q844g ORCID: https://orcid.org/0000-0002-5174-8882 Content not peer-reviewed by ChemRxiv. License: CC BY-NC 4.0

(a)

(b)

(c)

(d)

Figure A.3: Analyses of percent information: (a) as a function of the number of emission sources, (b)
as a function of site area, (c) across different basins, and (d) as a function of release rate.

25
https://doi.org/10.26434/chemrxiv-2024-q844g ORCID: https://orcid.org/0000-0002-5174-8882 Content not peer-reviewed by ChemRxiv. License: CC BY-NC 4.0

previously, site area is correlated to the number of sources. Therefore, the same inverse trend is
expected, which is evidenced by a coefficient of -10.72. This shows that an increase in a site area of
1 hectare results in a decrease of 10.72 in percent information. The site area is a good indicator of a
3-sensor network’s expected Information Density. Since sensor positions are fixed, increasing facility
area will significantly decrease the ability of the network to make detections from all the sources. A
relatively low R2 value of 0.36 exists, indicating that while the relationship between area and percent
information is significant, facility area only explains 36% of the variation, and site-specific factors
impact information more significantly.
The analysis of percent information by basin A.3 (c) and comparisons to a geographic region stability class distribution indicate a relationship between a geographic region stability class distribution
and percent information. Specifically, higher medians and larger upper whiskers for the Appalachian
and Haynesville basins are noticed, with the exception of the Barnett. The Appalachian and Haynesville basins had significantly more unstable stability class conditions. This indicates more drastic
and frequent changes in wind direction which would lead to more detections and higher percent information. Since the Barnett region experiences more frequent stable conditions, less fluctuation in
the wind direction could be expected. It indicates more dominant and consistent wind directions.
However, further investigation is required to support the suggested conclusions.
Figure A.3 (d) shows the impact of release rate on mean percent information. An increase in
mean percent information is observed as by increasing the release rate. While the increases across the
stratified release rates are linear, the percent information increase is non-linear. This is an artifact
of the thresholding process. The number of concentrations above 1 ppm significantly decreases with
lower release rates, especially when a sensor is off the center of the plume. As the release rate increases,
the number of concentrations above 1 increases. This presents itself in the data as the mean percent
information for a 3-sensor network increases from 8.4 to 25 as the release rate increases from 0.4 kg/hr
to 2.8 kr/hr. We notice diminishing marginal returns as the release rate increases. A higher release
rate adds little new information to the network and only adds information by pushing concentrations
that were below the threshold (1 ppm) above the threshold. The majority of the time it increases the
predicted concentration that the network had already detected, thus, no new information is added.
A.3.2

Blind Time Stratified

Figure A.4 (a) shows the relationship between the percent blind time and the number of emission
sources for a 3-sensor network. A slight indication of positive correlation is observed, but when
comparing the slopes to the percent information analysis, blind time is less sensitive to the number of
sources and site area (Figure A.4 (b). This is an artifact of the blind time metric analyzing 12-hour
windows of the sensitivity. It is more feasible for a 3-sensor network to consistently make detections
within windows with few sources or smaller site areas.
Figure A.4 (c) shows the distributions of percent blind time across basins. The inverse relationship
between the percent information and percent blind time becomes apparent. The Appalachian and
Haynesville basins had higher median percent information. The inverse relationship is stronger in the
Haynesville basin with a median blind time below 5%. The Appalachian relationship is not as clear due
to the majority of sites in the sample residing in this basin, which causes the relationship to become
muddled. Tying to the stability class, more unstable stability class regions correspond to lower blind
time.
A similar diminishing marginal return in mean percent blind time as the release rate increases
is observed as with mean percent information. A lower release rate significantly increases the mean
percent blind time. A three-sensor network has 28% blind time on average with a release rate of 0.4
kg/hr compared to a mean blind time of 4.7% with a release rate of 2.8 kg/hr. Lower release rate
significantly decreases the detectable information due to thresholding and higher release rates add
minimal new information, but increase the concentrations already detected.

26
https://doi.org/10.26434/chemrxiv-2024-q844g ORCID: https://orcid.org/0000-0002-5174-8882 Content not peer-reviewed by ChemRxiv. License: CC BY-NC 4.0

(a)

(b)

(c)

(d)

Figure A.4: Analyses of percent blind time: (a) as a function of the number of emission sources, (b)
as a function of site area, (c) across different basins, and (d) as a function of release rate.

27
https://doi.org/10.26434/chemrxiv-2024-q844g ORCID: https://orcid.org/0000-0002-5174-8882 Content not peer-reviewed by ChemRxiv. License: CC BY-NC 4.0

(a)

(b)

(c)

(d)

Figure A.5: Analyses of mean time to detection: (a) as a function of the number of emission sources,
(b) as a function of site area, (c) across different basins, and (d) as a function of release rate.
A.3.3

Mean Time to Detection Stratified

Figures A.5 (a and b) show the relationship between the number of sources and site area versus mean
time to detection, respectively. The number of emission sources shows minimal impact on the mean
time to detection (evidenced by the small coefficient of 1.04). Rather, the site area is a more explanative
variable with respect to time to detection shows the 3-sensor networks have limited spatial coverage.
Regardless, the results show that 3-sensor networks hold to ability to make timely detections, well
within six hours, for site areas ranging from 5,000 square meters to 16,0000 square meters.
Figure A.5 (c) shows the distribution of mean time to detection across the various oil and gas basins
in this study. The effect of the stability class stands out when analyzing time to detection. For the
more unstable basins, Appalachian and Haynesville, a significantly lower mean time to detection with
medians near 50 minutes is achieved. This shows that more timely detections by a 3-sensor network
can be made in regions where rapid fluctuations in the wind directions are more common.
Figure A.5 (d) shows the effect of release rate on TTD. The effect of a low release rate is significant
as the mean TTD for a 3-sensor network ∼ 3 times longer for a release rate of 0.4 kg/hr compared
to all the other rates (326 versus a maximum of 116 minutes). The UT Austin study analyzed TTD
versus distance and found that an 8-sensor network at a release rate of 0.4 kr/hr has mean TTD of
228.5 and 456.4 minutes for distances of 30 and 40 meters, respectively [7]. In our study, the average

28
https://doi.org/10.26434/chemrxiv-2024-q844g ORCID: https://orcid.org/0000-0002-5174-8882 Content not peer-reviewed by ChemRxiv. License: CC BY-NC 4.0

source-receptor distance was 40.47 meters. This highlights the impact of optimal placement as the
correct framework achieves a lower TTD with fewer sensors than a non-optimal 8-sensor network (326
versus 456 minutes) under the same release rate of 0.4 kg/hr and distance.
This document studies the effect of site-specific parameters on the key metrics used to evaluate the
performance of the algorithm and 3-sensor networks. the results show that the number of emission
sources and site area impact the percent information more than the percent blind time and mean
time to detection relatively. Investigation of the impact of stability class indicates that more unstable
atmospheric classes lead to better performance overall, showing that CMS can detect more consistently
under rapid fluctuations in the wind direction. The impact of increasing the simulated release rate
shows consistent diminishing marginal returns across all key metrics. Low release rates significantly
impact the performance of networks and the performance gain under a larger release rate is minimal.
The EPA’s criteria for CMS under the alternative test method is that systems should have action
thresholds of 1.2 and 1.6 over a 90-day rolling window. This study concludes that, with optimal sensor
placement and appropriate sensitivity, CMS performs well at these thresholds with minimal blind time
and timely detection.

29
https://doi.org/10.26434/chemrxiv-2024-q844g ORCID: https://orcid.org/0000-0002-5174-8882 Content not peer-reviewed by ChemRxiv. License: CC BY-NC 4.0

