Non-detects—measurements reported as “below the detection limit”—are ubiquitous in
environmental science and engineering. They are frequently replaced with a constant,
but this biases estimates of means, regression slopes, and correlation coefficients.
Omitting non-detects is worse, and has led to serious errors. Simple alternatives are
available: rank-based statistics, maximum likelihood estimation, and re-purposed
survival analysis routines. But many environmental datasets do not align well with the
assumptions these methods make—it is often necessary to account for hierarchy (e.g.,
measurements nested within lakes), sampling strategy (e.g., measurements collected
as time series), heterogeneity (e.g., site-dependent variance), and measurement error.
Bayesian methods offer the flexibility to do this; incorporating non-detects is also easy
and does not bias model parameter estimates as substitution does. Here we discuss
Bayesian implementations of common bivariate and multivariate statistical methods
relevant to environmental science. We use a dataset comprising time series of Ag, As,
Cd, Ce, Co, Sb, Ti, U, and V concentrations in municipal biosolids that includes many
non-detects. The models can be reproduced and extended to new problems using the
data and code accompanying this paper.
*Corresponding author
Graphical abstract
Non-detects—measurements recorded as less than a detection or reporting limit—are
ubiquitous in environmental science and engineering. In the statistical literature, they
are known as left-censored observations. A popular method of representing them in
statistical routines is to replace them with one-half, or some other fraction, of the
detection limit. But while common, this strategy can severely bias estimates based on
the data when the censoring rate is high. Worse still is omission—leaving out non-
detects has led to serious and well-documented errors.1
For basic tasks like comparing two groups or estimating a mean, linear regression
slope, or correlation coefficient, there are simple alternatives to substitution and
omission. These include rank-based methods, maximum likelihood estimation, and re-
purposed survival analysis routines.1 But many environmental datasets require more
complex models that account for hierarchy, sampling strategy, heterogeneity, and
measurement error. For instance, measurements may be collected across multiple
lakes with different characteristics or they may be recorded as time series (i.e., serially
dependent data). The standard toolbox for censored data analysis does not always
accommodate these features.
Bayesian methods excel in this context—since the sampling techniques they rely on
provide a near-universal approach to parameter estimation, they can be very flexible. In
particular, it is straightforward to account for non-detects in almost any model. Here, we
provide examples of common statistical models in environmental science and
engineering whose Bayesian versions can easily accommodate non-detects. They are
reproducible via the code and data that accompany this paper.
Materials and methods
Data collection
We fit models to a dataset comprising concentrations of Ag, As, Cd, Ce, Co, Sb, Ti, U,
and V in municipal biosolids. Biosolids samples were collected from the clarifiers of
three wastewater treatment facilities in 125 mL polypropylene bottles. Samples were
autoclaved, desiccated by baking at 105°C for approximately 60 hours, digested
according to EPA Method 3050B,2 diluted serially, and quantified by inductively coupled
plasma mass spectrometry according to Standard Method 3125.3 A summary of the
data is available in Table 1.
wastewater treatment facilities.
Median (µg g-1)

Lower quartile (µg g-1)

Upper quartile (µg g-1)

% censored
Data analysis
The data and code necessary to reproduce the main results from this paper are
functions used to fit the models in Stan are available in a separate R package.4 We
used R version 4.3.3 throughout,5 along with a collection of contributed packages.6–12
Results and discussion
Bayesian modeling
Bayesian inference entails fitting a probability model to data, then summarizing it as the
joint distribution of the model parameters, 𝜃.13 The model starts as a prior, 𝑃(𝜃),
quantifying the plausibility of all possible parameter values. The prior reflects
background knowledge and practical considerations.14
The data, 𝑥, are used to update the prior via Bayes’ theorem. It relates the posterior or
updated joint parameter distribution, 𝑃(𝜃|𝑥), with the prior and the likelihood. The
likelihood, 𝑃(𝑥|𝜃), quantifies the compatibility of the data with the proposed model.
In practice, model fitting follows these basic steps:
1.
2.
3.
4.

Assign a probability distribution to the data.
Choose a model for each of the distributional parameters.
Choose a distribution of prior probability for each parameter.
Iterate the following steps to obtain a sample from the posterior:
(a) Propose values for all parameters.
(b) Quantify their plausibility without reference to the data (via the prior distributions).
(c) Quantify the plausibility of each data point given the assumed data distribution and the
proposed parameter values (i.e., the likelihood).
(d) Obtain the relative posterior probability as the product of the likelihood and the prior (i.e.,
Bayes’ theorem).
Iterating over steps 4 (a–d) may require searching a high-dimensional parameter space,
which is often accomplished via the Hamiltonian Monte Carlo algorithm.14 Fortunately,
software packages make this straightforward: the R package brms7, for instance, fits a
huge variety of Bayesian regression models—including censored data models—with a
standard and familiar syntax. Further customization is possible using Stan,15 a
programming platform for Bayesian statistics written in C++.
Substitution biases parameter estimates
Replacing non-detects with a constant can bias parameter estimates substantially,
especially when the censoring rate is high. We show this using a small simulation study
that compares substitution at one-half the detection limit with a parameter estimation
strategy that relies on the cumulative distribution function.
When the dependent variable in a linear regression model includes left-censored
observations, one method of accounting for them is to construct the likelihood for every
censored observation using the appropriate cumulative distribution function in place of
the probability density function. Here, the cumulative distribution function quantifies the
probability that a data point is less than the detection limit—that is, the compatibility of a
non-detect with the proposed model. The likelihood, then, becomes 𝑃(𝑥|𝜃) =
𝐹(𝑥!"#$%&$' |𝜃)𝐺(𝑥($)#!%$' |𝜃), where 𝐹 and 𝐺 are the probability density and cumulative
distribution functions, respectively.
We simulated from a simple linear regression model, 𝑦* ∼ 𝑁(𝜇* = 3 + 0.15𝑥* , 𝜎 = 0.5),
where the dependent variable was partially censored—here, 𝑁 represents the normal
distribution with mean 𝜇* and standard deviation 𝜎. We fit a censored regression using
brms where the simulated non-detects were modeled using the normal cumulative
distribution function. We compared it to a naive model where the non-detects were
replaced with one-half the detection limit.
Specifically, the 𝑖 simulated observations, 𝑦* , were modeled as follows. Except for the
special handling of left-censored observations (𝑦* |𝑐𝑒𝑛𝑠𝑜𝑟𝑒𝑑* = 1), the naive model was
identical.
likelihood:
𝑦* |𝑐𝑒𝑛𝑠𝑜𝑟𝑒𝑑* = 0 ∼ 𝑁(𝜇* , 𝜎) [𝑜𝑏𝑠𝑒𝑟𝑣𝑒𝑑 ]
𝑦* |𝑐𝑒𝑛𝑠𝑜𝑟𝑒𝑑* = 1 ∼ 𝑁-𝐶𝐷𝐹(𝜇* , 𝜎) [𝑙𝑒𝑓𝑡-𝑐𝑒𝑛𝑠𝑜𝑟𝑒𝑑 ]
(1)

model for 𝜇:
𝜇* = 𝛽+ + 𝛽, 𝑥*
priors:
𝛽- ∼ 𝑇J𝜇. = 0, 𝜎. = 2.5, 𝜈. = 3M, for j = 0,1
𝜎 ∼ 𝑇(𝜇/ = 0, 𝜎/ = 2.5, 𝜈/ = 3)
In equation (1), 𝑐𝑒𝑛𝑠𝑜𝑟𝑒𝑑* is a binary variable (0 = 𝑜𝑏𝑠𝑒𝑟𝑣𝑒𝑑, 1 = 𝑙𝑒𝑓𝑡-𝑐𝑒𝑛𝑠𝑜𝑟𝑒𝑑), and
𝑁-𝐶𝐷𝐹 is the normal cumulative distribution function (i.e., 𝑃(𝑋 ≤ 𝑥), the probability that
a random variable 𝑋 is less than or equal to some value 𝑥).14,15 The parameters 𝛽+ and
𝛽, define the linear model for 𝜇* , and 𝑇 denotes the t distribution with degrees of
freedom—which controls probability density in the tails—parameterized by 𝜈.
model for 𝜇0 :
𝜇0 = 𝛼 + 𝛽#*0$ 𝑋#*0$ + 𝑓 (𝑡) + 𝜙 # 𝑟01#
𝑟01# = 𝑙𝑜𝑔([𝑇𝑖]01# ) − 𝛼 − 𝑓(𝑡 − 𝑠)
priors:
𝜎 ∼ 𝐻𝑎𝑙𝑓-𝑇(𝜇/ = 0, 𝜎/ = 2.5, 𝜈/ = 3)
𝛽#*0$ ∼ 𝑇J𝜇. = 0, 𝜎. = 2.5, 𝜈. = 3M
𝜈 ∼ 𝐺𝑎𝑚𝑚𝑎 (𝜇2 = 2, 𝛼2 = 0.1)
𝜙 ∼ 𝑁J𝜇3 = 0.5, 𝜎3 = 0.5M
𝛼 ∼ 𝑇(𝜇4 = 0, 𝜎4 = 2.5, 𝜈4 = 3)
where, in addition to the symbols defined above, 𝐻𝑎𝑙𝑓-𝑇 represents the positive-valued t
distribution and 𝐺𝑎𝑚𝑚𝑎 the gamma distribution, parameterized by mean 𝜇 and shape
parameter 𝛼. The linear model 𝛽#*0$ 𝑋#*0$ estimates a separate intercept for each site,
where 𝑋#*0$ is the design matrix and 𝛽#*0$ the coefficients. The autocorrelation
coefficient, 𝜙 # , and the residual at time 𝑡 − 𝑠, 𝑟01# , define the dependence of each
observation on the previous one, where 𝑠 is the spacing between adjacent
observations.16,20
The term 𝑓(𝑡) is a smooth spline function that captures nonlinear variation in the mean
with time. It takes the following form:
𝑓(𝑡) = 𝑋#56*)$ 𝛽#56*)$ + 𝑍𝑏
priors on spline parameters:
(3) 𝛽#56*)$ ∼ 𝑇J𝜇. = 0, 𝜎. = 2.5, 𝜈. = 3M [𝑢𝑛𝑝𝑒𝑛𝑎𝑙𝑖𝑧𝑒𝑑 ]
𝑏 ∼ 𝑁(0, 𝜎" ) [𝑝𝑒𝑛𝑎𝑙𝑖𝑧𝑒𝑑 ]
𝜎" ∼ 𝐻𝑎𝑙𝑓-𝑇J𝜇/! = 0, 𝜎/! = 2.5, 𝜈/! = 3M
where 𝑍 and 𝑋#56*)$ are matrices representing the penalized and unpenalized basis
functions, while 𝛽#56*)$ and 𝑏 represent the corresponding spline coefficient vectors.21
model for 𝜇:
𝜇[8!]" = 𝛽+[$%] + 𝛽, [𝐶𝑑]*
𝜇[8']" = 𝛽+[$']
priors:
𝛽- ∼ 𝑇J𝜇. = 0, 𝜎. = 2.5, 𝜈. = 3M, for j = 0,1
𝜎 ∼ 𝑇(𝜇/ = 0, 𝜎/ = 2.5, 𝜈/ = 3)
𝜈 ∼ 𝐺𝑎𝑚𝑚𝑎(𝜇2 = 2, 𝛼2 = 0.1)
When the model was fitted with a Gaussian likelihood, the extreme cobalt concentration
of 309 µg g-1 yielded a posterior mean with an extremely wide credible interval (Figure
3). The robust model, fitted with a Student t likelihood, yielded a much narrower credible
interval and a posterior mean that was much less heavily influenced by the extreme
value. A disadvantage of both models is that simulating from them may generate
negative concentrations, even though the posterior mean remains positive over its
range. This could be solved by modeling log-transformed Co concentrations instead,
resulting in a slightly different interpretation: the model would then predict geometric
mean concentrations on the scale of measurement.24
Multivariate models
In a multivariate context, the one-step multiple imputation strategy is often simpler to
apply, since multivariate cumulative distribution functions can be difficult to implement.
Two multivariate models with applications in environmental science are the intercept-
only model, used to estimate a correlation matrix, and principal component analysis.
A Bayesian correlation matrix
In addition to handling non-detects, Bayesian correlation has the advantage that it can
be readily applied to variables that are best described using distributions other than the
Gaussian. A relevant example for environmental sciences is robust correlation, where a
Student t distribution is assigned to each variable and its parameters estimated. This
tends to limit the influence of extreme values, which might otherwise exert undue
influence on the estimated correlation coefficients.
Given 𝑦, an 𝑁 × 𝐷 matrix containing 𝑁 concentrations of 𝐷 elements, we can estimate
the pairwise correlations as follows:
(5)
likelihood:
𝜇,
𝑦 ∼ 𝑀𝑢𝑙𝑡𝑖𝑣𝑎𝑟𝑖𝑎𝑡𝑒𝑁𝑜𝑟𝑚𝑎𝑙 _` … b , 𝛴d
𝜇:
𝜎, … 0
𝜎, … 0
𝛴 = ` ⋮ ⋱ ⋮ b𝑅` ⋮ ⋱ ⋮ b
0 … 𝜎:
0 … 𝜎:
priors:
𝜇- ∼ 𝑇J𝜇; = 0, 𝜎; = 10, 𝜈; = 3M, for j = 1, . . , 𝐷
𝜎- ∼ 𝑇(𝜇/ = 0, 𝜎/ = 10, 𝜈/ = 3), for j = 1, . . , 𝐷
𝑅 ∼ 𝐿𝐾𝐽𝑐𝑜𝑟𝑟(2)
where the 𝜇,...: are the column means of 𝑦, 𝛴 is the covariance matrix, the 𝜎,...: are the
column standard deviations of 𝑦, and 𝑅 is the correlation matrix. 𝐿𝐾𝐽𝑐𝑜𝑟𝑟(2) is a
regularizing prior that encodes mild skepticism of extreme correlation coefficients near -
1 or 1.14
𝑝𝑟𝑖𝑜𝑟𝑠:
𝑧 ∼ 𝑁(0, 𝐼)
𝑊 ∼ 𝑁(0, 𝜎𝛼)
𝜇 ∼ 𝐿𝑜𝑔𝑛𝑜𝑟𝑚𝑎𝑙J𝜇; = 2.5, 𝜎; = 1M
𝜎 ∼ 𝐿𝑜𝑔𝑛𝑜𝑟𝑚𝑎𝑙(𝜇/ = 0, 𝜎/ = 1)
𝛼 ∼ 𝐼𝑛𝑣𝑔𝑎𝑚𝑚𝑎(𝛼4 = 1, 𝛽4 = 1)
where 𝑧 is a 𝑘 × 𝑁 matrix of latent (i.e., unobserved) variables with 𝑘 ≤ 𝐷, 𝑊 is a 𝐷 × 𝑘
transformation matrix mapping from the latent space to the data space, 𝜎 is the
standard deviation of the error (also a latent parameter), 𝐼 is the identity matrix, and
𝐼𝑛𝑣𝐺𝑎𝑚𝑚𝑎 is the inverse gamma distribution.13
Figure 5. (a) The dataset projected onto the first two probabilistic principal components
(z in equation (6)). Values appearing outside the extents of the plot are annotated in
parentheses at the margins. (b) The first two principal axes; that is, the orthonormalized
columns of the transformation matrix 𝑊.
Differences in metals concentrations among the three sites are apparent in Figure 5a. In
particular, Site 3 scored differently on the two principal components, resulting in
substantial separation in the two-component space from the data representing the other
two sites. Differences in titanium concentrations among treatment facilities play a strong
role here: the principal axes—that is, the orthonormalized columns of the transformation
matrix 𝑊 (equation 6)—load titanium concentrations heavily. And titanium
concentrations were high in biosolids from Site 3 relative to Sites 1 and 2 (Figure 2).
Replacing non-detects with a constant—often one-half the detection limit—biases
estimates of means, regression slopes, and correlation coefficients. Simple alternatives
exist, but they are limited and not always applicable to complex environmental datasets
that exhibit hierarchy, complex dependence structures, and heterogeneity. Bayesian
methods have the flexibility to model all of these features, and they can easily
accommodate left-censoring by either modifying the likelihood or one-step multiple
imputation as a part of model fitting.
This work was supported by NSERC through an Alliance Grant (ALLRP 568507–21).
We acknowledge utility staff for facilitating sample collection at the treatment facilities
and Heather Daurie, Alyssa Chiasson, Evelyne Doré, Jorginea Bonang, Genevieve
Erjavec, Sebastian Munoz, and Bofu Li for laboratory assistance.
