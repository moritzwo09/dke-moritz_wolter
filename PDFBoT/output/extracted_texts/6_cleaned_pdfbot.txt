<title>A Framework for Optimizing Continuous Methane Monitoring System Conﬁguration for Minimal Blind Time: Application and Insights from over 100 Operational Oil & Gas Facilities</title>
<title>Noah Metzger , Ali Lashgari , Umair Ismail , David Ball , and Nathan Eichenlaub Project Canary, Denver CO Corresponding author: ali.lashgari@projectcanary.com October 2024</title>
<title>1 Introduction</title>
The role of methane, as a short-lived greenhouse gas with high heat-trapping potential, in global warming is undeniable. Given these characteristics, mitigating methane emissions is one of the most eﬀective and eﬃcient strategies for curbing short-term global warming. The energy sector, particularly the oil and gas industry, is a signiﬁcant contributor to human-caused methane emissions. Many oil and gas methane reduction strategies are cost-eﬀective or even proﬁtable, as captured gas can often be directed to pipelines and sold for additional proﬁt, making oil and gas methane mitigation a ﬁnancially viable option.
Gas releases from the oil and gas industry can be categorized as venting (releases of gas for known and planned reasons), fugitive (unintended releases or leaks), and incomplete combustion emissions (combustion slip). Accurate interpretation of oil and gas emissions proﬁle requires considering source types, event timing and intermittency (duration and frequency), release rate, and spatial emission source distribution. This underscores the critical need for timely and accurate detection and localization of emission events, reliable quantiﬁcation of gas release rate, and swift mitigation actions.
A combination of measurement technologies is often recommended for eﬀective detection and quantiﬁcation of methane emissions events [1–3]. Satellite measurements can remotely detect large-scale methane emissions occurring at the time of observation, enabling independent oversight and enhancing transparency, particularly in understudied regions with limited site access. However, satellite surveys have high detection thresholds and limited overﬂight frequency. Aerial platforms, including mass-balance piloted aircraft and uncrewed aerial vehicle (UAV) surveys, oﬀer lower detection thresholds compared to satellites, but are susceptible to spatial (below the lowest safe altitude for ﬂying for aircraft, and above maximum ﬂying height for UAVs) and temporal extrapolation errors. Other aerial platforms, including downward-looking laser methane column measurements, oﬀer more accurate plume detection and source localization. However, uncertainties in detection and ﬂux rate quantiﬁcation are signiﬁcantly inﬂuenced by factors such as ﬂight altitude and wind speed.
In general, snapshot emission measurement methods, including vehicle-based, aerial, and satellite measurements, often encounter limitations due to the lack of site-speciﬁc meteorological data and information on emission events timing (e.g., duration and frequency of intermittent events). This shortcoming is particularly problematic for emission estimations at complex and congested sites, such as midstream compressor stations.
Continuous monitoring systems using ﬁxed-point sensors (hereafter referred to as CMS) are a cornerstone of multiscale methane emissions measurement approaches, providing site-speciﬁc emissions knowledge. A CMS consists of one or more sensing units strategically placed at a safe distance from potential emission sources to continuously measure ambient pollution concentration levels. Combined with on-site meteorological measurements, this data can enable the detection, timing, localization, and quantiﬁcation of emission events. Key beneﬁts of the CMS include (i) rapid detection of emissions ranging from relatively low rates to super emitting events, (ii) capture of both short-duration/intermittent and continuous events (iii) accurate time-bounding of intermittent emission events, and (iv) complimenting other measurement methods using a continuous stream of site-speciﬁc data on emissions and meteorology.
To realize the full potential of CMS, reliable ambient concentration and meteorological measurements (hardware capabilities) should couple with algorithms capable of transforming raw data into accurate insights related to emission event timing, location, and magnitude (data analytics). The optimal CMS conﬁguration, including the number of sensing units (sensor network density) and placement (ﬁnding optimum locations for ﬁxed point-sensor installations, from a large number of candidate locations to satisfy a given objective function), is another key factor inﬂuencing the performance of CMS systems. Operational records and knowledge, such as parametric information and inspection records, can complement CMS measurements and enhance the diﬀerentiation between intended and unintended emission events, providing actionable insights.
Performance evaluation of continuous methane monitoring systems has been the subject of several studies in recent years. Single-blind testing at controlled release facilities, such as Colorado State University’s Methane Emissions Technology Evaluation Center (METEC) [4] and Stanford University’s testing facility in Casa Grande, Arizona [5], simulates challenging yet simpliﬁed ﬁeld conditions to assess the leak detection and quantiﬁcation capabilities of various CMS solutions. These studies demonstrate the rapid evolution of CMS. Recent METEC single-blind testing, conducted using the Advancing Development of Emissions Detection (ADED) protocol, revealed that one of the topperforming solutions achieved a 90% probability of detecting 0.5 kg/hr emissions while accurately localizing releases in 98.7% of cases, with a mean quantiﬁcation error of -0.064 kg/hr [6]. This results in a cumulative emissions quantiﬁcation error of 2.8% when comparing the overall quantiﬁed methane release amount to the actual methane release over the course of the ADED study. These studies investigate the performance of CMS assuming that the solution provider has employed an optimum network conﬁguration in its deployment. It is essential to emphasize that eﬀective monitoring relies not only on measurement hardware and emissions analytics capabilities but also on optimum CMS conﬁguration. In other words, the 2024 ADED results indicate that CMS systems are capable of achieving these performance levels, with optimal sensor network conﬁgurations, including appropriate sensor density and proper placement conﬁguration, considering facility-speciﬁc variables.
A well-conﬁgured CMS is expected to frequently reﬂect variations in ambient methane concentration levels caused by site-level emissions. Two primary characteristics of a well-conﬁgured CMS are proper sensor network density (considering the limited sensor budget) and optimum placement of sensors in the oil and gas facility. This multi-objective optimization network design problem aims to maximize signal coverage (in this context, ‘signal’ refers to ambient methane concentration levels reﬂecting the presence or absence of an on-site gas release event), ensuring that for a maximum amount of time, ﬂuctuations in ambient methane concentration levels due to on-site gas releases are captured by the CMS. These characteristics enhance rapid detection, time-bounding, localization, and quantiﬁcation of emission events using subsequent analyses.
Many factors can inﬂuence near-source pollutant transport and consequently, signal delivery from the release point(s) to sensors. The primary factors include wind direction, wind speed, atmospheric stability class, facility setup, surface elevation proﬁle, on-site airﬂow obstructions, source-receptor distance, plume release height, receptor height (speciﬁcally the intake air height for each sensor), nearby oﬀsite emission sources, and for speciﬁc sources (e.g., combustors) gas velocity and temperature.
Ideally, a CMS sensor network conﬁguration algorithm should consider all relevant factors. However, such an optimization algorithm can be computationally intensive and impractical with limited data access and operational constraints. A more practical approach involves reducing the complexity of this problem by focusing on the most signiﬁcant factors. This study aims to present a framework that incorporates some of the key factors into the CMS sensor network conﬁguration process. Although all of the factors are not considered in this framework, this oﬀers an initial and foundational step toward a more comprehensive, data-driven approach to CMS conﬁguration, considering a wider range of factors in decision-making.
Reliable emissions event detection requires a CMS conﬁguration (sensor network density and placement) that maximizes the frequency of direct pollution transport from source to sensor. In this context, a new metric, “blind time” is introduced in this work, representing the fraction of 12-hour periods with insuﬃcient data to enable accurate emissions quantiﬁcation due to limited direct pollution transport from sources to the sensor network. The deﬁnition of blind time is aligned with the EPA’s criteria for CMS to be considered as an alternative test method under the recently established OOOOb regulation to reduce methane emissions from the oil and natural gas industry. This regulation allows for the use of alternative test methods, including CMS, to assess compliance with emission standards, if the following requirements, among others, are satisﬁed the speciﬁc measurement solution is designated as an approved alternative test method by the EPA: (i) “... continuous monitoring means the ability of a methane monitoring system to determine and record a valid methane mass emissions rate or equivalent of aﬀected facilities at least once for every 12-hour block”, and (ii) the 90-day rolling average actionlevel is 1.6 kg/hr (3.6 lb/hr) of methane over the site-speciﬁc baseline emissions” and “The 7-day rolling average action level is 21 kg/hr (46 lb/hr) of methane over the site-speciﬁc baseline emissions.” Building upon these regulatory developments, optimizing sensor placement becomes even more critical to meet the requirements outlined by the EPA.
Previous studies have explored various aspects of methane emissions from oil and gas operations, including sensor placement optimization, and performance evaluation of CMS. The following literature review synthesizes key ﬁndings and identiﬁes areas for further research. Chen, Kimura, and Allen [7] employed dispersion modeling to develop robust detection limit deﬁnitions, for evaluating CMS performance characteristics based on a short timeframe of historical data (2 weeks) and a single emission source. This study oﬀered a comparison of simulations to controlled release testing results for CMS. This study showed that the performance of CMS and their minimum detection limits expressed as an emission rate, depend on meteorological conditions, facility-speciﬁc emissions characteristics, sensor placement, and the amount of time allowed for the CMS to detect an emission source (time to detection). This study demonstrated that a single sensor with a detection limit below 2 ppm, positioned 10 meters from a 0.4 kg/h source, could detect the emission within 12 hours. Deploying three sensors resulted in a lower detection time of 6 hours. While increasing the number of sensors from 4 to 8 slightly improved detection time, the most signiﬁcant reduction occurs between 1 and 4 sensors. Enhancing sensor sensitivity or adding more sensors generally decreases the average time to detection within the sensor network.
An earlier study by Chen et al. [8] evaluated the performance of CMS on a regional spatio-temporal simulation domain that incorporates multiple facilities, indicating that based on atmospheric dispersion simulations of 26 oil and gas production sites in the Permian Basin in Texas, ﬁfteen sensors, each capable of detecting concentration increases of 1 ppm, successfully identiﬁed emissions at all 26 sites during four weeks of meteorological episodes, assuming continuous emissions of 10 kg/h.
Sensor placement optimization falls under the broader category of combinatorial optimization, aiming to ﬁnd the best solution from a ﬁnite set of possibilities. These inherently complex problems arise in a variety of real-world scenarios, including sensor placement for environmental monitoring in ocean observation [9, 10], water contamination detection [11], hazardous gas leakage monitoring in chemical plants [12], pipeline leaks [13–15] and air quality monitoring [16]. Depending on the application and objectives, a wide range of methods have been employed to achieve optimum sensor placement, including heuristic methods like greedy algorithms [13, 14, 17], genetic algorithms [18], deterministic approaches like mixed integer programming [11, 19], and decomposition techniques [10, 16].
In the context of CMS for methane emission monitoring, several optimization methods have been employed to determine the best sensor placement and evaluate their performance in a simulated environment. Klise et al. [19] developed an open-source Python package, called Chama, employing atmospheric dispersion modeling and mixed integer linear programming (MILP) to determine optimum sensor locations and detection thresholds to maximize emission event detection. The algorithm oﬀered in this work formulates and solved maximum-coverage and minimum-impact problems in two steps, where ﬁrst methane emission dispersion was modeled using given leak rates, source locations, and wind variables. Then, sensor placement problem for a given sensor network density was solved using MILP. The application of Chama was showcased in the form of a simulated case study in their publication. In our study, we use Chama as an open-source benchmarking tool, oﬀering a comparative analysis of Chama and our proposed framework (Section 5).
Zi et al. [20] attempted to address challenges in sensor placement optimization using stochastic programming by introducing a distributionally robust optimization formulation of sensor placement under the uncertainty of wind conditions. Leveraging MILP for optimization, this study demonstrated signiﬁcant improvements to Chama, minimizing the detection time expectation, with a particular focus on worst-case scenarios.
Liu et al. [21] investigated the optimal sensor placement problem in a 2-dimensional domain using bi-level optimization. This algorithm used inverse modeling to estimate emission rates and determine the optimal sensor placement by minimizing the overall mean squared error of the estimated emission rates over various wind conditions. To solve the sensor placement problem, this study investigated the repeated sample average approximation and the stochastic gradient descent-based bi-level approximation methods. Numerical examples were used to illustrate the application of the proposed method. The authors noted that the sensor allocation problems in the 3-dimensional space (representing emission source and sensor height) will be more challenging, as it requires incorporating surface terrain modeling and computationally eﬃcient algorithms.
Jia, Sorensen, and Hammerling [18] presented an open-source modular framework using the Gaussian puﬀ dispersion model and genetic algorithm to solve the CMS sensor placement optimization problem in a 3-dimensional domain, considering sensor budget limitation and detection eﬃciency. This publication oﬀered case studies implementing the proposed framework for determining optimum sensor placements for CMS including 4 and 8 sensors. These case studies were implemented in both within-the-property-boundary and fenceline placement fashions. Results of this study indicated significant improvement in detection coverage, compared to a greedy search method. While a 3-dimensional domain oﬀers theoretical advantages, practical considerations, such as the implementation challenges, occupational safety considerations, and costs of installing sensors at various heights, often limit its applicability in real-world scenarios. Additionally, the resource-intensive nature of employing Gaussian puﬀ models for simulation, genetic algorithms for optimization, and a ﬁne grid size as candidate locations for sensor placement results in high computational demands, which impose practical constraints and hinder the at-scale implementation of this framework in real-world scenarios.
Typically, sensor optimization frameworks and case studies follow a structured approach, incorporating the following steps with minor variations: (a) deﬁne emission source locations and potential sensor locations, (b) use historical wind data in tandem with a forward dispersion model to simulate methane concentrations, (c) identify optimal sensor locations through various approaches, and (d) evaluate the detection capabilities of CMS.
Building upon the the existing knowledge from previous studies on the optimization of the CMS conﬁguration, the present study aims to take a more practical approach to this problem, by focusing on the performance capabilities of CMS installed in 124 real-world operational oil and gas facilities, to draw more general conclusions about the detection capabilities of CMS across a wide variety of geographic regions or facility designs. In the current work, we explore a greedy sensor placement framework for CMS at oil and gas facilities speciﬁcally designed to meet the criteria outlined in the recently ﬁnalized EPA OOOOb regulation, to study CMS capabilities to meet the criterion of 12-hour facility-level quantiﬁcation.
This study proposes a modular framework employing forward dispersion modeling and a greedy optimization algorithm to determine the best CMS conﬁguration (network sensor density and nearoptimum placement) and consequently determine several CMS performance metrics, including information gain, network blind time, and time to detection. An extensive real-world evaluation eﬀort is oﬀered to investigate the performance of the proposed framework across 124 oil and gas operational facilities. The overarching objective of this study is to investigate whether, based on the data from operational facilities, the proposed framework and CMS are capable of consistently detecting emissions of a given level. To the best of our knowledge, this is the ﬁrst study to rigorously test a sensor placement framework across a wide variety of real-world operational facilities and emission sources. Another novelty of this work lies in its purposeful design of the modular framework and metrics around regulatory requirements. Using a wide variety of real-world upstream oil and gas facilities for model evaluation, this study determines sensor network blind time based on the site-speciﬁc meteorological data. As part of a benchmarking study, a smaller operational dataset is processed by both the open-source Chama model and the proposed framework. The ﬁndings of this study will inform future eﬀorts related to the understanding of the capabilities and limitations of CMS related to continuous emissions measurement and quantiﬁcation. It should be noted that performance evaluation of emissions quantiﬁcation algorithms is beyond the scope of this study and will be discussed in a future publication.
<title>2 Methodology</title>
This modular framework optimizes the CMS conﬁguration and consequently determines sensor network blind time. Three primary steps are included in this framework: (i) input data processing and receptor grid determination, (ii) simulation of emissions using a forward dispersion model of choice, and (iii) optimization of CMS conﬁguration using a greedy algorithm to determine the best sensor placement conﬁguration for various sensor network densities. Figure 1 provides a schematic view of this framework. The algorithm is designed to optimize the performance of CMS in light of 40 CFR §60.5398b(c), meaning that CMS performance is characterized by the network’s ability to consistently detect new information from emission sources over 12-hour periods.
Three metrics are considered for the performance evaluation of CMS, including information gain, blind time, and time to detection. By introducing a metric called blind time, this study attempts to capture periods where the network fails to make detections that could satisfy the regulatory requirement of generating valid emissions quantiﬁcation every 12 hours. By applying this framework to 124 operational oil and gas production facilities with a wide variety of site characteristics (e.g., layout and complexity) and meteorological conditions, this study attempts to determine a representative blind time for near-optimum CMS conﬁgurations for operational facilities and then investigate the impact of diﬀerent sensor network densities on the performance of the CMS. The following subsections outline the key components of the proposed framework, including data (input data processing and receptor grid determination), simulation (forward dispersion modeling of emissions), and optimization (determining the best sensor placement for CMS conﬁguration using a greedy algorithm for various sensor network densities).
<title>2.1 Data</title>
This framework leverages site-speciﬁc data to accurately simulate real-world conditions from operational facilities. Three sets of input data are imported into the framework, including facility layout, facility-speciﬁc atmospheric data, and ﬁnancial constraints that may impose limitations on the sensor network budget. This section outlines data sources for facilities with existing site-speciﬁc data and provides alternative resources for gathering data in the absence of such programs.
Facility layout data includes site boundaries, on-site equipment inventory, and high-precision spa-
tial information (e.g., equipment locations, obstacles, and dimensions). The facility boundary is used to deﬁne the feasible region for CMS sensor placement. Equipment inventory aids in understanding site complexity and ensuring all potential emission sources are accurately represented in the framework. High-precision spatial data, such as aerial site imagery is used to identify equipment locations, obstacles, equipment dimensions, and the location of potential release points. It is crucial for deﬁning the search grid (for optimizing sensor placement), emissions simulation, and optimization processes. In the application of the CMS conﬁguration framework involving operational oil and gas facilities, high-resolution aerial imagery of all 124 facilities is collected and used to deﬁne those features. However, obtaining precise spatial data without such imagery can be challenging, potentially impacting emissions simulation and optimization accuracy.
Facility-speciﬁc atmospheric data includes minute-averaged samples of wind speed and wind direction. This framework uses site-speciﬁc meteorological data collected over the course of a year to capture seasonal variability in the optimization of the CMS conﬁguration. For greenﬁeld projects without site-speciﬁc meteorological data, wind characteristics from nearby facilities, or the NOAA’s real-time 3-km resolution, hourly updated wind characteristics from the High-Resolution Rapid Refresh (HRRR) model can be used. To balance computational eﬃciency and data representation, the application of the CMS conﬁguration framework optimization involving operational oil and gas facilities uses four weeks from each season instead of a full year of meteorological data. This approach captures seasonal variability while reducing computational burden, enabling minute-based concentration simulations and ensuring that the results are not biased toward the wind characteristics of a speciﬁc season. A brief examination of how the wind speed and the atmospheric stability classes (ASC) vary across the diﬀerent basins is presented in section A.2. Further determination of the optimal amount of meteorological data is case-dependent and beyond the scope of this paper.
Plume dispersion patterns are known to be more complex in the presence of low wind speed. This condition negatively impacts the capability of pollution transport models to accurately estimate concentration levels at given locations. As a result, this study adopts a common practice of excluding time windows with wind speeds U lower than 0.5 m/s. An optimization/test approach is employed to assess model performance. This study uses 75% of the meteorological data (12 weeks) for sensor placement optimization, while the remaining 25% of the data (4 weeks) serves as the test set, allowing a blind evaluation of out-of-sample data. The 75/25 split aligns with common practices and ensures proper training while the model’s generalizability is assessed using unseen data.
Facility-speciﬁc meteorological one-minute intervals of average wind speed and direction data were collected using Gill 2D ultrasonic anemometers or RM Young Ultrasonic Anemometers. In addition to facility layout and site-speciﬁc atmospheric data, operator-provided information on equipment inventory and monitoring budget constraints is crucial. Insights such as operator input regarding equipment inventory (for the labeling of potential emission sources) and additional sensor placement constraints (for the designation of infeasible sensor installation area) are used to further enhance this framework.
Input data processing involves source clustering and deﬁning feasible sensor installation regions. Facility layout and operator-provided information are used to label potential emission sources associated with on-site equipment. To reduce the dimensionality of the optimization, agglomerative clustering with a 15-meter spatial threshold is employed to deﬁne spatially distinct equipment groups, with each equipment group representing all the potential emission sources located within its 15-meter distance. This approach avoids bias towards equipment groups with dense potential emission points.
Following source group deﬁnition and characterization, a search grid is established for potential sensor locations. The grid boundaries are constructed by extending the minimum and maximum UTM X,Y coordinates of the set of emission sources by 30.8 meters. This results in 4 vertices that deﬁne the search grid area and will be referred to as the feasible region. Grid size impacts the resolution of the sensor placement optimization problem. A 10x10 meter mesh grid is overlaid on the feasible region to deﬁne search grid nodes (potential sensor locations). This resolution is chosen to adequately capture spatial variations in emission concentration while having reasonable processing times.
A 15.4-meter safety buﬀer is established around the equipment groups, deﬁning a buﬀered hull that represents infeasible regions (areas where sensor installation is prohibited). Additional operatorprovided information is incorporated to account for other operational restrictions when deﬁning infeasible regions. Examples of such operational restrictions could be inside-facility driving paths and areas preserved for future equipment installation or other purposes. The feasible region is further reﬁned by removing search grid nodes that fall within the infeasible region.
Figure 2 illustrates this process with an example. This modular framework allows for ﬂexibility in modifying selected values of a range of variables, including grid size and the distance used to deﬁne the outer limit of the feasible sensor installation region.
Figure 2: Example facility showing emission sources (blue), clustered sources (red), and the feasible region for potential sensor locations (orange).
<title>2.2 Simulation</title>
A forward dispersion modeling tool is employed, utilizing historical atmospheric measurements to simulate pollution transport for plumes originating at the clustered source location. A simulated plume will inform the expected atmospheric concentration levels originating from a speciﬁc source at each search grid node at any given time. Current work employs a Gaussian plume model to simulate pollution dispersion for the 124 operational oil and gas production facilities. However, other pollution transport models, such as Gaussian puﬀ can be used for this simulation. Our previous studies have not indicated signiﬁcant improvement in the simulation accuracy as a result of employing more complex models like Gaussian puﬀ for relatively simple site setups such as simple upstream oil and gas facilities.
The OOOOb rule mandates a response when the 90-day rolling average exceeds the action threshold of 1.6 kg/h above facility baseline. This algorithm is designed to optimize the performance of CMS in light of OOOOb. As a result, continuous emissions are simulated with a release rate of 1.6 kg/hr for each emission group. This approach reﬂects operational conditions where most of the leaks in the real world often persist until repairs are implemented. An in-depth sensitivity analysis of the model performance across various release rates is oﬀered in the supporting document (subsection A.3).
A source-sensitivity matrix S of size (n,m) is generated for every search grid node. Each row (indexed by i) represents a minute of atmospheric methane concentration (totaling n minutes of simulation) and each column (indexed by j) represents a given source group (totaling m source groups). The i, jth element of the source-sensitivity matrix represents the simulated atmospheric concentration value observed at the location of the search grid node, resulting from a release rate of 1.6 kg/hr from source j during minute i. Two sensitivity matrices are constructed for each search grid node, representing training and testing simulation data, hereafter referred to as “in-sample” and “out-of-sample”. In-sample and out-of-sample sensitivity matrices include 12 and 4 weeks of data, corresponding to 120,960 and 40,320 minutes of simulated atmospheric concentrations per sensor per source, respectively. It corresponds to a total of more than 305 million minutes of simulation across all 124 oil and gas operational facilities. This calculation is applied to every potential sensor, generating two unique sensitivity matrices, in-sample and out-of-sample, for each search grid node located in the feasible region. This results in approximately 47 billion simulated concentration values captured in the sensitivity matrices.
In this study, detection is deﬁned as an increase in atmospheric concentrations from a given source exceeding 1 ppm. This binary approach avoids optimization biases towards signal strength. This study assumes the use of tunable diode laser absorption spectroscopy (TDLAS) sensors, with a sensitivity of 0.4 ppm [22]. Therefore, the 1 ppm threshold provides a conservative detection level, 2.5 times the hardware sensitivity. By equally weighting concentrations greater than 1 ppm, we transfer the problem into a binary dimension (detection vs. non-detection), simplifying it to the presence or absence of a detectable signal.
For each search grid node, each element in the sensitivity matrix with a value greater than 1 ppm indicates an elevated atmospheric concentration detected at the speciﬁc search grid node, where the increase in atmospheric concentration results from a 1.6 kg/h release originating from source j and is detectable by a sensor during minute i. A threshold is applied to convert continuous concentration values (sensitivity matrix) into binary detection values in a node-speciﬁc detection matrix, . Each element i,j in the detection matrix is binary, with 1 indicating detection and 0 indicating non-detection. This binary element indicates detection status at the speciﬁc search grid node, during minute i, resulting from source group j.
<title>2.3 Optimization</title>
This framework employs a multi-objective optimization approach to solve CMS sensor network conﬁguration problem. This approach optimizes the network conﬁguration based on two key metrics: information density and blind time. By maximizing information density, the framework ensures that the exposure of the CMS to the elevated concentrations originating from onsite release events is maximized. Simultaneously, minimizing blind time helps prevent long periods where sources remain undetected by the CMS.
For a CMS with p sensors, the overall information density is calculated based on a ‘network detection matrix’, a binary matrix formed by applying the boolean logical or operator across the node-speciﬁc detection matrices associated with the p search grid nodes. The binary nature of the CMS detection matrix prevents double-counting of events detected using multiple sensors simultaneously. This metric is similar to those used in other studies on sensor location optimization [18, 19].
Since information density does not consider temporal variability in detections, relying solely on this metric for CMS conﬁguration can lead to extended periods where no sources are observed by the network. To fully leverage the continuous nature of the CMS measurements, incorporating a metric that minimizes the characteristic timescale of “gaps” between detections is essential. This paper introduces a new metric, blind time, to minimize such gaps between detections.
The term blind time notionally refers to the time period over which the CMS, on average, does not register enough ‘information’ to enable a suﬃciently accurate estimate of the site-level emissions. It is a function of several factors including sensor count, placement, and the modeling approach among others. An ensemble average of the blind time is obtained by sampling over a set of similar operational oil and gas facilities. Within the present context, a 12-hour rolling window is used to segment the simulated time horizon following the OOOOb rule requirements. On a per-source-group basis (i.e., for each column of S), the CMS network is assumed to be operating in the blind for the 12-hour duration highlighted by the rolling window if there are fewer than 10 non-zero entries in the relevant section of the sensitivity matrix S corresponding. This count of the 12-hour-long time periods identiﬁed as blind intervals is aggregated for each potential source group to estimate a cumulative count of blind intervals (BT) for each site. Mathematically, this can be expressed as:
Here, the number of rolling 12-hour interval covering a simulation period n minutes long is determined via w = n − (12 × 60) + 1. Q represents a subset of N over the window k for column j, where N is deﬁned below. The CMS detection matrix, N, is simply the boolean logical or operator (∨) applied element-wise across the p search grid nodes that are selected for the placement of the individual sensors. This is mathematically expressed as:
The CMS detection matrix is initialized as a matrix of size (n, m), where n is the number of minutes in the sample data and m is the number of source groups at the given site. This matrix indicates the detection status achieved using a given CMS of p sensors. A sensitivity analysis, detailed in A.1, informed the decision to use at least 10 nonzero entries in N to classify a 12-hour interval for each source group as either blind or detected.
As noted, the objective of this framework is to ﬁnd a near-optimum CMS conﬁguration that minimizes blind time while maximizing information density. Blind time is a non-linear metric based on a threshold within a rolling window. This framework implements greedy algorithm for ease of implementation and eﬃciency. Greedy is a simple optimization algorithm that makes locally optimal choices at each step, hoping to ﬁnd a near-globally optimal solution. While greedy algorithms are eﬃcient and easy to implement, they don’t guarantee the global optimal solution [17]. However, in this case, ﬁnding a near-optimal solution will be suﬃcient.
The greedy algorithm evaluates all local options of the search grid nodes in the feasible region at each step. By subtracting blind time from information density for all possible search grid node combinations for a CMS with sensor density of p, the algorithm computes ‘marginal gain’, ϵ, the diﬀerence between the information density and blind time as the optimization parameter. For every iteration (e.g., every possible combination of p search grid nodes) the search grid node that maximizes the marginal gain relative to the existing CMS detection matrix is selected. The total number of iterations is equal to the number of sensors to be deployed at the site. In each iteration, the CMS detection matrix is updated according to Equation 3 using all of the currently selected nodes. The algorithm is summarized in Algorithm 1.
<title>3 Results and Discussion</title>
124 operational oil and gas facilities were selected across a wide range of emission source geometries, atmospheric conditions, and associated geographical locations. Facility layout and site-speciﬁc historical atmospheric data were collected for each facility. Then the proposed framework was implemented to determine near-optimal CMS conﬁguration with varying network densities. On average, 15.3 source groups exist in each facility, with the minimum and maximum numbers of 4 and 39 source groups per facility, respectively. Several key metrics were calculated for the sensor network at each site, including information density, percentage blind time, and average time-to-detection. This section oﬀers the aggregated results of implementing the proposed framework using a CMS with 3 sensors in 124 operational oil and gas production facilities.
<title>3.1 Sensor Network Density</title>
This section investigates the impact of sensor network density on information density and compares primary ﬁndings to other studies. Most of this section focuses on a CMS with a set density of three sensors, as our studies of relatively simple oil and gas facilities suggest that this conﬁguration is suﬃcient for achieving reasonable information density with minimal blind time. A sensitivity analysis is conducted to assess the impact of varying sensor network density on blind time and information gain.
While CMS sensing networks are designed for continuous measurement of the ambient concentration of methane, they can experience “blind periods” when there is a lack of direct pollution transport from source to sensor network due to unfavorable wind direction for a sustained period of time. Previous studies [7, 19] have employed metrics similar to information density for the performance evaluation of CMS sensor networks. These metrics often estimate the fraction of time a given CMS can detect ongoing emissions.
As depicted in Figure 3, information density (in percentage form) increases as more sensors are added to the network. As an expected trend, increasing sensor density in the network enhances spatiotemporal coverage, thereby increasing the likelihood of detecting ongoing emissions by covering a wider range of wind directions. A near-linear relationship exists between sensor network density and percent information, with marginal returns slightly diminishing for additional sensors beyond three. Globally, the median percent information across sites for 1, 2, and 3 sensor networks is 8.2% 14.2% and 18.8% respectively.
A median information density of 18.8% for a 3-sensor network indicates that, due to the relatively sparse coverage of a 3-sensor network, it is unsurprising that the information density is correspondingly low (as evidenced by the fact that roughly 4/5 of atmospheric measurements correspond to conditions when the sensor network will not receive information about sources on a given facility). These results align with previous ﬁndings reported in [7], in terms of the achieved information density for a recommended CMS density.
by detection fraction or other metrics similar to information density. To evaluate the eﬀectiveness of CMS, it’s essential to consider the speciﬁc objectives, such as providing at least one valid quantiﬁcation estimate every 12 hours (as indicated in the OOOOb regulation), detecting all short emission events, or simply having the capability to detect large persistent leaks sooner than a quarterly OGI inspection would reveal them. Under diﬀerent objectives, the placement and density of sensors should be adjusted accordingly.
Chen, Kimura, and Allen [7] (hereafter referred to as the UT Austin study) deﬁned temporal coverage as the percent of time the network would detect a single emission source. This metric is comparable to the information density used throughout our study. However, in the present work, information density is calculated based on the temporal coverage across all emission sources. The UT Austin study concluded that an eight-sensor network, with detection capabilities of 1 ppm at 20 meters from the source, achieved 20% temporal coverage. These conclusions are consistent with the results of the present work.
simulations, this study incorporates 16 weeks of simulations per year to comprehensively account for seasonal variations in atmospheric conditions.
Comparable temporal coverage, despite varying sensor numbers, can be partially attributed to the higher release rate used in this study (1.6 kg/h), which is four times that of the UT Austin study. This increased release rate signiﬁcantly enhances detection probability. A detailed analysis of the impact of release rate on information density indicates that with a 0.4 kg/hr release rate, a 3-sensor network achieves 8% information density. This falls between the UT Austin study results for sensor networks at 30 and 40 meters, which have temporal coverage of 10.2% and 6.2% respectively. Further details related to this subject are presented in the Supporting Information (Figure A.3).
<title>3.2 Percent Blind Time</title>
This section introduces a new metric to evaluate CMS eﬀectiveness, aligned with the OOOOb requirements for continuous monitoring technologies to ensure that facility-level emissions are quantiﬁed at least once every 12 hours. We deﬁne percent blind time as:
where percent blind time is calculated as the fraction of 12-hour periods (in %) where a network fails to detect at least 10 measurements from each source group, summed across all sources. Here, w represents the total number of 12-hour rolling windows within the sampled data and m is the number of source groups. Percent blind time eﬀectively represents the probability that a CMS system fails to provide a reliable quantiﬁcation estimate within 12 hours, mainly due to insuﬃcient direct source-to-sensor atmospheric transport.
Note that wind direction is the primary factor inﬂuencing the detectability of elevated concentrations resulting from onsite release events. This highlights the role of site-speciﬁc wind data in optimizing sensor placement to ensure that the elements of CMS collectively maximize overall detections and minimize long gaps in direct source-to-sensor signals.
Figure 4 illustrates variations in percent blind time as a function of the CMS density, based on all 124 example sites in the dataset. A non-linear relationship exists between sensor density and percent blind time, with signiﬁcant reductions in percent blind time achieved by adding the ﬁrst few sensors. For example, the mean percent blind time decreases from 35.5% for a 1-sensor network to 15.0% for a 2-sensor network and 7.1% for a 3-sensor network, demonstrating a more than 2x reduction with each additional sensor. This contrasts with the near-linear trend observed in percent information. It indicates that for a CMS with 3 sensors, 7.1% of rolling windows in the detection matrices across all sites have fewer than 10 positive detections per source group. This translates to a 92.9% probability of a 3-sensor network providing a reliable quantiﬁcation estimate within a 12-hour period.
These results may be somewhat surprising given the relatively low information density demonstrated in Section 3.1. This apparent discrepancy highlights an important distinction: even though the probability of detecting emissions on a site with a 3-sensor network is relatively low, on average, there is suﬃcient wind variability over 12-hour windows such that, with proper sensor placement, each source on a site is observed 93% of the time.
<title>3.3 Time to Detection</title>
Time-to-detection (TTD) represents the delay between the start of an emission event and the detection of an elevated concentration associated with the event by the CMS. TTD is often used in studies as one of the metrics to evaluate the performance of CMS. This metric quantiﬁes the rapidity of direct sourceto-receptor pollution transport, which is inﬂuenced by favorable wind direction and the conﬁguration of the CMS.
Figure 4: Percent blind time as a function of CMS density. The diamonds identify the outliers for the box plot.
source group in each network detection matrix. The time elapsed between the selected event start time and the next detection associated with that speciﬁc source group in the time series is recorded for each Monte Carlo realization. Finally, the average time to detection is determined across all sources and Monte Carlo iterations at each of the 124 operational facilities.
Figure 5 illustrates the relationship between mean TTD and CMS density. Similar to the blind time analysis, the ﬁrst few additional sensors signiﬁcantly reduce mean TTD. For example, increasing from 1 to 2 sensors decreases mean TTD from 442.4 minutes to 164.0 minutes, a 64% reduction. Adding a third sensor further reduces TTD to 82.3 minutes, with 99.1% sites in the study having a mean TTD below 250 minutes.
The UT Austin study conducted a similar TTD analysis using a concentric ring of eight sensors at a 10-meter distance from the source. They optimized sensor placement iteratively, selecting sensors with the highest temporal coverage. The results of that study indicated mean TTDs of 567.1, 191.7, and 107.6 minutes for 1, 2, and 3 sensors, respectively. The ﬁndings of this present work align with their results, with the small diﬀerences in mean TTD which could be attributed to variations in sourcesensor positioning and release rate. Our optimized sensor placements and higher release rate likely contribute to the slightly lower TTDs observed in this study. Despite these diﬀerences, both studies consistently demonstrate that near-optimally conﬁgured 3-sensor CMS could achieve mean TTDs on the order of 80 to 100 minutes.
To analyze the probability of detecting a continuous release of a given duration, the Monte Carlo iterations were sampled to construct empirical cumulative distribution functions (CDF) for CMS with varying densities (Figure 6). These CDFs illustrate the theoretical probabilities of TTD at various CMS sensor network densities based on a large number of randomly selected release event start times. The results indicate that on average, a 3-sensor network achieves a 90% POD within 3.7 hours, demonstrating that optimal CMS sensor placement can consistently achieve timely detections concerning the OOOOb expectations.
<title>4 Case Studies</title>
As an illustrative example, this section presents a case study involving operational oil and gas facilities, applying the proposed framework to determine near-optimal CMS conﬁguration. An in-depth discussion of the near-optimal sensor placement algorithm and framework outputs is included. This section also compares simulation framework results with direct on-site methane concentration measurement data to assess agreement between simulated and measured blind time statistics.
<title>4.1 CMS Conﬁguration and Simulated Blind Time</title>
Figure 7 illustrates a representative example of a facility layout, windrose plot, and optimized sensor locations for a 3-sensor CMS. The numbered green dots indicate the order in which the optimization procedure selected sensor locations. The windrose plot reveals that the ﬁrst recommended sensor location is downwind of a large group of emission sources. The second sensor is positioned on the opposite side the sources, relative to the ﬁrst sensor, capturing emissions from the second most common wind direction. The algorithm positions the third sensor near an isolated source, demonstrating its ability to prioritize new information over redundant detections.
Figure 8 illustrates network detection matrices for this site, showcasing the impact of varying sensor network density. To enhance visual clarity, we focus on a single week of data instead of the full dataset used throughout this study, transposed minute-level network detection matrices. In other words, each row represents a source group’s timeseries of detections/nondetections, with black corresponding to a detection of the source at a given time, and white corresponding to a non-detection. Time periods that would contribute to blind time are highlighted with red rectangles.
Signiﬁcant blind time periods are observed across all sources for a single-sensor network (red rectangles in each row). However, adding more sensors to the network substantially reduces blind time. For the illustrated time period, the network’s percent blind time decreases from 52.8% for a 1-sensor network to 5.8% and 2.6% for 2 and 3-sensor networks, respectively.
Figure 7: Left: Output of the siting algorithm showing sources (blue), potential sensor locations (orange), selected sensors (green), Percent Blind Time (PBT). Right: Windrose diagram showing wind speed and direction distribution.
performance, increasing information density to 21% and halving blind time to 2.6%. This analysis demonstrates the algorithm’s ability to optimize sensor locations to maintain performance under varying wind conditions.
<title>4.2 Measured Blind Time</title>
A comparison of simulation results with actual on-site concentration measurement data was conducted to assess agreement between key simulated statistics (blind time, information density, timeto-detection) and statistics derived from the concentration timeseries directly measured by CMS at operational facilities. Five operational oil and gas facilities with 3-sensor CMS were selected and six months of ambient concentration data was collected from all sensors at each facility. These facilities were chosen for their likelihood of continuous operational emissions from equipment like compressors, minimizing temporal gaps in continuous release events.
measurements to deﬁne detections. The gap between detections (duration of continuous zeros) in the network detection matrices was then calculated, representing the “operational blind times” during the 6-month measurement period.
It’s important to note that this analysis assumes continuous emissions from sources. If emissions are intermittent, additional “blind periods” may occur due to operational breaks rather than the impact of wind direction. This results in more conservative (an upper limit) estimates of operational blind time, compared to the simulated results. The distribution of operational blind periods for the 5 operational facilities is illustrated in Figure 9. These results demonstrate good agreement with the simulated cases.
The mean operational blind time was 1.7 hours (102 minutes), aligning closely with the mean simulated TTD of 82 minutes across all sites. As stated before, intermittent operational emissions may contribute to the higher blind time based on direct measurements. Although calculated diﬀerently, both metrics eﬀectively characterize the duration of gaps where the sensor network fails to detect sources due to unfavorable wind direction. The strong agreement between simulated and measured blind times for 3-sensor networks suggests that: (i) the proposed framework provides acceptable outputs related to the determination of blind time for near-optimal CMS, and (ii) both simulated and operational data indicate that a CMS with 3 sensors will detect changing emissions within, on average, 80 to 102 minutes. This result is in good agreement with the theoretical results from [7]. While site complexity and other factors can inﬂuence these results, we expect these ﬁndings to be generally applicable to facilities with comparable sizes and number of source groups.
Figure 9: Distribution of blind periods at operational facilities.
<title>5 Model Validation</title>
To evaluate the performance of the proposed framework compared to the publicly available sources, a comparison between Chama [19] (a sensor placement optimization tool based on mixed-integer linear programming formulations) and the proposed framework (employing the greedy algorithm) was conducted across 15 randomly selected operational oil and gas production facilities. Using identical input parameters, optimal 3-sensor CMS conﬁgurations were identiﬁed for the second calendar quarter, and their performance was evaluated during the subsequent quarter. The atmospheric data used in this comparison is consistent with that described in Section 2.1.1. As a modular framework, Chama allows for external simulation data. For consistency, both Chama and the proposed framework use the same Gaussian Plume model as the simulation method. By using separate quarters for optimization and testing, we assess network performance on unobserved data. Note that the use of minute-level wind data for one calendar quarter diﬀers from the original Chama algorithm’s implementation, which could potentially introduce inconsistencies in its results.
First, a set of metrics, including percent information, percent blind time, and mean time to detection, is used to compare the performance of the models for diﬀerent CMS network densities (Figure 10). Chama demonstrated relatively better performance in achieving higher percent information compared to the proposed framework (Figure 10 (a)). The multi-objective optimization approach used in this framework, which prioritizes both percent information increase and blind time reduction, contributes to the observed performance diﬀerence. Figure 10 (b) and (c) illustrate key performance diﬀerences between Chama and the proposed framework in terms of blind time and TTD. The proposed framework eﬀectively reduces both blind time and TTD compared to Chama, albeit with a slight trade-oﬀ in percent information. For 3-sensor networks, the proposed framework achieves a mean percent blind time of 7.6% and a mean TTD of 81 minutes, outperforming Chama’s 16.7% blind time and 180 minutes TTD. This comparison demonstrates the eﬀectiveness of the proposed framework in achieving signiﬁcantly lower blind time and TTD with slightly lower percent information.
the sensor locations resulting from Chama (red) and the proposed framework (green) for an example facility (Figure 11). While Chama eﬀectively maximizes information percent, it exhibits a bias towards predominant wind directions. This bias can lead to extended blind periods when the wind deviates from the prevailing direction. The proposed framework addresses this shortcoming by strategically placing two sensors downwind of emission sources and a third sensor on the north-western side of the facility. This conﬁguration ensures that the CMS detects elevated concentrations under a wider range of wind directions. This example highlights the impact of diﬀerent objectives on CMS conﬁguration and resulting performance.
<title>6 Conclusion</title>
This study introduces a modular framework, employing forward dispersion modeling and a greedy algorithm, to optimize the conﬁguration of the continuous monitoring systems (CMS) using ﬁxed-point sensors. This framework aims to select near-optimum sensor placement for diﬀerent CMS network densities, speciﬁcally to meet the criteria outlined in the recently ﬁnalized EPA OOOOb rule, to ensure that CMS are able to determine a valid methane mass emissions rate at least once for every 12-hour block. A multi-objective optimization approach is used to maximize information density and minimize CMS blind time simultaneously. The framework was applied to 124 operational oil and gas production facilities with a wide variety of site characteristics and meteorological conditions to evaluate the eﬀectiveness of CMS in detecting and quantifying emissions. Three metrics are considered for the performance evaluation of CMS, including information gain, blind time, and time to detection.
Application of the framework to operational oil and gas production facilities indicates that on average, 3-sensor CMS have an information density of approximately 20% meaning that one out of ﬁve elements in the CMS detection matrix corresponds to a detection. However, systems rarely experience prolonged periods of blind time, with an average blind time of 7.1% for a 3-sensor CMS. This indicates that the probability of observing a 12-hour block without obtaining enough information to make a reasonable rate estimate on a given source is 7.1%. The average time to detection for a 3-sensor CMS is estimated to be 82.3 minutes, with all sites in the study having a mean time to detection below 250 minutes. These results demonstrate that, with proper CMS conﬁguration and suﬃcient sensitivity, on average, CMS can reliably detect emissions on 12-hour time blocks in 92.9% of cases, and also provide alerts of anomalous emissions within less than 1.5 hours of the event starting time.
hours, then as demonstrated, a metric such as blind time must be considered in the sensor placement optimization process. This study demonstrates that the proposed framework using multi-objective optimization processes can result in improved CMS performance, which is essential for meeting the criteria outlined in the EPA OOOOb rule.
<title>References</title>
[1] Jiayang Lyra Wang et al. “Multiscale methane measurements at oil and gas facilities reveal necessary frameworks for improved emissions accounting”. In: Environmental science & technology 56.20 (2022), pp. 14743–14752.
[2] William S Daniels et al. “Toward multiscale measurement-informed methane inventories: reconciling bottom-up site-level inventories with top-down measurements using continuous monitoring systems”. In: Environmental Science & Technology 57.32 (2023), pp. 11823–11833.
[3] Felipe J Cardoso-Salda˜na. “Tiered leak detection and repair programs at simulated oil and gas production facilities: Increasing emission reduction by targeting high-emitting sources”. In: Environmental Science & Technology 57.19 (2023), pp. 7382–7390.
[4] Chiemezie Ilonze et al. “Assessing the Progress of the Performance of Continuous Monitoring Solutions under a Single-Blind Controlled Testing Protocol”. In: Environmental Science & Technology (2024).
[5] Zhenlin Chen et al. “Comparing Continuous Methane Monitoring Technologies for High-Volume Emissions: A Single-Blind Controlled Release Study”. In: ACS ES&T Air (2024).
[6] CSU METEC. Continuous Monitoring Final Report. 2024. url: https://www.projectcanary. com/wp-content/uploads/2024/07/Project-Canary-2024-ADED-Results.pdf.
[7] Qining Chen, Yosuke Kimura, and David T Allen. “Deﬁning Detection Limits for Continuous Monitoring Systems for Methane Emissions at Oil and Gas Facilities”. In: Atmosphere 15.3 (2024), p. 383.
[8] Qining Chen et al. “Simulated methane emission detection capabilities of continuous monitoring networks in an oil and gas production region”. In: Atmosphere 13.4 (2022), p. 510.
[9] Chen Wang et al. Learning to Optimise Climate Sensor Placement using a Transformer. 2024. arXiv: 2310.12387. url: https://arxiv.org/abs/2310.12387.
[10] B Yildirim, C Chryssostomidis, and GE Karniadakis. “Eﬃcient sensor placement for ocean measurements using low-dimensional concepts”. In: Ocean Modelling 27.3-4 (2009), pp. 160–173.
[11] Jonathan Berry et al. “Sensor placement in municipal water networks with temporal integer programming models”. In: Journal of water resources planning and management 132.4 (2006), pp. 218–224.
[12] Jikai Dong et al. “Optimization of sensor deployment sequences for hazardous gas leakage monitoring and source term estimation”. In: Chinese Journal of Chemical Engineering 56 (2023), pp. 169–179.
[13] Andreas Krause, Ajit Singh, and Carlos Guestrin. “Near-optimal sensor placements in Gaussian processes: Theory, eﬃcient algorithms and empirical studies”. In: Journal of Machine Learning Research 9.2 (2008).
[14] Lina Sela and Saurabh Amin. “Robust sensor placement for pipeline monitoring: Mixed integer and greedy optimization”. In: Advanced engineering informatics 36 (2018), pp. 55–63.
[15] Safuriyawu Ahmed et al. “Development and Analysis of a Distributed Leak Detection and Localisation System for Crude Oil Pipelines”. In: Sensors 23.9 (2023), p. 4298.
[16] Makoto M Kelp et al. “Data-Driven Placement of PM2.5 Air Quality Sensors in the United States: An Approach to Target Urban Environmental Injustice”. In: Geohealth 7.9 (2023).
[17] Dravyansh Sharma, Ashish Kapoor, and Amit Deshpande. “On greedy maximization of entropy”. In: International Conference on Machine Learning. PMLR. 2015, pp. 1330–1338.
[18] Meng Jia, Troy Sorensen, and Dorit Hammerling. “Optimizing continuous monitoring sensor placement on oil and gas sites”. In: (2024).
[19] Katherine A Klise et al. “Sensor placement optimization software applied to site-scale methaneemissions monitoring”. In: Journal of Environmental Engineering 146.7 (2020), p. 04020054.
[20] Yuan Zi et al. “Distributionally robust optimal sensor placement method for site-scale methaneemission monitoring”. In: IEEE Sensors Journal 22.23 (2022), pp. 23403–23412.
[21] Xinchao Liu et al. “Optimal Sensor Allocation with Multiple Linear Dispersion Processes”. In: arXiv preprint arXiv:2401.10437 (2024).
[22] Project Canary. Project Canary Continuous Description of Technology. 2024. url: https:// methane.app.cloud.gov/review/46.
[23] Clay Bell et al. “Performance of Continuous Emission Monitoring Solutions under a Single-Blind Controlled Testing Protocol”. In: Environmental Science & Technology 57.14 (2023). PMID: 36977200, pp. 5794–5805. doi: 10.1021/acs.est.2c09235. eprint: https://doi.org/10.1021/ acs.est.2c09235. url: https://doi.org/10.1021/acs.est.2c09235.
[24] Roland R Draxler. “Estimating vertical diﬀusion from routine meteorological tower measurements”. In: Atmospheric Environment (1967) 13.11 (1979), pp. 1559–1564.
<title>A Supporting Information - A Framework for Optimizing Continuous Methane Monitoring System Conﬁguration for Minimal Blind Time: Application and Insights from over 100 Operational Oil & Gas Facilities</title>
This section provides statistics and supplemental analysis of the performance of CMS with respect to site-speciﬁc characteristics. A rationale for our operational choice to categorize a 12-hour interval as being ‘detected’ by the CMS when at least 10 entries per source group in S are non-zero is included here as well.
<title>A.1 A justiﬁcation for the chosen blind time criteria</title>
A justiﬁcation is oﬀered here by revisiting the single-source controlled-release experiments from Methane Emissions Technology Evaluation Center’s 2022 Advancing Development of Emissions Detection (ADED) campaign for which the in-house modeling approach correctly localizes the source group and determines the source intensity within a factor of 0.2 of the actual rate. Only those minuteaveraged concentration measurements from each experiment in the experimental subset are selected where the forward-model estimates of the ambient concentration at point-sensor positions exceed the detection criteria of 1 ppm, hereafter referred to as dataset D . A random sample of t measurements is drawn from D for each experiment and a quantiﬁcation estimate obtained by applying the in-house modeling approach. This procedure is repeated a 100 times to reduce the uncertainty in the individual quantiﬁcation estimates. Note that the identity of the active source group is assumed to be known during this analysis and the problem simpliﬁes into one with single unknown constant emission rate.
reveals that symbols representing larger t values are missing for several experiments in ﬁgure A.1(a), which is due to the absence of suﬃcient samples with estimates for concentration enhancement levels by the forward model above the threshold. While it is clear that for most experiments in ﬁgure A.1(a) the ratio moves towards unity as t increases, there are a few instances however, e.g. experiments labeled 8 and 15, for which the ratio noticeably worsens as t grows. Inspection suggests that underprediction and its further exacerbation with increasing t maybe related to the well-known limitations of the Gaussian plume model under very low wind speeds. For experiments 8 and 15, the mean horizontal wind speed magnitudes at the anemometer height of about 2 meters over the duration of the release are 0.64 and 0.98 m/s, respectively. It is further evident from ﬁgure A.1(a) that even when the ratio improves on increasing the randomly drawn sample sizes, this reﬁnement is generally marginal barring a couple of outliers that show improvement of more than a factor of 0.2 as t changes from 10 to 20. The two core arguments of the above analysis − i.e., a mean ratio around unity and observing limited improvement in predicted intensity as t increases − extend some rationale to our choice of classifying a 12-hour window as being detected by the CMS network when at least 10 estimates by the forward model exceed the 1ppm enhancement criteria.
Next, we examine the integrated set of Q (subset of N for each source group m and rolling window w) constructed by amalgamating the complete forward simulation results from all the sites analyzed in this study. The aim is to emphasize that the frequency of rolling windows with the number of detections per source group hovering around the criteria for minimum detection count occur rather seldomly. Figure A.1(b) plots the relative frequency histogram for the ‘sum of detections per source group in a rolling window ( )’ obtained after randomly sampling a million rows from the integrated set of Q mentioned earlier. It is evident that with a mean and median at 136 and 116, respectively, and the the relative frequency for the ﬁrst two bins at about 0.12, generally well exceeds the minimum threshold of 10 detections.
<title>A.2 Wind Statistics</title>
This section oﬀers key statistics regarding wind parameters and investigates certain trends between those parameters and various metrics. Figure A.2 shows wind speed distributions across major oil and gas basins. The data has been collected using over 1,000 anemometers installed in operational oil and gas production facilities in the US.
While there are diﬀerences in the mean characteristics across basins they all follow a log-normal distribution with 81 percent of readings falling below 3 m/s. Next, we inspect atmospheric stability class distributions across basins.
Table A.1 shows the percentage distribution of each stability class across the major basins. Stability classes are determined using the sigma theta method over a rolling time window. This method computes the circular standard deviation of wind direction over this rolling window and maps it to a Pasquill class via a table look-up [24]. The greater the sigma theta, the more unstable the conditions. At an individual stability class level, there are signiﬁcant diﬀerences across basins. Speciﬁcally, the Appalachian and Haynesville basins have larger percentages of very unstable regimes relative to the other basins. When aggregating stability classes to unstable (A, B), neutral (C, D), and stable (E, F), some similarities are observed. First, analyzing the unstable regime, an apparent diﬀerence between Appalachian and Haynesville and other basins is observed. It indicates that these two basins experience signiﬁcantly more unstable atmospheric conditions. When analyzing the neutral stability classes, all basins fall within -3.18 /+1.82 of the mean (60.68), indicating neutral stability conditions are relatively consistent across all basins. The neutral stability classes are the predominant conditions, with all basins having more than 50% stable conditions. Appalachian and Haynesville basins have signiﬁcantly lower stable atmospheric conditions relative to the other basins.
<title>A.3 Key Metrics Stratiﬁed by Site Speciﬁc Variables</title>
class, and simulated release rate.
The number of emission sources varies on a case-by-case basis. The eﬀect of the number of emissions sources and site area on the percent information is investigated. Figure A.3 (a) shows the relationship between the number of emission sources and percent information for a 3-sensor network. The relationship is inversely related such that by increasing the number of emission sources, percent information decreases. This is evidenced visually by the downward trend and the best-ﬁt line has a coeﬃcient of -0.35.
Only 18% of the variability in percent information is explained by the number of sources. It indicates that other variables such as equipment orientation and site-speciﬁc wind characteristics impact the ability of the CMS to detect emissions. The downward trend is in line with the underlying intuition of spatial coverage. Increasing the number of sources results in the expansion of the coverage area (an artifact of spatial clustering). While holding the number of devices constant, a decrease in the percent information of the CMS is expected.
Figure A.3: Analyses of percent information: (a) as a function of the number of emission sources, (b) as a function of site area, (c) across diﬀerent basins, and (d) as a function of release rate.
previously, site area is correlated to the number of sources. Therefore, the same inverse trend is expected, which is evidenced by a coeﬃcient of -10.72. This shows that an increase in a site area of 1 hectare results in a decrease of 10.72 in percent information. The site area is a good indicator of a 3-sensor network’s expected Information Density. Since sensor positions are ﬁxed, increasing facility area will signiﬁcantly decrease the ability of the network to make detections from all the sources. A relatively low R value of 0.36 exists, indicating that while the relationship between area and percent information is signiﬁcant, facility area only explains 36% of the variation, and site-speciﬁc factors impact information more signiﬁcantly.
The analysis of percent information by basin A.3 (c) and comparisons to a geographic region stability class distribution indicate a relationship between a geographic region stability class distribution and percent information. Speciﬁcally, higher medians and larger upper whiskers for the Appalachian and Haynesville basins are noticed, with the exception of the Barnett. The Appalachian and Haynesville basins had signiﬁcantly more unstable stability class conditions. This indicates more drastic and frequent changes in wind direction which would lead to more detections and higher percent information. Since the Barnett region experiences more frequent stable conditions, less ﬂuctuation in the wind direction could be expected. It indicates more dominant and consistent wind directions. However, further investigation is required to support the suggested conclusions.
Figure A.3 (d) shows the impact of release rate on mean percent information. An increase in mean percent information is observed as by increasing the release rate. While the increases across the stratiﬁed release rates are linear, the percent information increase is non-linear. This is an artifact of the thresholding process. The number of concentrations above 1 ppm signiﬁcantly decreases with lower release rates, especially when a sensor is oﬀ the center of the plume. As the release rate increases, the number of concentrations above 1 increases. This presents itself in the data as the mean percent information for a 3-sensor network increases from 8.4 to 25 as the release rate increases from 0.4 kg/hr to 2.8 kr/hr. We notice diminishing marginal returns as the release rate increases. A higher release rate adds little new information to the network and only adds information by pushing concentrations that were below the threshold (1 ppm) above the threshold. The majority of the time it increases the predicted concentration that the network had already detected, thus, no new information is added.
Figure A.4 (a) shows the relationship between the percent blind time and the number of emission sources for a 3-sensor network. A slight indication of positive correlation is observed, but when comparing the slopes to the percent information analysis, blind time is less sensitive to the number of sources and site area (Figure A.4 (b). This is an artifact of the blind time metric analyzing 12-hour windows of the sensitivity. It is more feasible for a 3-sensor network to consistently make detections within windows with few sources or smaller site areas.
Figure A.4 (c) shows the distributions of percent blind time across basins. The inverse relationship between the percent information and percent blind time becomes apparent. The Appalachian and Haynesville basins had higher median percent information. The inverse relationship is stronger in the Haynesville basin with a median blind time below 5%. The Appalachian relationship is not as clear due to the majority of sites in the sample residing in this basin, which causes the relationship to become muddled. Tying to the stability class, more unstable stability class regions correspond to lower blind time.
A similar diminishing marginal return in mean percent blind time as the release rate increases is observed as with mean percent information. A lower release rate signiﬁcantly increases the mean percent blind time. A three-sensor network has 28% blind time on average with a release rate of 0.4 kg/hr compared to a mean blind time of 4.7% with a release rate of 2.8 kg/hr. Lower release rate signiﬁcantly decreases the detectable information due to thresholding and higher release rates add minimal new information, but increase the concentrations already detected.
Figure A.4: Analyses of percent blind time: (a) as a function of the number of emission sources, (b) as a function of site area, (c) across diﬀerent basins, and (d) as a function of release rate.
Figure A.5: Analyses of mean time to detection: (a) as a function of the number of emission sources, (b) as a function of site area, (c) across diﬀerent basins, and (d) as a function of release rate.
Figures A.5 (a and b) show the relationship between the number of sources and site area versus mean time to detection, respectively. The number of emission sources shows minimal impact on the mean time to detection (evidenced by the small coeﬃcient of 1.04). Rather, the site area is a more explanative variable with respect to time to detection shows the 3-sensor networks have limited spatial coverage. Regardless, the results show that 3-sensor networks hold to ability to make timely detections, well within six hours, for site areas ranging from 5,000 square meters to 16,0000 square meters.
Figure A.5 (c) shows the distribution of mean time to detection across the various oil and gas basins in this study. The eﬀect of the stability class stands out when analyzing time to detection. For the more unstable basins, Appalachian and Haynesville, a signiﬁcantly lower mean time to detection with medians near 50 minutes is achieved. This shows that more timely detections by a 3-sensor network can be made in regions where rapid ﬂuctuations in the wind directions are more common.
source-receptor distance was 40.47 meters. This highlights the impact of optimal placement as the correct framework achieves a lower TTD with fewer sensors than a non-optimal 8-sensor network (326 versus 456 minutes) under the same release rate of 0.4 kg/hr and distance.
This document studies the eﬀect of site-speciﬁc parameters on the key metrics used to evaluate the performance of the algorithm and 3-sensor networks. the results show that the number of emission sources and site area impact the percent information more than the percent blind time and mean time to detection relatively. Investigation of the impact of stability class indicates that more unstable atmospheric classes lead to better performance overall, showing that CMS can detect more consistently under rapid ﬂuctuations in the wind direction. The impact of increasing the simulated release rate shows consistent diminishing marginal returns across all key metrics. Low release rates signiﬁcantly impact the performance of networks and the performance gain under a larger release rate is minimal. The EPA’s criteria for CMS under the alternative test method is that systems should have action thresholds of 1.2 and 1.6 over a 90-day rolling window. This study concludes that, with optimal sensor placement and appropriate sensitivity, CMS performs well at these thresholds with minimal blind time and timely detection.